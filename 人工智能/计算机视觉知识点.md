CV + 图像生成 + 多模态

# diffusion-pytorch 实现

https://github.com/lucidrains/denoising-diffusion-pytorch
#  stable-diffusion 原理

https://github.com/CompVis/stable-diffusion#reference-sampling-script
        
        
        
        
        

# 2023 年大模型总结

![image.png](https://p3-juejin-sign.byteimg.com/tos-cn-i-k3u1fbpfcp/fcd2f58f0f014db4ad2d9823ffa8d899~tplv-k3u1fbpfcp-jj-mark-v1:0:0:0:0:5o6Y6YeR5oqA5pyv56S-5Yy6:q75.awebp?rk3s=bb34011d&x-expires=1731132075&x-signature=xD5oy5qjtip44cIJZU6I%2FoBA7uE%3D)

# 前言内容

1. PEFT、SFT、RM、RLHF、大模型微调、、DeepSpeed、Megatron-LM、解决模型幻觉、知识对齐、可控生成 
2. 熟悉业界领先的 LLM 系列，包括但不限于 GPT、LLaMA、GLM、Bloom 等，LLM 预训练和优化。
3. Stable Diffusion、SD、ControlNet、diffusion、图片编辑、文生图、图生文、CLIP、SAM、VAE、U-Net

# 图像生成

1. AE ：encoder + decoder ，一般使用的是训练好的 encoder ，用于图片的压缩、特征提取、去噪等
2. VAE

    重参数化技巧:核心思想是通过引入噪声项 epsilon，将采样的过程拆分成两步。首先从标准正态分布中采样噪声 epsilon，然后使用潜在变量的均值和方差进行线性变换。这个过程使得采样过程是可微的，从而可以通过反向传播进行训练。 数学上，这个过程可以表示为：
  
    ![image.png](https://p3-juejin-sign.byteimg.com/tos-cn-i-k3u1fbpfcp/2988bf91e16e4b879906c4d15663ebb0~tplv-k3u1fbpfcp-jj-mark-v1:0:0:0:0:5o6Y6YeR5oqA5pyv56S-5Yy6:q75.awebp?rk3s=bb34011d&x-expires=1731132075&x-signature=l%2Fy2aQZFLNWMSo%2FWNlP38XjOJFE%3D)

# 文生图

[文本生成图像模型各种模型详细总结](https://swarma.org/?p=37227)

[文本生成图像模型各种模型详细总结](https://zhuanlan.zhihu.com/p/593896912)
    
# CLIP



![image.png](https://p3-juejin-sign.byteimg.com/tos-cn-i-k3u1fbpfcp/720a5634affe4b089b23e73bfd8122c4~tplv-k3u1fbpfcp-jj-mark-v1:0:0:0:0:5o6Y6YeR5oqA5pyv56S-5Yy6:q75.awebp?rk3s=bb34011d&x-expires=1731132075&x-signature=llgkHRPpLp4%2BTvEKWARAVq5t8Po%3D)
## 训练
CLIP的模型结构其实非常简单：包括两个部分，即文本编码器（Text Encoder） 和图像编码器（Image Encoder) 。Text Encoder选择的是Text Transformer模型；Image Encoder选择了两种模型，一是基于CNN的ResNet（对比了不同层数的ResNet），二是基于Transformer的ViT。


![CLIP 训练过程.gif](https://p3-juejin-sign.byteimg.com/tos-cn-i-k3u1fbpfcp/0f199606ba314341983aa8dc813559dc~tplv-k3u1fbpfcp-jj-mark-v1:0:0:0:0:5o6Y6YeR5oqA5pyv56S-5Yy6:q75.awebp?rk3s=bb34011d&x-expires=1731132075&x-signature=qoveneNPcj4HkJn%2BZAoUteF5VH4%3D)

## 推理

1.  根据所迁移的数据集将所有类别转换为文本。这里以Imagenet有1000类为例，我们得到了1000个文本：`A photo of {label}`。我们将这1000个文本全部输入 Text Encoder中，得到1000个编码后的向量 ，Ti（0<=i<=1000）,这被视作文本特征。
1.  我们将需要分类的图像（单张图像）输入Image Encoder中，得到这张图像编码后的向量 I1 ，将I1与得到的1000个文本特征分别计算余弦相似度。找出1000个相似度中最大的那一个，那么评定要分类的图片与第三个文本标签（dog）最匹配，即可将其分类为狗。

## github 仓库

1. https://link.zhihu.com/?target=https%3A//github.com/openai/CLIP
1. 仓库里面有完整的 python 使用教程

tokenizer
text encoder：就是 12 层重复的 masked multi-head self-Attention + Dense + LayerNorm + ResNet

# DALL·E 2
[论文地址](https://cdn.openai.com/papers/dall-e-2.pdf)

DALL·E 2 这个模型的任务很简单：输入文本 text ，生成与文本高度对应的图片。它主要包括三个部分：CLIP，先验模块 prior 和 img decoder 。其 中CLIP 又包 含text encoder 和 img encoder 。训练和推理图如下所示，虚线上方训练 CLIP 过程；虚线下方由文本生成图像过程。

 
![image.png](https://p3-juejin-sign.byteimg.com/tos-cn-i-k3u1fbpfcp/c880a8c674c9498c86b7964b649cde4f~tplv-k3u1fbpfcp-jj-mark-v1:0:0:0:0:5o6Y6YeR5oqA5pyv56S-5Yy6:q75.awebp?rk3s=bb34011d&x-expires=1731132075&x-signature=qrNPhHKBwpn1UuD6HJ%2ByegzCx%2BA%3D)



## 训练
DALL·E 2 是将其各个子模块分开训练的，最后将这些训练好的子模块拼接在一起，最后实现由文本生成图像的功能。

1. 训练 CLIP 使其能够编码文本和对应图像，这一步是与 CLIP 模型的训练方式完全一样的，目的是能够得到训练好的 text encoder 和 img encoder 。这么一来，文本和图像都可以被编码到相应的特征空间。对应上图中的虚线以上部分。

2. 训练 prior 使文本编码可以转换为图像编码，将CLIP中训练好的 text encoder 拿出来，输入文本 y ，得到文本编码 zt 。同样的，将 CLIP 中训练好的 img encoder 拿出来，输入图像 x 得到图像编码 zi 。我们希望 prior 能从 zt 获取相对应的 zi 。假设 zt  经过 prior 输出的特征为 zi′，那么我们自然希望 zi′ 与 zi 越接近越好，这样来更新我们的 prior 模块。最终训练好的 prior ，将与 CLIP 的 text encoder 串联起来，它们可以根据我们的输入文本 y 生成对应的图像编码特征 zi 了。关于具体如何训练 prior ，有兴趣的小伙伴可以精度一下原文，作者使用了主成分分析法 PCA 来提升训练的稳定性。


    ![image.png](https://p3-juejin-sign.byteimg.com/tos-cn-i-k3u1fbpfcp/2eef623bc6684ca79a4a5100279a1844~tplv-k3u1fbpfcp-jj-mark-v1:0:0:0:0:5o6Y6YeR5oqA5pyv56S-5Yy6:q75.awebp?rk3s=bb34011d&x-expires=1731132075&x-signature=d1sKAY2soKTtJaUQqDOWOL4r4xs%3D)

    在 DALL·E 2 模型中，作者团队尝试了两种先验模型：自回归式Autoregressive (AR) prior 和扩散模型 Diffusion prior  。实验效果上发现两种模型的性能相似，而因为扩散模型效率较高，因此最终选择了扩散模型作为 prior 模块。 
    
3. 训练 decoder 生成最终的图像，也就是说我们要训练decoder模块，从图像特征 zi 还原出真实的图像 x  。这个过程与自编码器类似，从中间特征层还原出输入图像，但又不完全一样。我们需要生成出的图像，只需要保持原始图像的显著特征就可以了，这样以便于多样化生成，如下图。DALL-E 2使用的是改进的 GLIDE 模型，这个模型可以根据 CLIP 图像编码的zi ，还原出具有相同与 x 有相同语义，而又不是与 x 完全一致的图像。


![image.png](https://p3-juejin-sign.byteimg.com/tos-cn-i-k3u1fbpfcp/48637af0f7aa4c338b4a6094d86f84e3~tplv-k3u1fbpfcp-jj-mark-v1:0:0:0:0:5o6Y6YeR5oqA5pyv56S-5Yy6:q75.awebp?rk3s=bb34011d&x-expires=1731132075&x-signature=fJCmxHkiQTOwZ%2BFCY%2Fmuwqy%2Bns8%3D)


## 推理
经过以上三个步骤的训练，已经可以完成 DALL·E 2 预训练模型的搭建了。我们丢掉 CLIP 中的 img encoder ，留下 CLIP 中的 text encoder ，以及新训练好的 prior 和 decoder 。这么一来流程自然很清晰了：由 text encoder 将文本进行编码，再由 prior 将文本编码转换为图像编码，最后由 decoder 进行解码生成图像。



![image.png](https://p3-juejin-sign.byteimg.com/tos-cn-i-k3u1fbpfcp/02d5cc14967044b18629b87e37a3cd87~tplv-k3u1fbpfcp-jj-mark-v1:0:0:0:0:5o6Y6YeR5oqA5pyv56S-5Yy6:q75.awebp?rk3s=bb34011d&x-expires=1731132075&x-signature=ALKKQIMdRT0zeHEauEhp43S7Jow%3D)
 

## 不足

- 容易将物体和属性混淆
- 对于将指定文本绘制图像中的能力不足
- 在生成复杂场景图片时，对细节处理有缺陷


# 图像分类、图生文、图像相似度、图像分割

1. 池化原理
2. 卷积原理、计算公式
3. 上采样、下采样
4. 插值
5. padding：same、valid ： https://blog.csdn.net/David_jiahuan/article/details/104722284
       # 
        
# denoising-diffusion-pytorch 实现

https://github.com/lucidrains/denoising-diffusion-pytorch
 
        
         
       