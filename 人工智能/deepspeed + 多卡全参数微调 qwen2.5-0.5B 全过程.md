
#### 1 window 4090 å•å¡+bsè®¾ç½®ä¸º 1

æ ·æœ¬ä¸€å…± 10748 ä¸ªï¼Œbatch_size ä¸º 1 ï¼Œepoch ä¸º 3 ï¼Œæ‰€ä»¥ 4029 ä¸ª steps ï¼Œæ¶ˆè€—æ˜¾å­˜ 5.7 G ï¼ŒCPU 3 ä¸ªæ ¸å¿ƒè·‘æ»¡äº†ï¼Œå†…å­˜ä½¿ç”¨1.2G ï¼Œé¢„è®¡è€—æ—¶ 40 åˆ†é’Ÿå·¦å³ã€‚https://hl8xut0wpb.feishu.cn/docx/QWy2daPELob4nVxj8XacgFkOnfb#share-XRDGdchTCoylslxSuesc7VWjnvh

ä»£ç å¦‚ä¸‹:

```
import json
import os
import numpy as np
import torch
from torch.utils.data import Dataset
from torch.utils.tensorboard import SummaryWriter
from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments,DataCollatorForSeq2Seq

class NerDataset(Dataset):
    def __init__(self, data_path, tokenizer, max_source_length, max_target_length) -> None:
        super().__init__()
        self.tokenizer = tokenizer
        self.max_source_length = max_source_length
        self.max_target_length = max_target_length
        self.max_seq_length = self.max_source_length + self.max_target_length
        self.data = []
        if data_path:
            with open(data_path, "r", encoding='utf-8') as f:
                for line in f:
                    if not line or line == "":
                        continue
                    json_line = json.loads(line)
                    text = json_line["text"]
                    label = json_line["label"]
                    label = json.dumps(label, ensure_ascii=False)
                    self.data.append({ "text": text,  "label": label })
        print("data load ï¼Œ sizeï¼š", len(self.data))

    def preprocess(self, text, label):
        messages = [ {"role": "system", "content": "ä½ çš„ä»»åŠ¡æ˜¯åšNerä»»åŠ¡æå–, æ ¹æ®ç”¨æˆ·è¾“å…¥æå–å‡ºå®Œæ•´çš„å®ä½“ä¿¡æ¯, å¹¶ä»¥JSONæ ¼å¼è¾“å‡ºã€‚"},  {"role": "user", "content": text} ]
        prompt = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        instruction = self.tokenizer(prompt, add_special_tokens=False, max_length=self.max_source_length, padding="max_length", pad_to_max_length=True, truncation=True)
        response = self.tokenizer(label, add_special_tokens=False, max_length=self.max_target_length, padding="max_length", pad_to_max_length=True, truncation=True)
        input_ids = instruction["input_ids"] + response["input_ids"] + [self.tokenizer.pad_token_id]
        attention_mask = (instruction["attention_mask"] + response["attention_mask"] + [1])
        labels = [-100] * len(instruction["input_ids"]) + response["input_ids"] + [self.tokenizer.pad_token_id]
        return input_ids, attention_mask, labels

    def __getitem__(self, index):
        item_data = self.data[index]
        input_ids, attention_mask, labels = self.preprocess(**item_data)
        return { "input_ids": torch.LongTensor(np.array(input_ids)), "attention_mask": torch.LongTensor(np.array(attention_mask)),  "labels": torch.LongTensor(np.array(labels))}

    def __len__(self):
        return len(self.data)

def main():
    print(f"GPU æ˜¯å¦å¯ç”¨{torch.cuda.is_available()}ï¼Œæœ‰{torch.cuda.device_count()}ä¸ªGPU")
    model_name = "/root/autodl-tmp/Qwen2.5-0.5B-Instruct"
    train_json_path = "/root/autodl-tmp/finetune-qwen7b/data/ner/train.json"
    val_json_path = "/root/autodl-tmp/finetune-qwen7b/data/ner/dev.json"
    max_source_length = 90  # text æ ·æœ¬æœ€é•¿çš„ token æ˜¯ 50 ï¼Œæ¨¡æ¿è‡ªèº«å¤§çº¦ 30 ä¸ªï¼Œæ€»å…±è‡³å°‘ 80 ä¸ª
    max_target_length = 70  # label æ ·æœ¬æœ€é•¿çš„ token æ˜¯ 70
    logs_dir = "logs"
    writer = SummaryWriter(logs_dir)
    # model and tokenizer
    tokenizer = AutoTokenizer.from_pretrained(model_name, )
    model = AutoModelForCausalLM.from_pretrained(model_name, attn_implementation="sdpa", torch_dtype="bfloat16")  # sdpa=Scaled Dot-Product Attention,  flash_attention_2 åªæ”¯æŒ fp16 å’Œ bf16 ä½†æ˜¯åŠ é€Ÿä¸æ˜æ˜¾ç”šè‡³å‡é€Ÿ
    print("Start Load Train and Validation Data...")
    training_set = NerDataset(train_json_path, tokenizer, max_source_length, max_target_length)
    val_set = NerDataset(val_json_path, tokenizer, max_source_length, max_target_length)
    # trainer
    training_args = TrainingArguments(
        output_dir="sft-7B-lora-ner", do_train=True, do_eval=True, seed=42, per_device_train_batch_size=1, per_device_eval_batch_size=1, gradient_accumulation_steps=8, gradient_checkpointing=True,
        num_train_epochs=3, learning_rate=1e-4, warmup_ratio=0.03, weight_decay=0.1, lr_scheduler_type="cosine", save_strategy="epoch", save_total_limit=3, evaluation_strategy="epoch", bf16=True,)
    trainer = Trainer( model=model, args=training_args,  train_dataset=training_set, eval_dataset=val_set, tokenizer=tokenizer, )
    print("Start Training...")
    trainer.train()
    writer.close()

if __name__ == '__main__':
    main()
```

æ—¥å¿—æ‰“å°ï¼š

```
GPU æ˜¯å¦å¯ç”¨Trueï¼Œæœ‰1ä¸ªGPU
Start Load Train and Validation Data...
data load ï¼Œ sizeï¼š 10748
data load ï¼Œ sizeï¼š 1343
Start Training...
Start Training...
11%|â–ˆ         | 428/4029 [04:13<35:35,  1.69it/s]
```

#### 2 linux 3080-20G å•å¡+bsè®¾ç½®ä¸º 1

æ ·æœ¬ä¸€å…± 10748 ä¸ªï¼Œbatch_size ä¸º 8 ï¼Œepoch ä¸º 3 ï¼Œæ‰€ä»¥ 4029 ä¸ª steps ï¼Œä¸€å¼ æ˜¾å¡æ¶ˆè€—æ˜¾å­˜ 6.6 G ï¼Œå¦ä¸€å¼ æ²¡æœ‰ä½¿ç”¨ï¼Œcpu ä½¿ç”¨ 100% å·¦å³ï¼Œå†…å­˜ä½¿ç”¨2.83Gï¼Œé¢„è®¡è€—æ—¶ 60 åˆ†é’Ÿå·¦å³ã€‚

å’Œã€window 4090 å•å¡+bsè®¾ç½®ä¸º 1ã€‘åŒæ ·çš„ä»£ç ï¼Œåªéœ€è¦å°†æ¨¡å‹å’Œæ•°æ®ä½ç½®ç­‰å‚æ•°ä¿®æ”¹ä¸€ä¸‹ï¼Œæ‰§è¡Œå‘½ä»¤ï¼š

```
torchrun --nnodes 1 --nproc_per_node 1 fft_ner_deepspeed.py
```

æ—¥å¿—æ‰“å°ï¼š

```
(torch-2.1-py-3.10) root@autodl-container-6d3046b0a3-7076630c:~/autodl-tmp/finetune-qwen7b# torchrun --nnodes 1 --nproc_per_node 1 fft_ner_deepspeed.py 
GPU æ˜¯å¦å¯ç”¨Trueï¼Œæœ‰2ä¸ªGPU
Start Load Train and Validation Data...
data load ï¼Œ sizeï¼š 10748
data load ï¼Œ sizeï¼š 1343
Start Training...
{'loss': 0.0908, 'grad_norm': 1.5234375, 'learning_rate': 8.802812842254916e-05, 'epoch': 0.74}  
{'eval_loss': 0.07041047513484955, 'eval_runtime': 30.532, 'eval_samples_per_second': 43.987, 'eval_steps_per_second': 43.987, 'epoch': 1.0}                             
4%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                   | 161/4029 [02:25<58:37,  1.10it/s]
```

#### 3 linux 3080-20G åŒå¡+bsè®¾ç½®ä¸º 1

æ ·æœ¬ä¸€å…± 10748 ä¸ªï¼Œbatch_size ä¸º 8 ï¼Œepoch ä¸º 3 ï¼Œä½†æ˜¯å› ä¸ºæ•°æ®å¹¶è¡Œåœ¨ä¸¤å¼ å¡ï¼Œæ‰€ä»¥ 2013 ä¸ª steps ï¼Œæ¶ˆè€—æ˜¾å­˜å‡ä¸º 6.3 G ï¼Œcpu ä½¿ç”¨ 200% å·¦å³ï¼Œå†…å­˜ä½¿ç”¨ 3.75Gï¼Œé¢„è®¡è€—æ—¶ 28m å·¦å³ï¼Œå¾ˆæ˜æ˜¾è¿™é‡Œè¿›è¡Œäº†æ•°æ®å¹¶è¡Œè®­ç»ƒï¼Œå¤§å¤§å‡å°‘äº†è®­ç»ƒæ—¶é—´ï¼Œç›¸æ¯”è¾ƒ ã€ linux 3080-20G å•å¡+bsè®¾ç½®ä¸º 1ã€‘ æ˜æ˜¾å‡å°‘äº†ä¸€åŠçš„è®­ç»ƒæ—¶é—´ ã€‚

å’Œã€window 4090 å•å¡+bsè®¾ç½®ä¸º 1ã€‘åŒæ ·çš„ä»£ç ï¼Œåªéœ€è¦å°†æ¨¡å‹å’Œæ•°æ®ä½ç½®ç­‰å‚æ•°ä¿®æ”¹ä¸€ä¸‹ï¼Œæ‰§è¡Œå‘½ä»¤ï¼š

```
torchrun --nnodes 1 --nproc_per_node 2 fft_ner_deepspeed.py
```

æ—¥å¿—æ‰“å°ï¼š

```
(torch-2.1-py-3.10) root@autodl-container-6d3046b0a3-7076630c:~/autodl-tmp/finetune-qwen# torchrun --nnodes 1 --nproc_per_node 2 fft_ner_deepspeed.py
GPU æ˜¯å¦å¯ç”¨Trueï¼Œæœ‰2ä¸ªGPU
GPU æ˜¯å¦å¯ç”¨Trueï¼Œæœ‰2ä¸ªGPU
Start Load Train and Validation Data...
data load ï¼Œ sizeï¼š 10748
data load ï¼Œ sizeï¼š 1343
[2024-11-15 15:15:18,076] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Start Load Train and Validation Data...
data load ï¼Œ sizeï¼š 10748
data load ï¼Œ sizeï¼š 1343
[2024-11-15 15:15:18,217] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-11-15 15:15:18,828] [INFO] [comm.py:652:init_distributed] cdb=None
Start Training...
[2024-11-15 15:15:18,974] [INFO] [comm.py:652:init_distributed] cdb=None
[2024-11-15 15:15:18,974] [INFO] [comm.py:683:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
 4%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                         | 75/2013 [01:04<27:00,  1.20it/s]
```

#### 4 linux 3080-20G åŒå¡+bs è®¾ç½®32

per_device_train_batch_size=32, per_device_eval_batch_size=32, gradient_accumulation_steps=1, 168/504 [01:59<03:57, 1.42it/s] , æ˜¾å­˜éƒ½ä¸º 16.05G , CPU 200% , å†…å­˜ 3.94G

![](https://p0-xtjj-private.juejin.cn/tos-cn-i-73owjymdk6/14e5cc18f8a34c7dbcebed49d2188aee~tplv-73owjymdk6-jj-mark-v1:0:0:0:0:5o6Y6YeR5oqA5pyv56S-5Yy6IEAg5oiR5piv546L5aSn5L2g5piv6LCB:q75.awebp?policy=eyJ2bSI6MywidWlkIjoiNTM2MjE3NDA1ODk1MTQ5In0%3D&rk3s=e9ecf3d6&x-orig-authkey=f32326d3454f2ac7e96d3d06cdbb035152127018&x-orig-expires=1737616636&x-orig-sign=OV2h0Yd7cF7m2WchAbSYbqyUUqA%3D)

#### 5 linux 3080-20G åŒå¡+bsè®¾ç½®ä¸º1+deepspeed stage 0

æ ·æœ¬ä¸€å…± 10748 ä¸ªï¼Œbatch_size ä¸º 8 ï¼Œepoch ä¸º 3 ï¼Œä½†æ˜¯å› ä¸ºæ•°æ®å¹¶è¡Œåœ¨ä¸¤å¼ å¡ï¼Œæ‰€ä»¥ 2013 ä¸ª steps ï¼Œæ¶ˆè€—æ˜¾å­˜éƒ½ä¸º 6.3G ï¼Œcpu ä½¿ç”¨ 200% ï¼Œå†…å­˜ä½¿ç”¨ 3.79Gï¼Œé¢„è®¡è€—æ—¶ 30åˆ†é’Ÿå·¦å³ã€‚å’Œã€linux 3080-20G åŒå¡+bsè®¾ç½®ä¸º 1 ã€‘ä¸€èŠ‚ä¸­çš„å„é¡¹æŒ‡æ ‡éƒ½å·®ä¸å¤šï¼Œä¹Ÿå°±æ˜¯è¯´ä½¿ç”¨äº† stage 0 å…¶å®å°±æ˜¯å•çº¯çš„å¤šå¡æ•°æ®å¹¶è¡Œè®­ç»ƒã€‚

è®°å¾—è°ƒæ•´ä¸‹é¢ï¼Œè®°å¾—åœ¨ä»£ç ä¸­çš„ TrainingArguments å‚æ•°ä¸­åŠ å…¥ deepspeed

```
deepspeed="/root/autodl-tmp/finetune-qwen/ds_config_zero0.json"
```

ä½¿ç”¨çš„ ds_config_zero0.json é…ç½®æ–‡ä»¶å¦‚ä¸‹ï¼š

{

"train_micro_batch_size_per_gpu": "auto",

"zero_optimization": {

"stage": 0

}

}

æ‰§è¡Œå¦‚ä¸‹å‘½ä»¤è¿›è¡Œè¿è¡Œï¼š

```
torchrun --nnodes 1 --nproc_per_node 2 fft_ner_deepspeed.py
```

æ—¥å¿—æ‰“å°ï¼š

```
(torch-2.1-py-3.10) root@autodl-container-6d3046b0a3-7076630c:~/autodl-tmp/finetune-qwen# torchrun --nnodes 1 --nproc_per_node 2 fft_ner_deepspeed.py
GPU æ˜¯å¦å¯ç”¨Trueï¼Œæœ‰2ä¸ªGPU
------------- {'': 0}
GPU æ˜¯å¦å¯ç”¨Trueï¼Œæœ‰2ä¸ªGPU
------------- {'': 1}
Start Load Train and Validation Data...
data load ï¼Œ sizeï¼š 10748
data load ï¼Œ sizeï¼š 1343
Start Load Train and Validation Data...
data load ï¼Œ sizeï¼š 10748
data load ï¼Œ sizeï¼š 1343
[2024-11-15 15:50:58,492] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-11-15 15:50:58,717] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Start Training...
Start Training...
  0%|                                                                                                                                                              | 0/2013 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
  7%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                | 139/2013 [02:04<27:55,  1.12it/s]
```

ä½†æ˜¯æœ‰ä¸€ä¸ªå¥‡æ€ªçš„ç°è±¡ï¼ŒåŒæ ·çš„é…ç½®å’Œä»£ç ï¼Œæœ‰æ—¶å€™å¯åŠ¨ä¹‹åè€—æ—¶ä¼š1h50må·¦å³ï¼Œæ˜¾å­˜ä¼šä¸Šå‡åˆ° 10.8G ï¼Œæ€€ç–‘æ˜¯ autodl å¹³å°çš„é—®é¢˜ã€‚

#### 6 linux 3080-20G åŒå¡+bsè®¾ç½®ä¸º32+deepspeed stage 0

å¤šå¡ + stage0, per_device_train_batch_size=32, per_device_eval_batch_size=32, gradient_accumulation_steps=1, 140/504 [02:02<05:18, 1.14it/s]ï¼Œæ˜¾å­˜éƒ½ä¸º 18.5G å¯ä»¥çœ‹å‡ºæ¥ GPU ä¸€ç›´å……åˆ†åˆ©ç”¨äº†èµ·æ¥ ï¼Œ CPU 200%ï¼Œå†…å­˜ 3.93G

![](https://p0-xtjj-private.juejin.cn/tos-cn-i-73owjymdk6/0662ad915f8c42f8b6d4a4b2178cd073~tplv-73owjymdk6-jj-mark-v1:0:0:0:0:5o6Y6YeR5oqA5pyv56S-5Yy6IEAg5oiR5piv546L5aSn5L2g5piv6LCB:q75.awebp?policy=eyJ2bSI6MywidWlkIjoiNTM2MjE3NDA1ODk1MTQ5In0%3D&rk3s=e9ecf3d6&x-orig-authkey=f32326d3454f2ac7e96d3d06cdbb035152127018&x-orig-expires=1737616636&x-orig-sign=mAKZhWd4ZfLa%2B1fFNWXb2ijhTRw%3D)

#### 7 linux 3080-20G åŒå¡+bsè®¾ç½®ä¸º1+deepspeed stage 1

  


æ ·æœ¬ä¸€å…± 10748 ä¸ªï¼Œbatch_size ä¸º 8 ï¼Œepoch ä¸º 3 ï¼Œä½†æ˜¯å› ä¸ºæ•°æ®å¹¶è¡Œåœ¨ä¸¤å¼ å¡ï¼Œæ‰€ä»¥ 2013 ä¸ª steps ï¼Œæ¶ˆè€—æ˜¾å­˜ä¸º 8.5G å’Œ 7.4G ï¼Œcpu ä½¿ç”¨ 200% ï¼Œå†…å­˜ä½¿ç”¨ 3.86Gï¼Œé¢„è®¡è€—æ—¶ 1h35m å·¦å³ã€‚

è®°å¾—è°ƒæ•´ä¸‹é¢éƒ¨åˆ†ä»£ç ï¼Œåœ¨è®°å¾—åœ¨ä»£ç ä¸­çš„ TrainingArguments å‚æ•°ä¸­åŠ å…¥ deepspeed

```
deepspeed="/root/autodl-tmp/finetune-qwen/ds_config_zero1.json"
```

ä½¿ç”¨çš„ ds_config_zero1.json é…ç½®æ–‡ä»¶å¦‚ä¸‹ï¼š

{

"train_micro_batch_size_per_gpu": "auto",

"zero_optimization": {

"stage": 1

}

}

æ‰§è¡Œå¦‚ä¸‹å‘½ä»¤è¿›è¡Œè¿è¡Œï¼š

```
torchrun --nnodes 1 --nproc_per_node 2 fft_ner_deepspeed.py
```

æ—¥å¿—æ‰“å°ï¼š

```
(torch-2.1-py-3.10) root@autodl-container-6d3046b0a3-7076630c:~/autodl-tmp/finetune-qwen# torchrun --nnodes 1 --nproc_per_node 2 fft_ner_deepspeed.py
GPU æ˜¯å¦å¯ç”¨Trueï¼Œæœ‰2ä¸ªGPU
------------- {'': 0}
GPU æ˜¯å¦å¯ç”¨Trueï¼Œæœ‰2ä¸ªGPU
------------- {'': 1}
Start Load Train and Validation Data...
data load ï¼Œ sizeï¼š 10748
data load ï¼Œ sizeï¼š 1343
[2024-11-15 15:59:52,232] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Start Load Train and Validation Data...
data load ï¼Œ sizeï¼š 10748
data load ï¼Œ sizeï¼š 1343
[2024-11-15 15:59:52,793] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-11-15 15:59:52,979] [INFO] [comm.py:652:init_distributed] cdb=None
[2024-11-15 15:59:52,980] [INFO] [comm.py:683:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
Start Training...
[2024-11-15 15:59:53,638] [INFO] [comm.py:652:init_distributed] cdb=None
Start Training...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
  0%|                                                                                                                                                              | 0/2013 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
  5%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                | 94/2013 [04:29<1:31:34,  2.86s/it]
```

#### 8 linux 3080-20G åŒå¡+bsè®¾ç½®ä¸º32+deepspeed stage 1

å¤šå¡ + stage1, per_device_train_batch_size=32, per_device_eval_batch_size=32, gradient_accumulation_steps=1, 180/504 [02:48<05:05, 1.06it/s] ï¼Œæ˜¾å­˜éƒ½æ˜¯ 16.2G å¯ä»¥çœ‹å‡ºæ¥ GPU ä¸€ç›´å……åˆ†åˆ©ç”¨äº†èµ·æ¥ ï¼ŒCPU 200% ï¼Œå†…å­˜ 4.03G

![](https://p0-xtjj-private.juejin.cn/tos-cn-i-73owjymdk6/738baef63a15445a9cbc7d78e2196c0c~tplv-73owjymdk6-jj-mark-v1:0:0:0:0:5o6Y6YeR5oqA5pyv56S-5Yy6IEAg5oiR5piv546L5aSn5L2g5piv6LCB:q75.awebp?policy=eyJ2bSI6MywidWlkIjoiNTM2MjE3NDA1ODk1MTQ5In0%3D&rk3s=e9ecf3d6&x-orig-authkey=f32326d3454f2ac7e96d3d06cdbb035152127018&x-orig-expires=1737616636&x-orig-sign=PmSpKg1ohPQ08e%2FhiulNveEmyDo%3D)

#### 9 linux 3080-20G åŒå¡+bsè®¾ç½®ä¸º32+deepspeed stage 2 + no offload

{

"fp16": {

"enabled": "auto",

"loss_scale": 0,

"loss_scale_window": 1000,

"initial_scale_power": 16,

"hysteresis": 2,

"min_loss_scale": 1

},

  


"optimizer": {

"type": "AdamW",

"params": {

"lr": "auto",

"betas": "auto",

"eps": "auto",

"weight_decay": "auto"

}

},

  


"scheduler": {

"type": "WarmupLR",

"params": {

"warmup_min_lr": "auto",

"warmup_max_lr": "auto",

"warmup_num_steps": "auto"

}

},

  


"zero_optimization": {

"stage": 2,

"allgather_partitions": true,

"allgather_bucket_size": 2e8,

"overlap_comm": true,

"reduce_scatter": true,

"reduce_bucket_size": 2e8,

"contiguous_gradients": true

},

  


"gradient_accumulation_steps": "auto",

"gradient_clipping": "auto",

"steps_per_print": 2000,

"train_batch_size": "auto",

"train_micro_batch_size_per_gpu": "auto",

"wall_clock_breakdown": false

}

  


å¤šå¡ + stage 2 + no offload , per_device_train_batch_size=32, per_device_eval_batch_size=32, gradient_accumulation_steps=1, 79/504 [01:03<05:40, 1.25it/s] ï¼Œ 17.45G ã€17.44G å¯ä»¥çœ‹å‡ºæ¥ GPU ä¸€ç›´å……åˆ†åˆ©ç”¨äº†èµ·æ¥ ,CPU 200 , å†…å­˜ 4G ,

![](https://p0-xtjj-private.juejin.cn/tos-cn-i-73owjymdk6/d4de335a87c845f6938661ee913dac86~tplv-73owjymdk6-jj-mark-v1:0:0:0:0:5o6Y6YeR5oqA5pyv56S-5Yy6IEAg5oiR5piv546L5aSn5L2g5piv6LCB:q75.awebp?policy=eyJ2bSI6MywidWlkIjoiNTM2MjE3NDA1ODk1MTQ5In0%3D&rk3s=e9ecf3d6&x-orig-authkey=f32326d3454f2ac7e96d3d06cdbb035152127018&x-orig-expires=1737616636&x-orig-sign=NGAip%2F5APSzg0qD3DfYNEZQMZpQ%3D)

  


æ—¥å¿—ï¼š

(torch-2.1-py-3.10) root@autodl-container-6d3046b0a3-7076630c:~/autodl-tmp/finetune-qwen# torchrun --nnodes 1 --nproc_per_node 2 fft_ner_deepspeed.py

GPU æ˜¯å¦å¯ç”¨Trueï¼Œæœ‰2ä¸ªGPU

Start Load Train and Validation Data...

data load ï¼Œ sizeï¼š 10748

data load ï¼Œ sizeï¼š 1343

/root/miniconda3/envs/torch-2.1-py-3.10/lib/python3.10/site-packages/transformers/training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead

warnings.warn(

[2024-11-15 17:37:33,935] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)

GPU æ˜¯å¦å¯ç”¨Trueï¼Œæœ‰2ä¸ªGPU

Start Load Train and Validation Data...

data load ï¼Œ sizeï¼š 10748

data load ï¼Œ sizeï¼š 1343

/root/miniconda3/envs/torch-2.1-py-3.10/lib/python3.10/site-packages/transformers/training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead

warnings.warn(

[2024-11-15 17:37:34,688] [INFO] [comm.py:652:init_distributed] cdb=None

[2024-11-15 17:37:34,705] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)

/root/autodl-tmp/finetune-qwen/fft_ner_deepspeed.py:65: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.

trainer = Trainer( model=model, args=training_args, train_dataset=training_set, eval_dataset=val_set, tokenizer=tokenizer, )

Start Training...

[2024-11-15 17:37:35,505] [INFO] [comm.py:652:init_distributed] cdb=None

[2024-11-15 17:37:35,505] [INFO] [comm.py:683:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl

/root/autodl-tmp/finetune-qwen/fft_ner_deepspeed.py:65: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.

trainer = Trainer( model=model, args=training_args, train_dataset=training_set, eval_dataset=val_set, tokenizer=tokenizer, )

Start Training...

Using /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...

Creating extension directory /root/.cache/torch_extensions/py310_cu118/fused_adam...

Using /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...

Detected CUDA files, patching ldflags

Emitting ninja build file /root/.cache/torch_extensions/py310_cu118/fused_adam/build.ninja...

Building extension module fused_adam...

Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)

[1/3] /usr/local/cuda/bin/nvcc -DTORCH_EXTENSION_NAME=fused_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"___libstdcpp\" -DPYBIND11_BUILD_ABI=\"_____cxxabi1011\" -I/root/miniconda3/envs/torch-2.1-py-3.10/lib/python3.10/site-packages/deepspeed/ops/csrc/includes -I/root/miniconda3/envs/torch-2.1-py-3.10/lib/python3.10/site-packages/deepspeed/ops/csrc/adam -isystem /root/miniconda3/envs/torch-2.1-py-3.10/lib/python3.10/site-packages/torch/include -isystem /root/miniconda3/envs/torch-2.1-py-3.10/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /root/miniconda3/envs/torch-2.1-py-3.10/lib/python3.10/site-packages/torch/include/TH -isystem /root/miniconda3/envs/torch-2.1-py-3.10/lib/python3.10/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /root/miniconda3/envs/torch-2.1-py-3.10/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS____ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_86,code=compute_86 -gencode=arch=compute_86,code=sm_86 --compiler-options '-fPIC' -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -lineinfo --use_fast_math -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_86,code=compute_86 -DBF16_AVAILABLE -U__CUDA_NO_BFLOAT16_OPERATORS__ -U__CUDA_NO_BFLOAT162_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ -std=c++17 -c /root/miniconda3/envs/torch-2.1-py-3.10/lib/python3.10/site-packages/deepspeed/ops/csrc/adam/multi_tensor_adam.cu -o multi_tensor_adam.cuda.o

[2/3] c++ -MMD -MF fused_adam_frontend.o.d -DTORCH_EXTENSION_NAME=fused_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/root/miniconda3/envs/torch-2.1-py-3.10/lib/python3.10/site-packages/deepspeed/ops/csrc/includes -I/root/miniconda3/envs/torch-2.1-py-3.10/lib/python3.10/site-packages/deepspeed/ops/csrc/adam -isystem /root/miniconda3/envs/torch-2.1-py-3.10/lib/python3.10/site-packages/torch/include -isystem /root/miniconda3/envs/torch-2.1-py-3.10/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /root/miniconda3/envs/torch-2.1-py-3.10/lib/python3.10/site-packages/torch/include/TH -isystem /root/miniconda3/envs/torch-2.1-py-3.10/lib/python3.10/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /root/miniconda3/envs/torch-2.1-py-3.10/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -O3 -std=c++17 -g -Wno-reorder -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DBF16_AVAILABLE -c /root/miniconda3/envs/torch-2.1-py-3.10/lib/python3.10/site-packages/deepspeed/ops/csrc/adam/fused_adam_frontend.cpp -o fused_adam_frontend.o

[3/3] c++ fused_adam_frontend.o multi_tensor_adam.cuda.o -shared -L/root/miniconda3/envs/torch-2.1-py-3.10/lib/python3.10/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o fused_adam.so

Loading extension module fused_adam...

Time to load fused_adam op: 22.77893567085266 seconds

Loading extension module fused_adam...

Time to load fused_adam op: 22.849246740341187 seconds

`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...

0%| | 0/504 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...

{'eval_loss': 0.062203299254179, 'eval_runtime': 3.7647, 'eval_samples_per_second': 356.732, 'eval_steps_per_second': 5.578, 'epoch': 1.0} 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 212/504 [03:04<03:54, 1.25it/s]

  


#### 10 linux 3080-20G åŒå¡+bsè®¾ç½®ä¸º1+deepspeed stage 2 + offload

  


æ ·æœ¬ä¸€å…± 10748 ä¸ªï¼Œbatch_size ä¸º 8 ï¼Œepoch ä¸º 3 ï¼Œä½†æ˜¯å› ä¸ºæ•°æ®å¹¶è¡Œåœ¨ä¸¤å¼ å¡ï¼Œæ‰€ä»¥ 2013 ä¸ª steps ï¼Œæ¶ˆè€—æ˜¾å­˜å„ä¸º 6.2G å’Œ 4.7G ï¼Œcpu ä½¿ç”¨ 200%-1100% æ¥å›è§„å¾‹éœ‡è¡ï¼Œå†…å­˜ä½¿ç”¨ 13.34 Gï¼Œé¢„è®¡è€—æ—¶ 1h40m å·¦å³ã€‚æ˜æ˜¾æ¯” ã€linux 3080-20G åŒå¡ã€‘ä¸€èŠ‚ä¸­çš„è€—æ—¶è¦å¤š 1 å°æ—¶ä»¥ä¸Šï¼Œä¹Ÿå°±æ˜¯è¯´ä½¿ç”¨äº† stage 2 ä¹‹ååè€Œè®­ç»ƒé€Ÿåº¦ä¸‹é™äº†ï¼Œä½†æ˜¯æ˜æ˜¾èƒ½çœ‹å‡ºæ¥æ˜¾å­˜ä½¿ç”¨æ˜æ˜¾é™ä½äº†ï¼Œcpu ä½¿ç”¨ç‡ä¹Ÿä¸Šå»äº†ï¼Œå†…å­˜å› ä¸ºæ¶‰åŠåˆ°è®¡ç®—ä¹Ÿä½¿ç”¨ä¸Šå‡äº†ã€‚

  


[DeepSpeed å­¦ä¹ ç¬”è®°](https://wqw547243068.github.io/deepspeed) ä¸­æœ‰ç±»ä¼¼åœ°åŸå› é˜è¿°ï¼ŒDeepSpeed ä»…é€‚ç”¨äº: æ˜¾å­˜æåº¦çŸ­ç¼ºçš„æƒ…å†µï¼Œä¹Ÿå°±æ˜¯ `batch_size == 1`ä¹Ÿè·‘ä¸äº†ï¼Œç”¨DeepSppedèŠ‚çœä¸‹æ¥çš„æ˜¾å­˜ï¼Œåˆšå¥½å¤Ÿæ”¯æŒæ›´å¤§çš„batch_sizeã€‚å¦åˆ™ï¼Œä½¿ç”¨DeepSpeedåªä¼šå¢åŠ æ—¶é—´å¼€é”€ï¼Œå¹¶æ²¡æœ‰å…¶ä»–ç›Šå¤„ã€‚ stage3 é€Ÿåº¦æå…¶ç¼“æ…¢ã€‚ åŸå…ˆéœ€è¦6hçš„è®­ç»ƒè¿‡ç¨‹ï¼Œç”¨ DeepSpeed stage3ä¹‹åï¼Œè¿è¡Œäº†2å¤©2å¤œï¼Œä¹Ÿæ²¡æœ‰ç»“æŸçš„è¿¹è±¡ã€‚

  


å¦å¤–ç”±äº DeepSpeed é€šè¿‡å ç”¨CPUå†…å­˜æ¥å‡ç¼“GPUçš„å¼€é”€ï¼Œå½“ç³»ç»ŸCPUä¸å¤Ÿçš„æ—¶å€™ï¼ŒDeepSpeed è¿›ç¨‹å°±ä¼šè‡ªåŠ¨è¢«ç³»ç»Ÿåœæ­¢ï¼Œé€ æˆæ²¡æœ‰ä»»ä½•æŠ¥é”™ï¼ŒDeepSpeedæ— æ³•å¯åŠ¨çš„ç°è±¡ã€‚å»ºè®®ç”¨estimationä¼°è®¡ä¸€ä¸‹CPUå†…å­˜å ç”¨ï¼Œç„¶åç”¨`free -h`æŸ¥çœ‹ä¸€ä¸‹æœºå™¨çš„CPUå†…å­˜ç©ºä½™é‡ï¼Œæ¥åˆ¤æ–­èƒ½å¦ä½¿ç”¨DeepSpeedã€‚

  


å’Œã€window 4090 å•å¡ã€‘åŒæ ·çš„ä»£ç ï¼Œåªéœ€è¦å°†æ¨¡å‹å’Œæ•°æ®ä½ç½®ç­‰å‚æ•°ä¿®æ”¹ä¸€ä¸‹ï¼Œå¦å¤–è¿˜éœ€è¦åœ¨ TrainingArguments ä¸­åŠ å…¥å‚æ•° deepspeed="/root/autodl-tmp/finetune-qwen7b/ds_config_zero2.json " ï¼Œå†æ‰§è¡Œå‘½ä»¤ï¼š

```
torchrun --nnodes 1 --nproc_per_node 2 fft_ner_deepspeed.py
```

æ—¥å¿—æ‰“å°ï¼š

```
(torch-2.1-py-3.10) root@autodl-container-6d3046b0a3-7076630c:~/autodl-tmp/finetune-qwen7b# torchrun --nnodes 1 --nproc_per_node 2 fft_ner_deepspeed.py
GPU æ˜¯å¦å¯ç”¨Trueï¼Œæœ‰2ä¸ªGPU
GPU æ˜¯å¦å¯ç”¨Trueï¼Œæœ‰2ä¸ªGPU
Start Load Train and Validation Data...
data load ï¼Œ sizeï¼š 10748
data load ï¼Œ sizeï¼š 1343
/root/miniconda3/envs/torch-2.1-py-3.10/lib/python3.10/site-packages/transformers/training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
Start Load Train and Validation Data...
[2024-11-08 14:36:45,196] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
data load ï¼Œ sizeï¼š 10748
data load ï¼Œ sizeï¼š 1343
/root/miniconda3/envs/torch-2.1-py-3.10/lib/python3.10/site-packages/transformers/training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
[2024-11-08 14:36:45,335] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-11-08 14:36:46,025] [INFO] [comm.py:652:init_distributed] cdb=None
[2024-11-08 14:36:46,026] [INFO] [comm.py:683:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
/root/autodl-tmp/finetune-qwen7b/fft_ner_deepspeed.py:65: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer( model=model, args=training_args,  train_dataset=training_set, eval_dataset=val_set, tokenizer=tokenizer, )
Start Training...
[2024-11-08 14:36:46,163] [INFO] [comm.py:652:init_distributed] cdb=None
/root/autodl-tmp/finetune-qwen7b/fft_ner_deepspeed.py:65: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer( model=model, args=training_args,  train_dataset=training_set, eval_dataset=val_set, tokenizer=tokenizer, )
Start Training...
Using /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...
Creating extension directory /root/.cache/torch_extensions/py310_cu118/cpu_adam...
Using /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...
Emitting ninja build file /root/.cache/torch_extensions/py310_cu118/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
[1/3] c++ -MMD -MF cpu_adam_impl.o.d -DTORCH_EXTENSION_NAME=cpu_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE="_gcc" -DPYBIND11_STDLIB="_libstdcpp" -DPYBIND11_BUILD_ABI="_cxxabi1011" -I/root/miniconda3/envs/torch-2.1-py-3.10/lib/python3.10/site-packages/deepspeed/ops/csrc/includes -isystem /root/miniconda3/envs/torch-2.1-py-3.10/lib/python3.10/site-packages/torch/include -isystem /root/miniconda3/envs/torch-2.1-py-3.10/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /root/miniconda3/envs/torch-2.1-py-3.10/lib/python3.10/site-packages/torch/include/TH -isystem /root/miniconda3/envs/torch-2.1-py-3.10/lib/python3.10/site-packages/torch/include/THC -isystem /root/miniconda3/envs/torch-2.1-py-3.10/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -O3 -std=c++17 -g -Wno-reorder -L/usr/local/cuda/lib64 -lcudart -lcublas -g -march=native -fopenmp -D__AVX512__ -D__ENABLE_CUDA__ -DBF16_AVAILABLE -c /root/miniconda3/envs/torch-2.1-py-3.10/lib/python3.10/site-packages/deepspeed/ops/csrc/adam/cpu_adam_impl.cpp -o cpu_adam_impl.o 
[2/3] c++ -MMD -MF cpu_adam.o.d -DTORCH_EXTENSION_NAME=cpu_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE="_gcc" -DPYBIND11_STDLIB="_libstdcpp" -DPYBIND11_BUILD_ABI="_cxxabi1011" -I/root/miniconda3/envs/torch-2.1-py-3.10/lib/python3.10/site-packages/deepspeed/ops/csrc/includes -isystem /root/miniconda3/envs/torch-2.1-py-3.10/lib/python3.10/site-packages/torch/include -isystem /root/miniconda3/envs/torch-2.1-py-3.10/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /root/miniconda3/envs/torch-2.1-py-3.10/lib/python3.10/site-packages/torch/include/TH -isystem /root/miniconda3/envs/torch-2.1-py-3.10/lib/python3.10/site-packages/torch/include/THC -isystem /root/miniconda3/envs/torch-2.1-py-3.10/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -O3 -std=c++17 -g -Wno-reorder -L/usr/local/cuda/lib64 -lcudart -lcublas -g -march=native -fopenmp -D__AVX512__ -D__ENABLE_CUDA__ -DBF16_AVAILABLE -c /root/miniconda3/envs/torch-2.1-py-3.10/lib/python3.10/site-packages/deepspeed/ops/csrc/adam/cpu_adam.cpp -o cpu_adam.o 
[3/3] c++ cpu_adam.o cpu_adam_impl.o -shared -lcurand -L/usr/local/cuda/lib64 -L/root/miniconda3/envs/torch-2.1-py-3.10/lib/python3.10/site-packages/torch/lib -lc10 -ltorch_cpu -ltorch -ltorch_python -o cpu_adam.so
Loading extension module cpu_adam...
Time to load cpu_adam op: 28.95137596130371 seconds
Loading extension module cpu_adam...
Time to load cpu_adam op: 29.05465316772461 seconds
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
  0%|                                                                                                                                                 | 0/2013 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
  6%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                    | 118/2013 [06:03<1:36:57,  3.07s/it]
```

#### 11 linux 3080-20G åŒå¡+bsè®¾ç½®ä¸º32+deepspeed stage 2 + offload

å¤šå¡ + stage 2 + offload ,per_device_train_batch_size=32, per_device_eval_batch_size=32, gradient_accumulation_steps=1, 127/504 [03:08<09:24, 1.50s/it] ï¼Œ æ˜¾å­˜ 14.0G ã€12.5G å¯ä»¥çœ‹å‡ºæ¥ GPU ä¸€ç›´åœ¨éœ‡è¡ï¼Œæ²¡æœ‰å……åˆ†åˆ©ç”¨èµ·æ¥ ,CPU 200%-1100% , å†…å­˜ 12.66G ,

![](https://p0-xtjj-private.juejin.cn/tos-cn-i-73owjymdk6/46029a520f664a1b8eb802f601aa6cfd~tplv-73owjymdk6-jj-mark-v1:0:0:0:0:5o6Y6YeR5oqA5pyv56S-5Yy6IEAg5oiR5piv546L5aSn5L2g5piv6LCB:q75.awebp?policy=eyJ2bSI6MywidWlkIjoiNTM2MjE3NDA1ODk1MTQ5In0%3D&rk3s=e9ecf3d6&x-orig-authkey=f32326d3454f2ac7e96d3d06cdbb035152127018&x-orig-expires=1737616637&x-orig-sign=R3HhCPqXCN4U7i3snECXNDUkyG8%3D)

#### 12 linux 3080-20G åŒå¡+bsè®¾ç½®ä¸º32+deepspeed stage 3 + no offload

{

"fp16": {

"enabled": "auto",

"loss_scale": 0,

"loss_scale_window": 1000,

"initial_scale_power": 16,

"hysteresis": 2,

"min_loss_scale": 1

},

  


"optimizer": {

"type": "AdamW",

"params": {

"lr": "auto",

"betas": "auto",

"eps": "auto",

"weight_decay": "auto"

}

},

  


"scheduler": {

"type": "WarmupLR",

"params": {

"warmup_min_lr": "auto",

"warmup_max_lr": "auto",

"warmup_num_steps": "auto"

}

},

  


"zero_optimization": {

"stage": 3,

"overlap_comm": true,

"contiguous_gradients": true,

"sub_group_size": 1e9,

"reduce_bucket_size": "auto",

"stage3_prefetch_bucket_size": "auto",

"stage3_param_persistence_threshold": "auto",

"stage3_max_live_parameters": 1e9,

"stage3_max_reuse_distance": 1e9,

"stage3_gather_16bit_weights_on_model_save": true

},

  


"gradient_accumulation_steps": "auto",

"gradient_clipping": "auto",

"steps_per_print": 2000,

"train_batch_size": "auto",

"train_micro_batch_size_per_gpu": "auto",

"wall_clock_breakdown": false

}

å¤šå¡ + stage 3 + no offload , per_device_train_batch_size=32, per_device_eval_batch_size=32, gradient_accumulation_steps=1, 178/504 [02:33<05:02, 1.08it/s] , éƒ½æ˜¯ 17.19G å¯ä»¥çœ‹å‡ºæ¥ GPU ä¸€ç›´å……åˆ†åˆ©ç”¨äº†èµ·æ¥ , CPU 200% , å†…å­˜ 3.99 G

![](https://p0-xtjj-private.juejin.cn/tos-cn-i-73owjymdk6/cea1e726b99443f6af85ae0b40b45e36~tplv-73owjymdk6-jj-mark-v1:0:0:0:0:5o6Y6YeR5oqA5pyv56S-5Yy6IEAg5oiR5piv546L5aSn5L2g5piv6LCB:q75.awebp?policy=eyJ2bSI6MywidWlkIjoiNTM2MjE3NDA1ODk1MTQ5In0%3D&rk3s=e9ecf3d6&x-orig-authkey=f32326d3454f2ac7e96d3d06cdbb035152127018&x-orig-expires=1737616636&x-orig-sign=JlKrjze0qVO4BeCGiEGEcDPiW48%3D)

#### 13 linux 3080-20G åŒå¡+bsè®¾ç½®ä¸º1+deepspeed stage 3 + offload

  


æ ·æœ¬ä¸€å…± 10748 ä¸ªï¼Œbatch_size ä¸º 8 ï¼Œepoch ä¸º 3 ï¼Œä½†æ˜¯å› ä¸ºæ•°æ®å¹¶è¡Œåœ¨ä¸¤å¼ å¡ï¼Œæ‰€ä»¥ 2013 ä¸ª steps ï¼Œæ¶ˆè€—æ˜¾å­˜å„ä¸º 3.6G å’Œ 3.6G ï¼Œcpu ä½¿ç”¨ 200%-2200% æ¥å›è¾ƒä¸ºè§„å¾‹åœ°éœ‡è¡ï¼Œå†…å­˜ä½¿ç”¨ 13.71 Gï¼Œé¢„è®¡è€—æ—¶ 3h30m å·¦å³ã€‚æ˜æ˜¾æ¯” ã€linux 3080-20G åŒå¡ã€‘ä¸€èŠ‚ä¸­çš„è€—æ—¶è¦å¤š 2 å°æ—¶å·¦å³ï¼Œä¹Ÿå°±æ˜¯è¯´ä½¿ç”¨äº† stage 3 ä¹‹ååè€Œè®­ç»ƒé€Ÿåº¦æ›´æ…¢äº†ï¼Œä½†æ˜¯æ˜æ˜¾èƒ½çœ‹å‡ºæ¥æ˜¾å­˜ä½¿ç”¨æ˜æ˜¾é™ä½äº†ï¼Œcpu ä½¿ç”¨ç‡ä¹Ÿä¸Šå»äº†ï¼Œå†…å­˜å› ä¸ºæ¶‰åŠåˆ°è®¡ç®—ä¹Ÿä½¿ç”¨ä¸Šå‡äº†ã€‚

  


[å®˜æ–¹](https://huggingface.co/docs/transformers/zh/main_classes/deepspeed#zero-2-%E5%92%8C-zero-3-%E6%80%A7%E8%83%BD%E5%AF%B9%E6%AF%94)ä¹Ÿæœ‰è§£é‡Šï¼Œå¦‚æœå…¶ä»–ä¸€åˆ‡éƒ½é…ç½®ç›¸åŒï¼ŒZeRO-3 å¯èƒ½æ¯” ZeRO-2 æ…¢ï¼Œå› ä¸ºå‰è€…é™¤äº† ZeRO-2 çš„æ“ä½œå¤–ï¼Œè¿˜å¿…é¡»æ”¶é›†æ¨¡å‹æƒé‡ã€‚å¦‚æœ ZeRO-2 æ»¡è¶³æ‚¨çš„éœ€æ±‚ï¼Œè€Œä¸”æ‚¨ä¸éœ€è¦æ‰©å±•åˆ°å‡ ä¸ª GPU ä»¥ä¸Šï¼Œé‚£ä¹ˆæ‚¨å¯ä»¥é€‰æ‹©ç»§ç»­ä½¿ç”¨å®ƒã€‚é‡è¦çš„æ˜¯è¦ç†è§£ï¼ŒZeRO-3 ä»¥é€Ÿåº¦ä¸ºä»£ä»·å®ç°äº†æ›´é«˜çš„å¯æ‰©å±•æ€§ã€‚

  


å’Œã€window 4090 å•å¡ã€‘åŒæ ·çš„ä»£ç ï¼Œåªéœ€è¦å°†æ¨¡å‹å’Œæ•°æ®ä½ç½®ç­‰å‚æ•°ä¿®æ”¹ä¸€ä¸‹ï¼Œå¦å¤–è¿˜éœ€è¦åœ¨ TrainingArguments ä¸­åŠ å…¥å‚æ•° deepspeed="/root/autodl-tmp/finetune-qwen7b/[ds_config_zero3.json](https://huggingface.co/docs/transformers/zh/main_classes/deepspeed#zero-3-%E7%A4%BA%E4%BE%8B)" ï¼Œå†æ‰§è¡Œå‘½ä»¤ï¼š

```
torchrun --nnodes 1 --nproc_per_node 2 fft_ner_deepspeed.py
```

æ—¥å¿—æ‰“å°ï¼š

  


```
(torch-2.1-py-3.10) root@autodl-container-6d3046b0a3-7076630c:~/autodl-tmp/finetune-qwen7b# torchrun --nnodes 1 --nproc_per_node 2 fft_ner_deepspeed.py
GPU æ˜¯å¦å¯ç”¨Trueï¼Œæœ‰2ä¸ªGPU
Start Load Train and Validation Data...
data load ï¼Œ sizeï¼š 10748
data load ï¼Œ sizeï¼š 1343
/root/miniconda3/envs/torch-2.1-py-3.10/lib/python3.10/site-packages/transformers/training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
[2024-11-08 14:46:28,915] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
GPU æ˜¯å¦å¯ç”¨Trueï¼Œæœ‰2ä¸ªGPU
Start Load Train and Validation Data...
data load ï¼Œ sizeï¼š 10748
data load ï¼Œ sizeï¼š 1343
/root/miniconda3/envs/torch-2.1-py-3.10/lib/python3.10/site-packages/transformers/training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
[2024-11-08 14:46:29,659] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-11-08 14:46:29,746] [INFO] [comm.py:652:init_distributed] cdb=None
/root/autodl-tmp/finetune-qwen7b/fft_ner_deepspeed.py:65: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer( model=model, args=training_args,  train_dataset=training_set, eval_dataset=val_set, tokenizer=tokenizer, )
Start Training...
[2024-11-08 14:46:30,460] [INFO] [comm.py:652:init_distributed] cdb=None
[2024-11-08 14:46:30,460] [INFO] [comm.py:683:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
/root/autodl-tmp/finetune-qwen7b/fft_ner_deepspeed.py:65: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer( model=model, args=training_args,  train_dataset=training_set, eval_dataset=val_set, tokenizer=tokenizer, )
Start Training...
Using /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...
Using /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...
Emitting ninja build file /root/.cache/torch_extensions/py310_cu118/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.4738075733184814 seconds
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.516721725463867 seconds
Parameter Offload: Total persistent parameters: 71552 in 121 params
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
   2%|â–ˆâ–ˆâ–                            | 36/2013 [03:56<3:25:16,  6.23s/it]
```

#### 14 linux 3080-20G åŒå¡+bsè®¾ç½®ä¸º32+deepspeed stage 3 + offload

å¤šå¡ + stage 3 + offload , per_device_train_batch_size=32, per_device_eval_batch_size=32, gradient_accumulation_steps=1, 223/504 [08:00<09:29, 2.02s/it] , æ˜¾å­˜ 13.43G ã€13.5G å¯ä»¥çœ‹å‡ºæ¥ GPU ä¸€ç›´åœ¨éœ‡è¡ï¼Œæ²¡æœ‰å……åˆ†åˆ©ç”¨èµ·æ¥ , CPU 1600%-2500% , å†…å­˜ 15.62G

![](https://p0-xtjj-private.juejin.cn/tos-cn-i-73owjymdk6/86e65474507d4bf1b0b413a759b43a90~tplv-73owjymdk6-jj-mark-v1:0:0:0:0:5o6Y6YeR5oqA5pyv56S-5Yy6IEAg5oiR5piv546L5aSn5L2g5piv6LCB:q75.awebp?policy=eyJ2bSI6MywidWlkIjoiNTM2MjE3NDA1ODk1MTQ5In0%3D&rk3s=e9ecf3d6&x-orig-authkey=f32326d3454f2ac7e96d3d06cdbb035152127018&x-orig-expires=1737616637&x-orig-sign=d58o95MoqnEWFDsmiDu%2BMDZZwYM%3D)

  


#### 15 linux 3080-20G åŒå¡+accelerate

`Trainer` ç±»ç¡®å®å·²ç»é›†æˆäº† `accelerate`ï¼Œå› æ­¤ï¼Œä½ ä¸éœ€è¦æ‰‹åŠ¨è°ƒç”¨ `accelerate` æ¥åŠ é€Ÿè®­ç»ƒã€‚åªè¦ä½ ä½¿ç”¨äº† `Trainer` å¹¶ä¸”é…ç½®äº†å¦‚ `fp16=True` æˆ–è€…å¤šGPUè®­ç»ƒç­‰é€‰é¡¹ï¼Œ`Trainer` ä¼šè‡ªåŠ¨ä½¿ç”¨ `accelerate` æ¥åŠ é€Ÿè®­ç»ƒè¿‡ç¨‹ã€‚å¦‚æœç¡®å®æƒ³è®¾ç½®å¯ä»¥åœ¨TrainingArguments ä¸­ä¼ å…¥ accelerator_config å‚æ•°ï¼Œå¯ä»¥æ˜¯ json æ–‡ä»¶è·¯å¾„ï¼Œå¯ä»¥æ˜¯å­—å…¸ï¼Œå¯ä»¥æ˜¯ AcceleratorConfig ç±»çš„å®ä¾‹ã€‚

#### æ€»ç»“

1.  å•çº¯çš„åŒå¡çš„æ•°æ®å¹¶è¡Œæ˜æ˜¾æ¯”å•å¡è®­ç»ƒé€Ÿåº¦å¿«ä¸€å€å·¦å³ã€‚ï¼ˆè§2å’Œ3ï¼‰
1.  æ˜¾å­˜å…è®¸çš„æƒ…å†µä¸‹ï¼Œ bs è¶Šå¤§ï¼Œè®­ç»ƒè¶Šå¿«ã€‚ï¼ˆè§3å’Œ4ï¼‰
1.  ä½¿ç”¨ã€å¤šå¡+stage0ã€‘å’Œå•çº¯çš„å¤šå¡æ•°æ®å¹¶è¡Œä¸€æ ·å¾—æ•ˆæœã€‚ï¼ˆè§3å’Œ5ã€è§4å’Œ6ï¼‰
1.  åœ¨è®¾ç½® bs è¶³å¤Ÿå¤§å¾—æ—¶å€™ï¼Œã€å¤šå¡+stage1ã€‘æ¯”ã€å¤šå¡+stage0ã€‘æ¶ˆè€—æ˜¾å­˜æ˜æ˜¾ä¸¤å¼ å¡å„èŠ‚çœ2Gå·¦å³ï¼Œè€—æ—¶ç•¥æ…¢ä¸€ç‚¹ä½†æ˜¯ç›¸å·®ä¸å¤šã€‚ï¼ˆè§6å’Œ8ï¼‰
1.  åœ¨ä½¿ç”¨ deepspeed æ¡†æ¶çš„æ—¶å€™ï¼Œå½“ per_device_train_batch_size è®¾ç½®ä¸º 1 çš„æ—¶å€™ï¼ŒåŸºæœ¬ä¸Šæ•ˆæœå¾ˆå·®ï¼Œå¾ˆéš¾å‘æŒ¥å‡ºèŠ‚çº¦æ˜¾å­˜çš„æ•ˆæœï¼Œéœ€è¦è°ƒå¤§æ‰è¡Œã€‚
1.  åœ¨æ˜¾å­˜å……è¶³çš„æƒ…å†µä¸‹ï¼Œå°½é‡ä¸è¦ä½¿ç”¨ offload ï¼Œå°½ç®¡æ˜¾å­˜æœ‰æ˜æ˜¾çš„èŠ‚çœï¼Œ ä½†æ˜¯ä¼šå› ä¸ºé€šä¿¡æˆæœ¬æ‹–æ…¢è®­ç»ƒé€Ÿåº¦ï¼Œå¢åŠ ä¸€å€ä»¥ä¸Šå¾—è€—æ—¶ï¼ŒCPU ä¹Ÿä¼šéœ‡è¡å¾—å‰å®³ï¼Œå†…å­˜ä¼šè¢«å¤§é‡å ç”¨ã€‚ï¼ˆè§9å’Œ11ã€12å’Œ14ï¼‰
1.  åŒæ ·çš„é…ç½®ä¸‹ï¼Œåœ¨ä½¿ç”¨äº†å¤šå¡ + stage 0-3 å„ä¸ªé…ç½®è¿›è¡Œè®­ç»ƒï¼Œç†è®ºä¸Šæ¶ˆè€—æ˜¾å­˜ä¼šæ˜æ˜¾é™ä½ï¼Œä½†æ˜¯stage2/3 æ˜æ˜¾æ˜¾å­˜ä½¿ç”¨ä¸Šå‡ï¼Œå¯èƒ½çš„åŸå› æ˜¯è¶Šå¤æ‚çš„åˆ†å¸ƒå¼è®­ç»ƒä¼˜åŒ–æ–¹å¼ï¼Œç®¡ç†è¶Šå¤æ‚ï¼Œé€šä¿¡è¶Šå¤šï¼Œå¼€é”€è¶Šå¤§ã€‚ï¼ˆè§6å’Œ8å’Œ9å’Œ12ï¼‰
 
![image.png](https://p0-xtjj-private.juejin.cn/tos-cn-i-73owjymdk6/9d99f3282e564c36ad68b65b19d5ff82~tplv-73owjymdk6-jj-mark-v1:0:0:0:0:5o6Y6YeR5oqA5pyv56S-5Yy6IEAg5oiR5piv546L5aSn5L2g5piv6LCB:q75.awebp?policy=eyJ2bSI6MywidWlkIjoiNTM2MjE3NDA1ODk1MTQ5In0%3D&rk3s=e9ecf3d6&x-orig-authkey=f32326d3454f2ac7e96d3d06cdbb035152127018&x-orig-expires=1737615214&x-orig-sign=D%2Fbw5s0Zkm726cAYppeTkvlK%2BgY%3D)