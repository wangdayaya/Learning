 linux è·‘é€šå¤šå¡å¾®è°ƒ llama å’Œ qwen2.5-7B
 
 # é€‰å¡å’Œé•œåƒ
 
 
## å¡
 æˆ‘è¿™é‡Œé€‰æ‹©çš„æ˜¯4090å¡ï¼Œå…·ä½“ç®—åŠ›è‡ªå·±æŸ¥çœ‹å³å¯ã€‚

 
![image.png](https://p26-juejin-sign.byteimg.com/tos-cn-i-k3u1fbpfcp/94d942d160334816a364d7f20670ad67~tplv-k3u1fbpfcp-jj-mark-v1:0:0:0:0:5o6Y6YeR5oqA5pyv56S-5Yy6:q75.awebp?rk3s=bb34011d&x-expires=1731132075&x-signature=XM8nBMy3lZug1%2B5%2FjfvOt1hZsuE%3D)

 ## é•œåƒ
 
 é€‰æ‹© autoDL è‡ªå¸¦çš„é•œåƒï¼Œæ–¹ä¾¿ä¸€äº›ã€‚
 
![image.png](https://p26-juejin-sign.byteimg.com/tos-cn-i-k3u1fbpfcp/7c734bec79894c54afe49f7c43e1e317~tplv-k3u1fbpfcp-jj-mark-v1:0:0:0:0:5o6Y6YeR5oqA5pyv56S-5Yy6:q75.awebp?rk3s=bb34011d&x-expires=1731132075&x-signature=vAN9lKUvPv7HU%2B5e%2FlayI2%2FBiUM%3D)

å¦‚æœä½ æœ‰è‡ªå·±çš„é•œåƒï¼Œä¹Ÿå¯ä»¥ç”¨è‡ªå·±çš„é•œåƒåˆ›å»ºæ–°çš„å®ä¾‹ã€‚
 
 # å‡†å¤‡å·¥ä½œ
 
 å¯ä»¥å…ˆé€‰æ‹©**æ— å¡æ¨¡å‹**è¿›è¡Œç¯å¢ƒé…ç½®ï¼Œæ¯”è¾ƒçœé’±ä¸€ç‚¹ï¼Œç­‰éœ€è¦è·‘æ¨¡å‹çš„æ—¶å€™å†ç”¨å¡ã€‚
 
 
 ## è™šæ‹Ÿç¯å¢ƒ
 
- åˆ›å»ºï¼šconda create --name torch-2.1-py-3.10 python=3.10  
- è¿›å…¥ï¼šconda activate torch-2.1-py-3.10ï¼Œå¦‚æœæŠ¥é”™å¦‚ä¸‹ï¼Œåªéœ€è¦å½»åº•å…³é—­å½“å‰ shell ï¼Œé‡å¯ä¹‹åå†æ‰§è¡Œå‘½ä»¤å³å¯ã€‚

        conda activate torch-2.1-py-3.10 

        CommandNotFoundError: Your shell has not been properly configured to use 'conda activate'.
        To initialize your shell, run

            $ conda init <SHELL_NAME>

        Currently supported shells are:
          - bash
          - fish
          - tcsh
          - xonsh
          - zsh
          - powershell

        See 'conda init --help' for more information and options.

        IMPORTANT: You may need to close and restart your shell after running 'conda init'.
  
  ç„¶åå¼€å§‹å®‰è£…ç›®æ ‡ç‰ˆæœ¬çš„ pytorch å’Œå„ç§åº“ï¼Œå¯èƒ½ä¼šæœ‰ç‰ˆæœ¬å†²çªï¼Œä½†æ˜¯ä¸€ä¸ªä¸€ä¸ªæ’æŸ¥è§£å†³å³å¯ã€‚ä¸ç®¡æœ‰æ²¡æœ‰æŠ¥é”™å†²çªï¼Œåªè¦ pip list çœ‹æœ‰æˆåŠŸè£…ä¸Šå³å¯ã€‚
  
```
-   transformers==4.46.0
-   optree==0.13.0
-   deepspeed==0.15.3
-   ml_dtypes==0.5.0(å¯é€‰)
-   tf_keras==2.17.0(å¯é€‰)
-   peft==0.7.1
-   accelerate==1.0.1
-   bitsandbytes==0.44.1
-   torch==2.1.2+cu121
-   torchaudio==2.1.2+cu121
-   torchkeras==3.9.9(å¯é€‰)
-   torchvision==0.16.2+cu121
-   tensorboard==2.18.0
-   numpy==1.26.4
-   pandas==2.0.3
-   datasets==2.18.0
-   tokenizers==0.20.1
-   xformers==0.0.28.post2
-   dataclasses-json==0.6.4
```
  
 - å®‰è£… flash-attn ï¼Œæˆ‘ä¸‹è½½çš„æ˜¯ flash_attn-2.6.3+cu118torch2.1cxx11abiFALSE-cp310-cp310-linux_x86_64.whl 
 - ï¼ˆå¯é€‰ï¼Œåªè¦å¤„ç†é‡åŒ–æ‰éœ€è¦ï¼‰æ‹‰ llama.cpp ä»“åº“ï¼Œç„¶åè¿›å…¥åˆ°æ–‡ä»¶ç›®å½•ä¹‹ä¸­è¿›è¡Œ make ç¼–è¯‘ã€‚ä¼šæ‰“å°å¦‚ä¸‹ä¿¡æ¯ï¼š
```
I ccache not found. Consider installing it for faster compilation.
I llama.cpp build info: 
I UNAME_S:   Linux
I UNAME_P:   x86_64
I UNAME_M:   x86_64
I CFLAGS:    -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  -std=c11   -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -fopenmp -Wdouble-promotion 
I CXXFLAGS:  -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX 
I NVCCFLAGS: -std=c++11 -O3 -g 
I LDFLAGS:    
I CC:        cc (Ubuntu 11.3.0-1ubuntu1~22.04) 11.3.0
I CXX:       c++ (Ubuntu 11.3.0-1ubuntu1~22.04) 11.3.0

c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  -c ggml/src/llamafile/sgemm.cpp -o ggml/src/llamafile/sgemm.o
c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  -c ggml/src/ggml-amx.cpp -o ggml/src/ggml-amx.o
...çœç•¥
c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  ggml/src/llamafile/sgemm.o ggml/src/ggml-amx.o ggml/src/ggml-amx/mmq.o ggml/src/ggml.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-quants.o ggml/src/ggml-aarch64.o src/llama.o src/llama-vocab.o src/llama-grammar.o src/llama-sampling.o src/unicode.o src/unicode-data.o common/common.o common/arg.o common/log.o common/console.o common/ngram-cache.o common/sampling.o common/train.o common/build-info.o common/json-schema-to-grammar.o examples/main/main.o -o llama-cli  

====  Run ./llama-cli -h for help.  ====

c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  -c examples/convert-llama2c-to-ggml/convert-llama2c-to-ggml.cpp -o examples/convert-llama2c-to-ggml/convert-llama2c-to-ggml.o
...çœç•¥
NOTICE: The 'main' binary is deprecated. Please use 'llama-cli' instead.
c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  examples/deprecation-warning/deprecation-warning.o -o server  
```

## æŠ¥è­¦

    use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False

å¯èƒ½è¦åŠ è½½åŸºåº•æ¨¡å‹åè®¾ç½®`model.config.use_cache = False`ã€‚ä¸çŸ¥é“æ˜¯å¦å¯è¡Œã€‚

## æŠ¥é”™
å®‰è£… deepspeed==0.15.3 ä¹‹åè¿è¡Œå¯èƒ½æŠ¥é”™ï¼Œ

    AttributeError: module 'torch.compiler' has no attribute 'is_compiling'
## å…¨å‚æ•°å¾®è°ƒ qwen2.5-0.5B

#### window 4090 å•å¡
æ ·æœ¬ä¸€å…± 10748 ä¸ªï¼Œbatch_size ä¸º 8 ï¼Œepoch ä¸º 3 ï¼Œæ‰€ä»¥ 4029 ä¸ª steps ï¼Œæ¶ˆè€—æ˜¾å­˜ 5.7 G ï¼ŒCPU 3 ä¸ªæ ¸å¿ƒè·‘æ»¡äº†ï¼Œé¢„è®¡è€—æ—¶ 40 åˆ†é’Ÿå·¦å³ã€‚


ä»£ç å¦‚ä¸‹:
```
import json
import os
import numpy as np
import torch
from torch.utils.data import Dataset
from torch.utils.tensorboard import SummaryWriter
from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments,DataCollatorForSeq2Seq

class NerDataset(Dataset):
    def __init__(self, data_path, tokenizer, max_source_length, max_target_length) -> None:
        super().__init__()
        self.tokenizer = tokenizer
        self.max_source_length = max_source_length
        self.max_target_length = max_target_length
        self.max_seq_length = self.max_source_length + self.max_target_length
        self.data = []
        if data_path:
            with open(data_path, "r", encoding='utf-8') as f:
                for line in f:
                    if not line or line == "":
                        continue
                    json_line = json.loads(line)
                    text = json_line["text"]
                    label = json_line["label"]
                    label = json.dumps(label, ensure_ascii=False)
                    self.data.append({ "text": text,  "label": label })
        print("data load ï¼Œ sizeï¼š", len(self.data))

    def preprocess(self, text, label):
        messages = [ {"role": "system", "content": "ä½ çš„ä»»åŠ¡æ˜¯åšNerä»»åŠ¡æå–, æ ¹æ®ç”¨æˆ·è¾“å…¥æå–å‡ºå®Œæ•´çš„å®ä½“ä¿¡æ¯, å¹¶ä»¥JSONæ ¼å¼è¾“å‡ºã€‚"},  {"role": "user", "content": text} ]
        prompt = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        instruction = self.tokenizer(prompt, add_special_tokens=False, max_length=self.max_source_length, padding="max_length", pad_to_max_length=True, truncation=True)
        response = self.tokenizer(label, add_special_tokens=False, max_length=self.max_target_length, padding="max_length", pad_to_max_length=True, truncation=True)
        input_ids = instruction["input_ids"] + response["input_ids"] + [self.tokenizer.pad_token_id]
        attention_mask = (instruction["attention_mask"] + response["attention_mask"] + [1])
        labels = [-100] * len(instruction["input_ids"]) + response["input_ids"] + [self.tokenizer.pad_token_id]
        return input_ids, attention_mask, labels

    def __getitem__(self, index):
        item_data = self.data[index]
        input_ids, attention_mask, labels = self.preprocess(**item_data)
        return { "input_ids": torch.LongTensor(np.array(input_ids)), "attention_mask": torch.LongTensor(np.array(attention_mask)),  "labels": torch.LongTensor(np.array(labels))}

    def __len__(self):
        return len(self.data)

def main():
    print(f"GPU æ˜¯å¦å¯ç”¨{torch.cuda.is_available()}ï¼Œæœ‰{torch.cuda.device_count()}ä¸ªGPU")
    model_name = "/root/autodl-tmp/Qwen2.5-0.5B-Instruct"
    train_json_path = "/root/autodl-tmp/finetune-qwen7b/data/ner/train.json"
    val_json_path = "/root/autodl-tmp/finetune-qwen7b/data/ner/dev.json"
    max_source_length = 90  # text æ ·æœ¬æœ€é•¿çš„ token æ˜¯ 50 ï¼Œæ¨¡æ¿è‡ªèº«å¤§çº¦ 30 ä¸ªï¼Œæ€»å…±è‡³å°‘ 80 ä¸ª
    max_target_length = 70  # label æ ·æœ¬æœ€é•¿çš„ token æ˜¯ 70
    logs_dir = "logs"
    writer = SummaryWriter(logs_dir)
    # model and tokenizer
    tokenizer = AutoTokenizer.from_pretrained(model_name, )
    model = AutoModelForCausalLM.from_pretrained(model_name, attn_implementation="sdpa", torch_dtype="bfloat16")  # sdpa=Scaled Dot-Product Attention,  flash_attention_2 åªæ”¯æŒ fp16 å’Œ bf16 ä½†æ˜¯åŠ é€Ÿä¸æ˜æ˜¾ç”šè‡³å‡é€Ÿ
    print("Start Load Train and Validation Data...")
    training_set = NerDataset(train_json_path, tokenizer, max_source_length, max_target_length)
    val_set = NerDataset(val_json_path, tokenizer, max_source_length, max_target_length)
    # trainer
    training_args = TrainingArguments(
        output_dir="sft-7B-lora-ner", do_train=True, do_eval=True, seed=42, per_device_train_batch_size=1, per_device_eval_batch_size=1, gradient_accumulation_steps=8, gradient_checkpointing=True,
        num_train_epochs=3, learning_rate=1e-4, warmup_ratio=0.03, weight_decay=0.1, lr_scheduler_type="cosine", save_strategy="epoch", save_total_limit=3, evaluation_strategy="epoch", bf16=True,)
    trainer = Trainer( model=model, args=training_args,  train_dataset=training_set, eval_dataset=val_set, tokenizer=tokenizer, )
    print("Start Training...")
    trainer.train()
    writer.close()

if __name__ == '__main__':
    main()
 ```
 
 æ—¥å¿—æ‰“å°ï¼š
  ```
GPU æ˜¯å¦å¯ç”¨Trueï¼Œæœ‰1ä¸ªGPU
Start Load Train and Validation Data...
data load ï¼Œ sizeï¼š 10748
data load ï¼Œ sizeï¼š 1343
Start Training...
Start Training...
 11%|â–ˆ         | 428/4029 [04:13<35:35,  1.69it/s]
 ```
 

#### linux 3080-20G å•å¡

æ ·æœ¬ä¸€å…± 10748 ä¸ªï¼Œbatch_size ä¸º 8 ï¼Œepoch ä¸º 3 ï¼Œæ‰€ä»¥ 4029 ä¸ª steps ï¼Œæ¶ˆè€—æ˜¾å­˜ 6.6 G ï¼Œcpu ä½¿ç”¨ 100% å·¦å³ï¼Œå†…å­˜ä½¿ç”¨1.32Gï¼Œé¢„è®¡è€—æ—¶ 62 åˆ†é’Ÿå·¦å³ã€‚

å’Œã€window 4090 å•å¡ã€‘åŒæ ·çš„ä»£ç ï¼Œåªéœ€è¦å°†æ¨¡å‹å’Œæ•°æ®ä½ç½®ç­‰å‚æ•°ä¿®æ”¹ä¸€ä¸‹ï¼Œæ‰§è¡Œå‘½ä»¤ï¼š

    torchrun --nnodes 1 --nproc_per_node 1 fft_ner_deepspeed.py


æ—¥å¿—æ‰“å°ï¼š
```
(torch-2.1-py-3.10) root@autodl-container-6d3046b0a3-7076630c:~/autodl-tmp/finetune-qwen7b# torchrun --nnodes 1 --nproc_per_node 1 fft_ner_deepspeed.py 
GPU æ˜¯å¦å¯ç”¨Trueï¼Œæœ‰2ä¸ªGPU
Start Load Train and Validation Data...
data load ï¼Œ sizeï¼š 10748
data load ï¼Œ sizeï¼š 1343
Start Training...
{'loss': 0.0908, 'grad_norm': 1.5234375, 'learning_rate': 8.802812842254916e-05, 'epoch': 0.74}  
{'eval_loss': 0.07041047513484955, 'eval_runtime': 30.532, 'eval_samples_per_second': 43.987, 'eval_steps_per_second': 43.987, 'epoch': 1.0}                             
 37%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                 | 1482/4029 [22:46<39:02,  1.09it/s]
```



#### linux 3080-20G åŒå¡


æ ·æœ¬ä¸€å…± 10748 ä¸ªï¼Œbatch_size ä¸º 8 ï¼Œepoch ä¸º 3 ï¼Œä½†æ˜¯å› ä¸ºæ•°æ®å¹¶è¡Œåœ¨ä¸¤å¼ å¡ï¼Œæ‰€ä»¥ 2013 ä¸ª steps ï¼Œæ¶ˆè€—æ˜¾å­˜å‡ä¸º 6.3 G ï¼Œcpu ä½¿ç”¨ 200% å·¦å³ï¼Œå†…å­˜ä½¿ç”¨ 2.21Gï¼Œé¢„è®¡è€—æ—¶ 30 åˆ†é’Ÿå·¦å³ã€‚


å’Œã€window 4090 å•å¡ã€‘åŒæ ·çš„ä»£ç ï¼Œåªéœ€è¦å°†æ¨¡å‹å’Œæ•°æ®ä½ç½®ç­‰å‚æ•°ä¿®æ”¹ä¸€ä¸‹ï¼Œæ‰§è¡Œå‘½ä»¤ï¼š

    torchrun --nnodes 1 --nproc_per_node 2 fft_ner_deepspeed.py
    
æ—¥å¿—æ‰“å°ï¼š
```
(torch-2.1-py-3.10) root@autodl-container-6d3046b0a3-7076630c:~/autodl-tmp/finetune-qwen7b# torchrun --nnodes 1 --nproc_per_node 2 fft_ner_deepspeed.py 
GPU æ˜¯å¦å¯ç”¨Trueï¼Œæœ‰2ä¸ªGPU
GPU æ˜¯å¦å¯ç”¨Trueï¼Œæœ‰2ä¸ªGPU
Start Load Train and Validation Data...
data load ï¼Œ sizeï¼š 10748
data load ï¼Œ sizeï¼š 1343
Start Load Train and Validation Data...
Start Training...
data load ï¼Œ sizeï¼š 10748
data load ï¼Œ sizeï¼š 1343
 16%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ          | 322/2013 [04:52<25:21,  1.11it/s]
```



#### linux 3080-20G åŒå¡+deepspeed stage 2
#### linux 3080-20G åŒå¡+deepspeed stage 3

 ## LoRAå¾®è°ƒ qwen2.5-7B
 
 #### window 4090 å•å¡
 
 
å¯è®­ç»ƒå‚æ•°`20,185,088`ï¼Œæ ·æœ¬`10748`ä¸ªï¼Œbatch_size ä¸º 8 ï¼Œepoch ä¸º 3 ï¼Œä¸€å…± `4029 ä¸ª step` ã€‚è®­ç»ƒè¿‡ç¨‹ä¸ºäº†é¿å…çˆ†å¡ï¼Œè¿˜æ­é…ä½¿ç”¨äº† bfloat16ç²¾åº¦è¿›è¡Œæ¨¡å‹åŠ è½½ï¼Œä½¿ç”¨bf16æ··åˆç²¾åº¦è®­ç»ƒï¼Œè¿™éœ€è¦æ˜¾å¡åœ¨Ampereæˆ–è€…æ›´é«˜çº§åˆ«æ¶æ„é…ç½®ï¼ŒåŠ è½½æ¨¡å‹éœ€è¦`14.7G`ï¼Œå¾®è°ƒè®­ç»ƒéœ€è¦`2.9G`ï¼Œ æ€»å…±æ¶ˆè€—æ˜¾å­˜ `17.6G` ï¼Œè®­ç»ƒé¢„è®¡`è€—æ—¶58åˆ†é’Ÿ`ã€‚

è¯¦ç»†è®­ç»ƒä»£ç å¦‚ä¸‹ï¼š

```
import json
import numpy as np
import torch
from peft import LoraConfig, TaskType, get_peft_model
from torch.utils.data import Dataset
from torch.utils.tensorboard import SummaryWriter
from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments
from tqdm import tqdm
import time, sys

class NerDataset(Dataset):
    def __init__(self, data_path, tokenizer, max_source_length, max_target_length) -> None:
        super().__init__()
        self.tokenizer = tokenizer
        self.max_source_length = max_source_length
        self.max_target_length = max_target_length
        self.max_seq_length = self.max_source_length + self.max_target_length
        self.data = []
        if data_path:
            with open(data_path, "r", encoding='utf-8') as f:
                for line in f:
                    if not line or line == "":
                        continue
                    json_line = json.loads(line)
                    text = json_line["text"]
                    label = json_line["label"]
                    label = json.dumps(label, ensure_ascii=False)
                    self.data.append({"text": text, "label": label})
        print("data load ï¼Œ sizeï¼š", len(self.data))

    def preprocess(self, text, label):
        messages = [{"role": "system",
                     "content": "ä½ çš„ä»»åŠ¡æ˜¯åšNerä»»åŠ¡æå–, æ ¹æ®ç”¨æˆ·è¾“å…¥æå–å‡ºå®Œæ•´çš„å®ä½“ä¿¡æ¯, å¹¶ä»¥JSONæ ¼å¼è¾“å‡ºã€‚"},
                    {"role": "user", "content": text}]
        prompt = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        instruction = self.tokenizer(prompt, add_special_tokens=False, max_length=self.max_source_length,
                                     padding="max_length", pad_to_max_length=True, truncation=True)
        response = self.tokenizer(label, add_special_tokens=False, max_length=self.max_target_length,
                                  padding="max_length", pad_to_max_length=True, truncation=True)
        input_ids = instruction["input_ids"] + response["input_ids"] + [self.tokenizer.pad_token_id]
        attention_mask = (instruction["attention_mask"] + response["attention_mask"] + [1])
        labels = [-100] * len(instruction["input_ids"]) + response["input_ids"] + [self.tokenizer.pad_token_id]
        return input_ids, attention_mask, labels

    def __getitem__(self, index):
        item_data = self.data[index]
        input_ids, attention_mask, labels = self.preprocess(**item_data)
        return {"input_ids": torch.LongTensor(np.array(input_ids)),
                "attention_mask": torch.LongTensor(np.array(attention_mask)),
                "labels": torch.LongTensor(np.array(labels))}

    def __len__(self):
        return len(self.data)

def main():
    print(f"GPU æ˜¯å¦å¯ç”¨{torch.cuda.is_available()}ï¼Œæœ‰{torch.cuda.device_count()}ä¸ªGPU")
    model_name = "D:\Qwen2.5-7B-Instruct"
    train_json_path = "data/ner/train.json"
    val_json_path = "data/ner/dev.json"
    max_source_length = 90  # text æ ·æœ¬æœ€é•¿çš„ token æ˜¯ 50 ï¼Œæ¨¡æ¿è‡ªèº«å¤§çº¦ 30 ä¸ªï¼Œæ€»å…±è‡³å°‘ 80 ä¸ª
    max_target_length = 70  # label æ ·æœ¬æœ€é•¿çš„ token æ˜¯ 70
    logs_dir = "logs"
    writer = SummaryWriter(logs_dir)
    # model and tokenizer
    tokenizer = AutoTokenizer.from_pretrained(model_name, )
    model = AutoModelForCausalLM.from_pretrained(model_name, attn_implementation="sdpa",
                                                 torch_dtype="bfloat16")  # sdpa=Scaled Dot-Product Attention,  flash_attention_2 åªæ”¯æŒ fp16 å’Œ bf16 ä½†æ˜¯åŠ é€Ÿä¸æ˜æ˜¾ç”šè‡³å‡é€Ÿ
    print("Start Load Train and Validation Data...")
    training_set = NerDataset(train_json_path, tokenizer, max_source_length, max_target_length)
    val_set = NerDataset(val_json_path, tokenizer, max_source_length, max_target_length)
    # lora
    peft_config = LoraConfig(task_type=TaskType.CAUSAL_LM,
                             target_modules="q_proj,v_proj,k_proj,o_proj,gate_proj,down_proj,up_proj".split(","),
                             inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.05)
    model = get_peft_model(model, peft_config)
    model.print_trainable_parameters()
    # trainer
    training_args = TrainingArguments(
        output_dir="sft-7B-lora-ner", do_train=True, do_eval=True, seed=42, per_device_train_batch_size=1,
        per_device_eval_batch_size=1, gradient_accumulation_steps=8,  # gradient_checkpointing=True,
        num_train_epochs=3, learning_rate=1e-4, warmup_ratio=0.03, weight_decay=0.1, lr_scheduler_type="cosine",
        save_strategy="epoch", save_total_limit=3, evaluation_strategy="epoch", bf16=True, )
    trainer = Trainer(model=model, args=training_args, train_dataset=training_set, eval_dataset=val_set,
                      tokenizer=tokenizer, )
    print("Start Training...")
    trainer.train()
    writer.close()


if __name__ == '__main__':
    main()
```

ç›´æ¥è¿è¡Œå¯åŠ¨ï¼Œæ—¥å¿—æ‰“å°:
```
GPU æ˜¯å¦å¯ç”¨Trueï¼Œæœ‰1ä¸ªGPU
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 11.11it/s]
Start Load Train and Validation Data...
data load ï¼Œ sizeï¼š 10748
data load ï¼Œ sizeï¼š 1343
trainable params: 20,185,088 || all params: 7,635,801,600 || trainable%: 0.26434798934534914
Start Training...
  4%|â–         | 164/4029 [02:22<56:25,  1.14it/s]
```
 
 
 
 
#### linux 3080-20G å•å¡

è¯¦ç»†è®­ç»ƒå‚æ•°å¦‚ä¸‹ï¼Œè®­ç»ƒæ ·æœ¬`10748`ä¸ªï¼Œbatch_size ä¸º 8 ï¼Œepoch ä¸º 3 ï¼Œä¸€å…± `4029` ä¸ª step ã€‚å¯è®­ç»ƒå‚æ•°`20,185,088`ï¼Œå•å¡è®­ç»ƒæ¶ˆè€—æ˜¾å­˜ `19.5G` ï¼ŒCPU ä½¿ç”¨ `100%` å·¦å³ï¼Œå¤§çº¦éœ€è¦è€—æ—¶`1h50m`å·¦å³ã€‚è¯¦ç»†è®­ç»ƒä»£ç åŒä¸Šä¸€èŠ‚ã€window 4090 å•å¡ã€‘ä¸€æ ·ï¼Œåªéœ€è¦æ”¹åŠ¨ä¸‹æ¨¡å‹å’Œæ•°æ®å­˜æ”¾çš„ä½ç½®å³å¯ã€‚æ§åˆ¶å°æ‰§è¡Œå‘½ä»¤ï¼š

    torchrun --nnodes 1 --nproc_per_node 1 finetune_lora_ner_deepspeed.py

æ—¥å¿—æ‰“å°ï¼š

```
(torch-2.1-py-3.10) root@autodl-container-6d3046b0a3-7076630c:~/autodl-tmp/finetune-qwen7b# torchrun --nnodes 1 --nproc_per_node 1 finetune_lora_ner_deepspeed.py
GPU æ˜¯å¦å¯ç”¨Trueï¼Œæœ‰2ä¸ªGPU
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 13.79it/s]
Start Load Train and Validation Data...
data load ï¼Œ sizeï¼š 10748
data load ï¼Œ sizeï¼š 1343
trainable params: 20,185,088 || all params: 7,635,801,600 || trainable%: 0.26434798934534914
Start Training...
  2%|â–ˆâ–ˆâ–ˆâ–Š               | 95/4029 [02:32<1:46:08,  1.62s/it]
```

#### linux 3080-20G åŒå¡
 
 
å¯è®­ç»ƒå‚æ•° `20,185,088` ï¼Œæ ·æœ¬ `10748` ä¸ªï¼Œbatch_size ä¸º 8 ï¼Œepoch ä¸º 3 ï¼Œä½†æ˜¯å› ä¸ºå°†æ•°æ®åˆ†ç»™äº†ä¸¤å¼ å¡ä¸Šï¼Œæ‰€ä»¥ä¸€å…± `2013` ä¸ª step ã€‚è®­ç»ƒæ¶ˆè€—ä¸¤å¼ å¡å„è‡ª `18.2G`ã€`18.9G` ï¼ŒCPU ä½¿ç”¨ `200%` å·¦å³ï¼Œè€—æ—¶é¢„è®¡ `55` åˆ†é’Ÿå·¦å³ã€‚å¯ä»¥çœ‹å‡ºæ¯”åœ¨æœåŠ¡å™¨ä¸Šä½¿ç”¨å•å¡èŠ‚çº¦å¤§çº¦ 1 ä¸ªå°æ—¶ï¼Œæå‡æ•ˆæœå¾ˆæ˜æ˜¾ã€‚ä½†æ˜¯ä¸4090å•å¡è®­ç»ƒçš„æ•ˆæœç›¸å·®ä¸å¤§ï¼ŒçŒœæµ‹å¯èƒ½æ˜¯ç”±äº 4090 æ˜¾å¡æ¯”3080æ˜¾å¡çš„æ€§èƒ½å¥½ã€‚

è®­ç»ƒä»£ç åŒä¸€èŠ‚çš„ ã€window 4090 å•å¡ã€‘ï¼Œåªéœ€è¦æ”¹åŠ¨ä¸‹æ¨¡å‹å’Œæ•°æ®å­˜æ”¾çš„ä½ç½®å³å¯ã€‚åªéœ€è¦å°†ä¸‹é¢çš„å‘½ä»¤ä¸­ nproc_per_node æ”¹æˆ 2 å³å¯ï¼Œç›´æ¥æ§åˆ¶å°è¿è¡Œå‘½ä»¤ï¼š

    torchrun --nnodes 1 --nproc_per_node 2 finetune_lora_ner_deepspeed.py
    
æ—¥å¿—æ‰“å°ï¼Œå› ä¸ºæœ‰ä¸¤å¼ å¡æ‰€ä»¥æ‰“å°ä¸¤ä»½ä¿¡æ¯ï¼š
```
(torch-2.1-py-3.10) root@autodl-container-6d3046b0a3-7076630c:~/autodl-tmp/finetune-qwen7b# torchrun --nnodes 1 --nproc_per_node 2 finetune_lora_ner_deepspeed.py
GPU æ˜¯å¦å¯ç”¨Trueï¼Œæœ‰2ä¸ªGPU
GPU æ˜¯å¦å¯ç”¨Trueï¼Œæœ‰2ä¸ªGPU
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00,  7.62it/s]
Start Load Train and Validation Data...
data load ï¼Œ sizeï¼š 10748
data load ï¼Œ sizeï¼š 1343
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00,  8.48it/s]
Start Load Train and Validation Data...
data load ï¼Œ sizeï¼š 10748
data load ï¼Œ sizeï¼š 1343
trainable params: 20,185,088 || all params: 7,635,801,600 || trainable%: 0.26434798934534914
Start Training...
{'loss': 0.2988, 'grad_norm': 0.2218552976846695, 'learning_rate': 8.80307486729813e-05, 'epoch': 0.74}                                                                                                                       
{'eval_loss': 0.02959359996020794, 'eval_runtime': 53.1918, 'eval_samples_per_second': 25.248, 'eval_steps_per_second': 12.634, 'epoch': 1.0}                                                                                 
 40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 802/2013 [22:18<32:22,  1.60s/it]
```

#### linux 3080-20G åŒå¡+deepspeed stage 2

å¯è®­ç»ƒå‚æ•° `20,185,088` ï¼Œæ ·æœ¬ `10748` ä¸ªï¼Œbatch_size ä¸º 8 ï¼Œepoch ä¸º 3 ï¼Œä½†æ˜¯å› ä¸ºå°†æ•°æ®åˆ†ç»™äº†ä¸¤å¼ å¡ä¸Šï¼Œæ‰€ä»¥ä¸€å…± `2013` ä¸ª step ã€‚è®­ç»ƒæ¶ˆè€—ä¸¤å¼ å¡å„è‡ª `17.9G`ã€`18.5G` ï¼ŒCPU ä½¿ç”¨ `200%` å·¦å³ï¼Œè€—æ—¶é¢„è®¡ `53` åˆ†é’Ÿå·¦å³ï¼Œå¯ä»¥çœ‹å‡ºæ¥åœ¨åŒå¡è®­ç»ƒåŸºç¡€ä¸Šå¢åŠ ä½¿ç”¨ deepspeed stage 2 æŠ€æœ¯ä¹‹åè®­ç»ƒæ—¶é—´å‡å°‘ä¸æ˜æ˜¾ï¼Œè€ƒè™‘åˆ°æœåŠ¡å™¨çš„ä½¿ç”¨æ³¢åŠ¨æƒ…å†µï¼Œå…¶å®ç›¸å½“äºæ²¡ä»€ä¹ˆæå‡æ•ˆæœã€‚å…¶å®å¾ˆå¥½ç†è§£ï¼Œå› ä¸º zero-stage2 ä¸»è¦åšçš„æ˜¯æ¢¯åº¦åˆ†åŒºï¼Œä½†æ˜¯æˆ‘è¿™é‡Œå› ä¸ºæ¯å¼ å¡éƒ½èƒ½å„è‡ªåŠ è½½æ¨¡å‹è¿›è¡Œè®­ç»ƒè¿‡ç¨‹ï¼Œç”¨ä¸åˆ°æ¢¯åº¦åˆ†åŒºçš„æ€§èƒ½æå‡ä½œç”¨ã€‚

è®­ç»ƒä»£ç åŒä¸€èŠ‚çš„ ã€window 4090 å•å¡ã€‘ï¼Œåªéœ€è¦æ”¹åŠ¨ä¸‹æ¨¡å‹å’Œæ•°æ®å­˜æ”¾çš„ä½ç½®å³å¯ã€‚ç›´æ¥æ§åˆ¶å°è¿è¡Œå‘½ä»¤ï¼š

    torchrun --nnodes 1 --nproc_per_node 2 finetune_lora_ner_deepspeed.py --deepspeed ds_config_zero2.json
    
å…¶ä¸­ [ds_config_zero2.json](https://huggingface.co/docs/transformers/zh/main_classes/deepspeed#zero-2-%E7%A4%BA%E4%BE%8B) å’Œ finetune_lora_ner_deepspeed.py æ–‡ä»¶åœ¨åŒä¸€ä¸ªç›®å½•ï¼Œå…·ä½“å†…å®¹å¦‚ä¸‹ï¼š
```
{
    "fp16": {
        "enabled": "auto",
        "loss_scale": 0,
        "loss_scale_window": 1000,
        "initial_scale_power": 16,
        "hysteresis": 2,
        "min_loss_scale": 1
    },

    "optimizer": {
        "type": "AdamW",
        "params": {
            "lr": "auto",
            "betas": "auto",
            "eps": "auto",
            "weight_decay": "auto"
        }
    },

    "scheduler": {
        "type": "WarmupLR",
        "params": {
            "warmup_min_lr": "auto",
            "warmup_max_lr": "auto",
            "warmup_num_steps": "auto"
        }
    },

    "zero_optimization": {
        "stage": 2,
        "offload_optimizer": {
            "device": "cpu",
            "pin_memory": true
        },
        "allgather_partitions": true,
        "allgather_bucket_size": 2e8,
        "overlap_comm": true,
        "reduce_scatter": true,
        "reduce_bucket_size": 2e8,
        "contiguous_gradients": true
    },

    "gradient_accumulation_steps": "auto",
    "gradient_clipping": "auto",
    "steps_per_print": 2000,
    "train_batch_size": "auto",
    "train_micro_batch_size_per_gpu": "auto",
    "wall_clock_breakdown": false
}
```

æ—¥å¿—æ‰“å°ï¼š

```
(torch-2.1-py-3.10) root@autodl-container-6d3046b0a3-7076630c:~/autodl-tmp/finetune-qwen7b# torchrun --nnodes 1 --nproc_per_node 2 finetune_lora_ner_deepspeed.py --deepspeed ds_config_zero2.json
GPU æ˜¯å¦å¯ç”¨Trueï¼Œæœ‰2ä¸ªGPU
GPU æ˜¯å¦å¯ç”¨Trueï¼Œæœ‰2ä¸ªGPU
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 10.79it/s]
Start Load Train and Validation Data...
data load ï¼Œ sizeï¼š 10748
data load ï¼Œ sizeï¼š 1343
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 13.57it/s]
Start Load Train and Validation Data...
data load ï¼Œ sizeï¼š 10748
data load ï¼Œ sizeï¼š 1343
trainable params: 20,185,088 || all params: 7,635,801,600 || trainable%: 0.26434798934534914
trainable params: 20,185,088 || all params: 7,635,801,600 || trainable%: 0.26434798934534914
Start Training...
Start Training...
  6%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 114/2013 [03:00<50:16,  1.59s/it]

```


#### linux 3080-20G åŒå¡+deepspeed stage 3

å¯è®­ç»ƒå‚æ•° `20,185,088` ï¼Œæ ·æœ¬ `10748` ä¸ªï¼Œbatch_size ä¸º 8 ï¼Œepoch ä¸º 3 ï¼Œä½†æ˜¯å› ä¸ºå°†æ•°æ®åˆ†ç»™äº†ä¸¤å¼ å¡ä¸Šï¼Œæ‰€ä»¥ä¸€å…± `2013` ä¸ª step ã€‚è®­ç»ƒæ¶ˆè€—ä¸¤å¼ å¡å„è‡ª `17.9G`ã€`18.3G` ï¼ŒCPU ä½¿ç”¨ `200%` å·¦å³ï¼Œè€—æ—¶é¢„è®¡ `52` åˆ†é’Ÿå·¦å³ï¼Œå¯ä»¥çœ‹å‡ºæ¥åœ¨åŒå¡è®­ç»ƒåŸºç¡€ä¸Šå¢åŠ ä½¿ç”¨ deepspeed stage 3 æŠ€æœ¯ä¹‹åè®­ç»ƒæ—¶é—´å‡å°‘ä¸æ˜æ˜¾ï¼Œè€ƒè™‘åˆ°æœåŠ¡å™¨çš„ä½¿ç”¨æ³¢åŠ¨æƒ…å†µï¼Œå…¶å®ç›¸å½“äºæ²¡ä»€ä¹ˆæå‡æ•ˆæœã€‚å…¶å®å¾ˆå¥½ç†è§£ï¼Œå› ä¸º zero-stage2 ä¸»è¦åšçš„æ˜¯å‚æ•°åˆ†åŒºï¼Œä½†æ˜¯æˆ‘è¿™é‡Œå› ä¸ºæ¯å¼ å¡éƒ½èƒ½å„è‡ªåŠ è½½æ¨¡å‹è¿›è¡Œè®­ç»ƒè¿‡ç¨‹ï¼Œç”¨ä¸åˆ°å‚æ•°åˆ†åŒºçš„æ€§èƒ½æå‡åŠŸèƒ½ã€‚

è®­ç»ƒä»£ç åŒä¸€èŠ‚çš„ ã€window 4090 å•å¡ã€‘ï¼Œåªéœ€è¦æ”¹åŠ¨ä¸‹æ¨¡å‹å’Œæ•°æ®å­˜æ”¾çš„ä½ç½®å³å¯ã€‚ç›´æ¥æ§åˆ¶å°è¿è¡Œå‘½ä»¤ï¼š

    torchrun --nnodes 1 --nproc_per_node 2 finetune_lora_ner_deepspeed.py --deepspeed ds_config_zero3.json
    
å…¶ä¸­ [ds_config_zero3.json](https://huggingface.co/docs/transformers/zh/main_classes/deepspeed#zero-3-%E7%A4%BA%E4%BE%8B) å’Œ finetune_lora_ner_deepspeed.py æ–‡ä»¶åœ¨åŒä¸€ä¸ªç›®å½•ï¼Œå…·ä½“å†…å®¹å¦‚ä¸‹ï¼š
```
{
    "fp16": {
        "enabled": "auto",
        "loss_scale": 0,
        "loss_scale_window": 1000,
        "initial_scale_power": 16,
        "hysteresis": 2,
        "min_loss_scale": 1
    },

    "optimizer": {
        "type": "AdamW",
        "params": {
            "lr": "auto",
            "betas": "auto",
            "eps": "auto",
            "weight_decay": "auto"
        }
    },

    "scheduler": {
        "type": "WarmupLR",
        "params": {
            "warmup_min_lr": "auto",
            "warmup_max_lr": "auto",
            "warmup_num_steps": "auto"
        }
    },

    "zero_optimization": {
        "stage": 3,
        "offload_optimizer": {
            "device": "cpu",
            "pin_memory": true
        },
        "offload_param": {
            "device": "cpu",
            "pin_memory": true
        },
        "overlap_comm": true,
        "contiguous_gradients": true,
        "sub_group_size": 1e9,
        "reduce_bucket_size": "auto",
        "stage3_prefetch_bucket_size": "auto",
        "stage3_param_persistence_threshold": "auto",
        "stage3_max_live_parameters": 1e9,
        "stage3_max_reuse_distance": 1e9,
        "stage3_gather_16bit_weights_on_model_save": true
    },

    "gradient_accumulation_steps": "auto",
    "gradient_clipping": "auto",
    "steps_per_print": 2000,
    "train_batch_size": "auto",
    "train_micro_batch_size_per_gpu": "auto",
    "wall_clock_breakdown": false
}
```
    
æ—¥å¿—æ‰“å°ï¼Œå› ä¸ºæœ‰ä¸¤å¼ å¡æ‰€ä»¥æ‰“å°ä¸¤ä»½ä¿¡æ¯ï¼š
```
(torch-2.1-py-3.10) root@autodl-container-6d3046b0a3-7076630c:~/autodl-tmp/finetune-qwen7b# torchrun --nnodes 1 --nproc_per_node 2 finetune_lora_ner_deepspeed.py --deepspeed ds_config_zero3.json
GPU æ˜¯å¦å¯ç”¨Trueï¼Œæœ‰2ä¸ªGPU
GPU æ˜¯å¦å¯ç”¨Trueï¼Œæœ‰2ä¸ªGPU
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 10.98it/s]
Loading checkpoint shards:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                        | 3/4 [00:00<00:00, 10.24it/s]Start Load Train and Validation Data...
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 10.63it/s]
Start Load Train and Validation Data...
data load ï¼Œ sizeï¼š 10748
data load ï¼Œ sizeï¼š 1343
data load ï¼Œ sizeï¼š 10748
data load ï¼Œ sizeï¼š 1343
trainable params: 20,185,088 || all params: 7,635,801,600 || trainable%: 0.26434798934534914
/root/miniconda3/envs/torch-2.1-py-3.10/lib/python3.10/site-packages/transformers/training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
/root/autodl-tmp/finetune-qwen7b/finetune_lora_ner_deepspeed.py:71: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer( model=model, args=training_args,  train_dataset=training_set, eval_dataset=val_set, tokenizer=tokenizer, )
trainable params: 20,185,088 || all params: 7,635,801,600 || trainable%: 0.26434798934534914
/root/miniconda3/envs/torch-2.1-py-3.10/lib/python3.10/site-packages/transformers/training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
/root/autodl-tmp/finetune-qwen7b/finetune_lora_ner_deepspeed.py:71: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer( model=model, args=training_args,  train_dataset=training_set, eval_dataset=val_set, tokenizer=tokenizer, )
Start Training...
Start Training...
[W reducer.cpp:1346] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1346] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
 14%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 291/2013 [07:44<45:53,  1.60s/it]
```

 ## LoRAå¾®è°ƒ llama3-8b
 
 #### window 4090å•å¡
 åœ¨ windows å°å¼æœºä¸Šå•å¡ 4090 ä¸Šè¿è¡Œï¼Œä½¿ç”¨ 99973 æ¡æ ·æœ¬è¿›è¡Œè®­ç»ƒï¼Œå‚æ•°å¦‚ä¸‹ï¼Œæ¶ˆè€—æ˜¾å­˜17G å·¦å³ï¼Œbatch_size ä¸º 8 ï¼Œæ€»å…±éœ€è¦ 12496 ä¸ª step ï¼Œè€—æ—¶å¤§çº¦ 2h15m ï¼š
 ```
 --model_name_or_path
D:\Chinese-LLaMA-Alpaca-3\Llama-3-8B
--tokenizer_name_or_path
D:\Chinese-LLaMA-Alpaca-3\Llama-3-8B
--dataset_dir
D:\Chinese-LLaMA-Alpaca-3\data\train
--per_device_train_batch_size
1
--per_device_eval_batch_size
1
--do_train
1
--do_eval
1
--seed
42
--num_train_epochs
1
--lr_scheduler_type
cosine
--learning_rate
1e-4
--warmup_ratio
0.03
--weight_decay
0.1
--save_strategy
steps
--save_total_limit
5
--evaluation_strategy
steps
--eval_steps
2000
--save_steps
200
--gradient_accumulation_steps
8
--preprocessing_num_workers
8
--max_seq_length
150
--output_dir
D:\Chinese-LLaMA-Alpaca-3\lora
--overwrite_output_dir
1
--lora_rank
8
--lora_alpha
32
--trainable
"q_proj,v_proj,k_proj"
--lora_dropout
0.05
--bf16
1
--torch_dtype
bfloat16
--validation_file
"D:\Chinese-LLaMA-Alpaca-3\data\eval\eval_2737.json"
--load_in_kbits
16
 ```
 
 
 æ—¥å¿—æ‰“å°ï¼š
  ```
  20%|â–ˆâ–ˆ        | 2509/12496 [28:15<1:47:52,  1.54it/s]
 ```
 
  #### linux 3080-20G å•å¡
  
  å°†ç›¸åŒçš„æ•°æ®å’Œä»£ç éƒ½ä¸Šä¼ åˆ°æœåŠ¡å™¨ä¸Šï¼Œå¹¶ä¸”æ–°å»ºè„šæœ¬æ‰§è¡Œ bash run_sft.sh ç”¨äºè¿è¡Œç¨‹åºã€‚
  
  å¯ä»¥çœ‹å‡ºæ¥æ€»å…± 99973 ä¸ªæ ·æœ¬ï¼Œ batch_size ä¸º 8 ï¼Œæ‰€ä»¥ä¸€å…± 12496 ä¸ª step ï¼Œæ¶ˆè€—æ˜¾å­˜ 17G å·¦å³ï¼Œè€—æ—¶é¢„è®¡ 3h50m ã€‚
  
   ```
 ########å‚æ•°éƒ¨åˆ†########
lr=1e-4
lora_rank=8
lora_alpha=32
lora_trainable="q_proj,v_proj,k_proj"
modules_to_save=None
lora_dropout=0.05

pretrained_model=/root/autodl-tmp/Llama-3-8B
dataset_dir=/root/autodl-tmp/ft-llama3/train
per_device_train_batch_size=1
per_device_eval_batch_size=1
gradient_accumulation_steps=8
max_seq_length=512
output_dir=llama-3-8b-chinese-lora
validation_file=/root/autodl-tmp/ft-llama3/eval/eval_2737.json

torchrun --nnodes 1 --nproc_per_node 1 run_clm_sft_with_peft.py \
    --model_name_or_path ${pretrained_model} \
    --tokenizer_name_or_path ${pretrained_model} \
    --dataset_dir ${dataset_dir} \
    --per_device_train_batch_size ${per_device_train_batch_size} \
    --per_device_eval_batch_size ${per_device_eval_batch_size} \
    --do_train 1\
    --do_eval 1\
    --seed 42 \
    --bf16 1\
    --num_train_epochs 1 \
    --lr_scheduler_type cosine \
    --learning_rate ${lr} \
    --warmup_ratio 0.05 \
    --weight_decay 0.1 \
    --logging_strategy steps \
    --logging_steps 500 \
    --save_strategy steps \
    --save_total_limit 3 \
    --evaluation_strategy steps \
    --eval_steps 500 \
    --save_steps 500 \
    --gradient_accumulation_steps ${gradient_accumulation_steps} \
    --preprocessing_num_workers 8 \
    --max_seq_length ${max_seq_length} \
    --output_dir ${output_dir} \
    --overwrite_output_dir 1\
    --ddp_timeout 30000 \
    --logging_first_step True \
    --lora_rank ${lora_rank} \
    --lora_alpha ${lora_alpha} \
    --trainable ${lora_trainable} \
    --lora_dropout ${lora_dropout} \
    --modules_to_save ${modules_to_save} \
    --torch_dtype bfloat16 \
    --validation_file ${validation_file} \
    --load_in_kbits 16
 ```
  
  æ—¥å¿—æ‰“å°ï¼š
   ```
  WARNING:__main__:Num train_samples  99973
  WARNING:__main__:Model vocab size: 128256
WARNING:__main__:len(tokenizer):128256
WARNING:__main__:Init new peft model
WARNING:__main__:target_modules: ['q_proj', 'v_proj', 'k_proj']
WARNING:__main__:lora_rank: 8ï¼Œlora_alphaï¼š32.0ï¼Œlora_dropoutï¼š0.05
trainable params: 4,718,592 || all params: 8,034,979,840 || trainable%: 0.058725623386256066
/root/autodl-tmp/ft-llama3/run_clm_sft_with_peft.py:235: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer( model=model, args=training_args, train_dataset=train_dataset, eval_dataset=eval_dataset, tokenizer=tokenizer, data_collator=data_collator, )
  0%|                                                                                                                                                                          | 0/12496 [00:00<?, ?it/s]/root/miniconda3/envs/torch-2.1-py-3.10/lib/python3.10/site-packages/transformers/data/data_collator.py:657: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:261.)
  batch["labels"] = torch.tensor(batch["labels"], dtype=torch.int64)
[W reducer.cpp:1346] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
{'loss': 1.9671, 'grad_norm': 9.245835304260254, 'learning_rate': 1.6e-07, 'epoch': 0.0}                                                                                                                 
{'loss': 1.6672, 'grad_norm': 2.070274591445923, 'learning_rate': 8e-05, 'epoch': 0.04}                                                                                                                  
{'eval_loss': nan, 'eval_runtime': 201.6847, 'eval_samples_per_second': 13.571, 'eval_steps_per_second': 13.571, 'epoch': 0.04}                                                                          
  7%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 899/12496 [19:52<3:33:30,  1.10s/it]
  
   ```
  
  #### linux 3080-20G åŒå¡

å°† --nproc_per_node 1 æ”¹ä¸º 2 å³å¯ï¼Œå†æ¬¡æ‰§è¡ŒåŒä¸€ä¸ªè„šæœ¬ã€‚
 
åœ¨ä¸¤å¼ 3080-20G çš„å¡ä¸Šè¿›è¡Œè®­ç»ƒï¼Œèƒ½å¤Ÿæ­£å¸¸è¿è¡Œï¼Œå¯è®­ç»ƒå‚æ•° 4,718,592 ï¼Œæ ·æœ¬ 99973 ï¼Œä¸¤å¼ å¡å„è‡ªæ¶ˆè€—æ˜¾å­˜ 17G å·¦å³ã€‚batch_size ä¸º 8 ï¼Œå› ä¸ºæœ‰ä¸¤å¼ å¡ï¼Œæ‰€ä»¥æ¯å¼ å¡åˆ†é…åˆ°ä¸€åŠè®­ç»ƒæ•°æ®ï¼Œæ€»å…±éœ€è¦æ‰§è¡Œ 6248 ä¸ª step ï¼Œé¢„è®¡ 2h5m å®Œæˆè®­ç»ƒï¼Œå¯ä»¥çœ‹å‡ºæ¥æ¯”å•å¡è®­ç»ƒè€—æ—¶å‡å°‘ 1h45m ï¼Œè€—æ—¶å‡å°‘ 84% å·¦å³ã€‚

ä½†æ˜¯ç›¸å¯¹äº window 4090 å•å¡çš„è€—æ—¶ç»Ÿè®¡æ•°æ®ä¸Šåªç¼©å‡äº†10åˆ†é’Ÿè®­ç»ƒæ—¶é—´ã€‚è‡³äºä¸ºä»€ä¹ˆè€—æ—¶æ²¡æœ‰æ˜æ˜¾å‡å°‘ï¼Œå¯èƒ½æ˜¯å› ä¸ºè¿™ä¸ªæ˜¯ 3080 ï¼Œç®—åŠ›ä¸å¦‚ 4090 ï¼Œä¹Ÿå¯èƒ½æ˜¯ä¸¤å¼ å¡æ•ˆæœç”±äºé€šä¿¡æŠ˜æŸæ•ˆç‡å¯¼è‡´ä¸æ˜æ˜¾ï¼Œéœ€è¦æ›´å¤šå¡ã€‚

  ```
{'eval_loss': nan, 'eval_runtime': 103.5874, 'eval_samples_per_second': 26.422, 'eval_steps_per_second': 13.216, 'epoch': 0.16}
 27%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–     | 1670/6248 [36:36<1:26:38,  1.14s/it
 ```