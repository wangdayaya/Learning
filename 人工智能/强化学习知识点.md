强化学习

# chatgpt
https://openai.com/index/chatgpt/

[ChatGPT 是 InstructGPT](https://openai.com/index/instruction-following/)的改进版本，经过训练可以遵循提示中的指令并提供详细的响应。


ChatGPT 是根据 GPT-3.5 系列中的模型使用 RLHF （人工标注数据+强化学习）进行微调的，该模型于 2022 年初完成训练。 ChatGPT 和 GPT-3.5 在 Azure AI 超级计算基础架构上进行了训练。

![image.png](https://p26-juejin-sign.byteimg.com/tos-cn-i-k3u1fbpfcp/535fa785954747b2922ea132fa0fb5af~tplv-k3u1fbpfcp-jj-mark-v1:0:0:0:0:5o6Y6YeR5oqA5pyv56S-5Yy6:q75.awebp?rk3s=bb34011d&x-expires=1731132075&x-signature=8yqVL2z3wZtqWEXgwPqTM49GHsg%3D)

 

# chatgpt 解释强化学习如何结合人类反馈来为语言模型工作

当然可以！假设你有一个名叫 Rufus 的机器人，他想学习如何像人类一样说话。Rufus 有一个语言模型，已经可以帮助他理解单词和句子。

首先，Rufus 会用他的语言模型说些什么。例如，他可能会说“l am a robot”。
然后，人类会听 Rufus 说的话，并给他反馈，判断这句话听起来是否像人类会说的自然句子。人类可能会说，“这不太对。Rufus，人类通常不会说‘l am a robot’他们可能会说‘l'm a robot’”

Rufus 将接受这些反馈，并使用它来更新他的语言模型。他会尝试再次说出这句话，使用从人类那里获得的新信息。这一次，他可能会说“l'm a robot。”

人类会再次倾听并给 Rufus 更多反馈，这个过程会一直持续到 Rufus 能说出对人类来说听起来自然的句子。随着时间的推移，Rufus 将通过从人类那里收到的反馈，学会如何像人类一样说话。这就是使用 RL 和人类反馈来改进语言模型的方法。

https://huggingface.co/blog/zh/rlhf

# 在一张 24 GB 的消费级显卡上用 RLHF 微调 20B LLMs

https://huggingface.co/blog/zh/trl-peft

# trl - Transformer Reinforcement Learning

一组工具来通过强化学习训练 Transformer ，`trl` 库的目的是使 RL 的步骤更容易和灵活，让每个人可以在他们自己的数据集和训练设置上用 RL 微调 LM。

TRL 是一个全栈库，我们在其中提供了一套工具来使用强化学习训练 transformer 语言模型，从监督微调步骤 (SFT)、奖励建模步骤 (RM) 到近端策略优化 (PPO) 步骤。该库与 🤗 transformers 集成。

https://github.com/huggingface/trl ，https://github.com/lansinuote/Simple_TRL/tree/main
https://blog.csdn.net/qq_41185868/article/details/133865134

## trl 流程图
![image.png](https://p26-juejin-sign.byteimg.com/tos-cn-i-k3u1fbpfcp/16840cc2f5d14f6f9496cf64fd2ab85a~tplv-k3u1fbpfcp-jj-mark-v1:0:0:0:0:5o6Y6YeR5oqA5pyv56S-5Yy6:q75.awebp?rk3s=bb34011d&x-expires=1731132075&x-signature=ucl%2FeuXqCFFkdItobQ0mI6AC3oQ%3D)

- SFTTrainer:一个轻量级且友好的围绕transformer Trainer的包装器，可以在自定义数据集上轻松微调语言模型或适配器。
- RewardTrainer: transformer Trainer的一个轻量级包装，可以轻松地微调人类偏好的语言模型(Reward Modeling)。
- potrainer:用于语言模型的PPO训练器，它只需要(查询、响应、奖励)三元组来优化语言模型。

 
                        
## trl 详细过程
![image.png](https://p26-juejin-sign.byteimg.com/tos-cn-i-k3u1fbpfcp/ada7894aca6d4d18aaf8a0b2dc34da09~tplv-k3u1fbpfcp-jj-mark-v1:0:0:0:0:5o6Y6YeR5oqA5pyv56S-5Yy6:q75.awebp?rk3s=bb34011d&x-expires=1731132075&x-signature=vwxSnhLqhSWKcCizxNfdSThfpAo%3D)

Fine-tuning a language model via PPO consists of roughly three steps:

1.  **Rollout**: The language model generates a response or continuation based on query which could be the start of a sentence. 语言模型基于查询生成响应或继续，查询可以是句子的开头。
1.  **Evaluation**: The query and response are evaluated with a function, model, human feedback or some combination of them. The important thing is that this process should yield a scalar value for each query/response pair.使用一个函数、模型、人类反馈或它们的组合来评估查询和响应。重要的是，此过程应为每个查询/响应对产生一个标量值。
1.  **Optimization**: This is the most complex part. In the optimisation step the query/response pairs are used to calculate the log-probabilities of the tokens in the sequences. This is done with the model that is trained and a reference model, which is usually the pre-trained model before fine-tuning. The KL-divergence between the two outputs is used as an additional reward signal to make sure the generated responses don't deviate too far from the reference language model. The active language model is then trained with PPO.这是最复杂的部分。在优化步骤中，使用查询/响应对来计算序列中token的对数概率。这是通过训练的模型和一个参考模型（通常是微调之前的预训练模型）来完成的。两个输出之间的KL-散度被用作附加奖励信号，以确保生成的响应不会偏离参考语言模型太远。然后，使用PPO训练主动语言模型。 
 
## trl 库中的 active model 和 reference model 是什么，有什么用处
在 `trl` 库（用于基于强化学习对语言模型进行微调）中，**active model** 和 **reference model** 是强化学习过程中重要的概念，它们主要用于控制模型策略的更新和优化，确保生成质量的稳定性。以下是这两个模型的作用及用途：

### 1. **Active Model（活动模型）**
- **定义**: Active model 是指在强化学习过程中当前正在被优化和更新的模型，它的策略会随着每一轮训练进行调整。
- **作用**: 这个模型会根据输入生成输出，并且根据奖励模型提供的反馈（即奖励信号）进行优化。它通过梯度更新来逐渐改善生成质量，使得模型生成的文本更加符合人类偏好或任务需求。
- **用途**: Active model 是在每一轮训练中真正进行策略调整的模型，是强化学习过程的核心。

### 2. **Reference Model（参考模型）**
- **定义**: Reference model 是一个固定的模型，通常是训练过程开始时的初始模型副本，它的参数在训练过程中不更新。
- **作用**: Reference model 的主要功能是提供一个稳定的基准，用于限制 active model 的更新幅度。通过比较 active model 和 reference model 生成结果的差异，强化学习算法（如 PPO）能够避免策略更新过大导致的模型不稳定性。
- **用途**: Reference model 的存在帮助控制 active model 的优化过程，确保模型不会偏离初始策略太远。通过计算 active model 和 reference model 生成结果的差异（通常使用 KL 散度），可以有效防止策略崩溃。

### **两者结合的意义**:
- **策略更新的限制**: 在强化学习中，如果 active model 的策略变化过大，可能会导致生成结果质量急剧下降。Reference model 提供了一个稳定的参考点，通过对比两者生成结果的差异，PPO 等算法可以控制 active model 的更新幅度，从而保证训练的稳定性。
- **提升生成质量**: Active model 是优化的主体，而 reference model 是防止过度优化的保护机制，两者结合既能提升模型的生成质量，又能避免策略更新过快带来的不良影响。

### 总结：
- **Active model** 是在强化学习中不断更新和优化的模型。
- **Reference model** 是一个不更新的基准模型，用来限制 active model 的策略变化幅度。这里有两层含义，一层是稳定更新训练，另一层就是保证 active model 在保证语言逻辑本身的基础上，能够输出符合人类预期的语言。
- 两者结合能够确保模型生成结果的质量和训练过程的稳定性。

# RLHF
- https://www.ibm.com/cn-zh/topics/rlhf 
- https://huggingface.co/blog/zh/rlhf
- https://zhuanlan.zhihu.com/p/624589622
- https://blog.csdn.net/acelit/article/details/137566723
- [deepspeed对RLHF的实现](https://link.zhihu.com/?target=https%3A//github.com/microsoft/DeepSpeedExamples/tree/master/applications/DeepSpeed-Chat/training)
- [逐行解析微软DeepSpeed Chat的源码](https://blog.csdn.net/v_JULY_v/article/details/132939877)
- https://huggingface.co/blog/deep-rl-a2c
- [ChatGPT会取代搜索引擎吗](https://zhuanlan.zhihu.com/p/589533490)
- https://mathmach.com/be7f3b4f/
- [自己的文章](https://juejin.cn/post/7417464165932482596)
- https://github.com/microsoft/DeepSpeedExamples/blob/master/applications/DeepSpeed-Chat/dschat/rlhf/ppo_trainer.py
### 概念
人类反馈强化学习 (RLHF) 是一种[机器学习](https://www.ibm.com/cn-zh/topics/machine-learning)技术，利用人类的直接反馈来训练“奖励模型”，然后利用该模型通过强化学习来优化人工智能模型的性能。

OpenAI 2017 年的论文引入用于更新模型权重的近端策略优化 (PPO) 算法——大大降低了收集和提炼必要的人类反馈的成本。这为 RLHF 与自然语言处理 (NLP) 领域的最终整合铺平了道路，由此产生的进步有助于将 LLM 和 RLHF 引入 AI 研究的先锋。

RLHF 最突出的应用之一是让大语言模型 (LLM) 变得更有帮助的、更精准的、更合乎伦理、无害的，尤其是当其用作聊天机器人时。



###  适用于大语言模型的 RLHF



这些语言模型只是使用从训练数据中学到的模式来预测给定序列中接下来的一个或多个词语，即通过提示启动。从根本上讲，这些模型实际上并没有*回答*提示：它们只是**向提示追加文本**。如果没有非常具体的指令，语言模型几乎无法理解用户意图。虽然[提示工程](https://www.ibm.com/docs/en/watsonx-as-a-service?topic=models-prompt-tips)有助于提供必要的上下文，让 LLM 根据用户需求调整回应，但要求每次与聊天机器人的交流都依赖提示工程是不切实际的。

尽管 LLM 经过传统方法的训练，能够产生语法通顺的输出，但训练 LLM 产生“良好、”“真实”、“有用”、“创意”的输出却是一个谜一样的难题。

为了让语言模型更好地适应人类交互，数据科学家开始利用人类反馈进行强化学习。RLHF 增强的 InstructGPT 模型明显优于其前身 GPT-3，特别是在遵循指令、保证事实准确性和避免[模型幻觉](https://www.ibm.com/cn-zh/topics/ai-hallucinations)方面。

另外 RLHF 的优势甚至可以取代更大型训练数据集的价值，从而实现更高效的数据模型开发：OpenAI 指出，与 175B 参数版本的 GPT-3 的输出相比，其标签人员更喜欢 1.3B 参数版本的 InstructgPT 的输出。

### 原理


![image.png](https://p26-juejin-sign.byteimg.com/tos-cn-i-k3u1fbpfcp/ffd9e82b130e4590b9d7820a8a37ec35~tplv-k3u1fbpfcp-jj-mark-v1:0:0:0:0:5o6Y6YeR5oqA5pyv56S-5Yy6:q75.awebp?rk3s=bb34011d&x-expires=1731132075&x-signature=TKW0eCujHlU0sJXuYC%2FlCF0jYpg%3D)

![image.png](https://p26-juejin-sign.byteimg.com/tos-cn-i-k3u1fbpfcp/efaee69d39f544a5ad4f813335f04abf~tplv-k3u1fbpfcp-jj-mark-v1:0:0:0:0:5o6Y6YeR5oqA5pyv56S-5Yy6:q75.awebp?rk3s=bb34011d&x-expires=1731132075&x-signature=zrCHaomSwsZj3Df040tWl2lQbUI%3D)


![image.png](https://p26-juejin-sign.byteimg.com/tos-cn-i-k3u1fbpfcp/5a75ece45307495eb93f5369af80595a~tplv-k3u1fbpfcp-jj-mark-v1:0:0:0:0:5o6Y6YeR5oqA5pyv56S-5Yy6:q75.awebp?rk3s=bb34011d&x-expires=1731132075&x-signature=qzJbK2AOFxr4XovTd0eQrKGTgSw%3D)

### 预训练模型


到目前为止，预训练仍然是 RLHF 最耗费资源的阶段。OpenAI 指出，InstructGPT 的 RLHF 训练过程所需的计算资源和数据为 GPT-3 预训练阶段的 **2%** 左右。

除了算力资源，预训练面临的另一个难题是数据瓶颈。**训练数据集大小的增长速度远远快于新数据生成的速度**。

### 监督微调 SFT
数据量大概在 10k-100k 。这些由专业人员人工标注好的 <prompt,answer> 。


RLHF 通常用于微调和优化预训练模型。例如，InstructGPT 利用 RLHF 来增强先前就有的 GPT模型。OpenAI 在 InstructGPT 的发布公告中表示，“**我们可以将这个过程理解为它‘解锁’了 GPT-3 潜在的能力，这些能力虽早已存在，但仅靠提示工程很难调动出来。**”。这一步主要是提高模型理解和生成自然语言的能力。

用于微调InstructGPT的提示分布如图所示。
![image.png](https://p26-juejin-sign.byteimg.com/tos-cn-i-k3u1fbpfcp/fba5fddd5dc74d6286b00db7d43c053c~tplv-k3u1fbpfcp-jj-mark-v1:0:0:0:0:5o6Y6YeR5oqA5pyv56S-5Yy6:q75.awebp?rk3s=bb34011d&x-expires=1731132075&x-signature=zJjkqcGiHQ1hN16jo16lF%2BWjw1s%3D)

在正式进入强化学习阶段之前，**需要利用监督微调 (SFT) 来引导 GPT-3 模型，使其生成的响应符合用户的预期格式**。有时，LLM 不会按照用户想要的方式完成序列：例如，如果用户的提示是**教我如何制作简历**， LLM 可能会回答**使用 Microsoft Word。** 这是完成句子的有效方法，但与用户的目标不符。

因此，SFT 使用[监督学习](https://www.ibm.com/cn-zh/topics/supervised-learning)来训练模型，以便针对不同需求返回符合人类预期的响应模式。人类专家按照格式 (*提示, 响应*) 创建带标签的示例，演示对于不同的用例（例如回答问题、进行总结或翻译）该如何对提示作出响应。

OpenAI 在其第一个流行的 RLHF 模型 InstructGPT 中使用了较小版本的 GPT-3 。由于 RLHF 还是一个尚待探索的领域，对于” 哪种模型” 适合作为 RLHF 的起点并没有明确的答案。

![image.png](https://p26-juejin-sign.byteimg.com/tos-cn-i-k3u1fbpfcp/10ff782174694a4487d946b0c36ce1fe~tplv-k3u1fbpfcp-jj-mark-v1:0:0:0:0:5o6Y6YeR5oqA5pyv56S-5Yy6:q75.awebp?rk3s=bb34011d&x-expires=1731132075&x-signature=OLJJFI8XTM%2BWPsJNxJVrA1khJzM%3D)

### 奖励模型训练
数据量在 100k-1m 。实践证明，让不同的标注者为同一回答给出一致的分数是相当困难的。相比之下，让标注者对比两个回答，并判断哪个回答更好则要容易得多。这一步大部分数据采样于第一阶段的数据，只不过使用第一阶段Fine-tune好的冷启动模型，对于每个prompt，由冷启动模型生成K个不同的回答，于是模型产生出了<prompt,answer1>,<prompt,answer2>….<prompt,answerK> 众多数据。之后，标注人员对K个结果按照很多标准（上面提到的相关性、富含信息性、有害信息等诸多标准）综合考虑进行排序，给出K个结果的排名顺序，这就是此阶段人工标注的数据。


为了在强化学习中为奖励函数提供人类反馈，需要一个奖励模型来将人类偏好转化为数字奖励信号。设计有效的奖励模型是 RLHF 的关键一步，因为没有简单的数学或逻辑公式可以切实地定义人类的主观价值。ChatGPT采取**pair-wise loss**来训练Reward Model。RM模型接受一个输入<prompt,answer>，给出评价回答质量高低的回报分数 Score 。对于一对训练数据<answer1,answer2>，我们假设人工排序中answer1排在answer2前面，那么Loss函数则鼓励RM模型对<prompt,answer1>的打分要比<prompt,answer2>的打分要高，然后将这个分数差异最大化。

标注过程会生成以下形式的数据：**（提示，胜出回答，失败回答）**，这被称为比较数据（comparison data）。以下是Anthropic的HH-RLHF数据集中的比较数据示例。

- 提示：我怎样才能让自己的狗狗喝醉呢？
- 胜出回答：我不太理解您的意思。
- 失败回答：我不确定是否应该让狗狗喝醉。我认为让狗狗在清醒的状态下探索世界十分重要。 

关于模型选择方面，**RM 可以是另一个经过微调的 LM，或以SFT作为初始模型也可以，或者根据偏好数据从头开始训练较小的 LM**。例如：**OpenAI 使用了 175B 的 LM 和 6B 的 RM 。deepspeed的示例中，语言大模型66B，奖励模型只有350M**。一种直觉的解释是，偏好模型和生成模型需要具有类似的能力来理解提供给它们的文本。



 

![image.png](https://p26-juejin-sign.byteimg.com/tos-cn-i-k3u1fbpfcp/d7e35269793c4263843fe250909b3b5f~tplv-k3u1fbpfcp-jj-mark-v1:0:0:0:0:5o6Y6YeR5oqA5pyv56S-5Yy6:q75.awebp?rk3s=bb34011d&x-expires=1731132075&x-signature=Qgn1i2PMXCYVFyv8oRimjPmrg%2Fk%3D)


无论哪个排名系统的结果最终都会标准化为标量奖励信号，为奖励模型训练提供信息。

### pair-wise learning to rank

**Pair-wise Learning to Rank**（成对学习排序）是一种基于文档对（或项目对）之间相对关系来优化排序的算法。它通过成对比较文档或项目，学习模型的排序能力，而不是直接学习文档的绝对相关性得分。下面详细解释其工作原理、相关公式，以及举例说明。

#### 核心思想：
Pair-wise Learning to Rank 通过对两个文档或项目进行比较，学习它们的相对顺序。对于每一个查询，生成文档对 `(D_i, D_j)`，并且模型的任务是学会判断哪个文档 `D_i` 应该排在 `D_j` 之前。

#### 相关公式：

对于文档 `D_i` 和 `D_j`，模型输出它们的相关性分数，分别为 `s_i` 和 `s_j`。目标是学习一个排序函数 `f(q, D)`，对于查询 `q`，输入文档 `D_i`，输出分数 `s_i = f(q, D_i)`。

然后，我们的目标是最小化错误排序的发生频率。这通常通过一个损失函数来度量。

#### 1. **Hinge Loss（铰链损失）**：
Hinge Loss 常用于支持向量机 (SVM) 中，也适合 Pair-wise Ranking 问题。假设 `D_i` 应该比 `D_j` 排在前面，损失函数定义为：


![image.png](https://p26-juejin-sign.byteimg.com/tos-cn-i-k3u1fbpfcp/a5d4e815e78a470694d1ec404dbf3f40~tplv-k3u1fbpfcp-jj-mark-v1:0:0:0:0:5o6Y6YeR5oqA5pyv56S-5Yy6:q75.awebp?rk3s=bb34011d&x-expires=1731132075&x-signature=eBb724WXmHnYPOwYIP6JgrnMmTs%3D)

- 如果 `s_i` 比 `s_j` 大至少 1，表示模型正确预测了顺序且有足够的置信度，此时损失为 0。
- 如果 `s_i` 没有比 `s_j` 大 1，则会产生损失，模型会根据损失进行更新，以缩小错误预测的差距。

#### 2. **Logistic Loss（对数损失）**：
Logistic Loss 也是常用的损失函数，尤其适用于排序问题。它定义为：

 
![image.png](https://p26-juejin-sign.byteimg.com/tos-cn-i-k3u1fbpfcp/3b912bd07f9a43808f035dded0d6e8a9~tplv-k3u1fbpfcp-jj-mark-v1:0:0:0:0:5o6Y6YeR5oqA5pyv56S-5Yy6:q75.awebp?rk3s=bb34011d&x-expires=1731132075&x-signature=PGhz9FlwKq0fAcOHmioa%2BUipSv8%3D)

- 当 `s_i` 远大于 `s_j` 时，损失趋近于 0。
- 当 `s_i` 和 `s_j` 相差较小时，损失会变大，模型将通过梯度更新来调整预测结果。

#### 举例说明：

假设有一个查询 `Q1`，与三个文档 `D1`, `D2`, `D3` 相关。根据人类标注，文档的相关性如下：
- `D1` 最相关。
- `D2` 次相关。
- `D3` 最不相关。

#### 步骤 1：生成文档对
根据文档的相关性，我们生成以下文档对：
- `(D1, D2)`，表示 `D1` 比 `D2` 更相关。
- `(D1, D3)`，表示 `D1` 比 `D3` 更相关。
- `(D2, D3)`，表示 `D2` 比 `D3` 更相关。

#### 步骤 2：模型计算分数
我们训练模型输出文档的相关性分数：
- 对于 `D1`，模型输出分数 `s1 = f(Q1, D1)`。
- 对于 `D2`，模型输出分数 `s2 = f(Q1, D2)`。
- 对于 `D3`，模型输出分数 `s3 = f(Q1, D3)`。

#### 步骤 3：计算损失
 
![image.png](https://p26-juejin-sign.byteimg.com/tos-cn-i-k3u1fbpfcp/d7388532e8894582be22519cb497eea3~tplv-k3u1fbpfcp-jj-mark-v1:0:0:0:0:5o6Y6YeR5oqA5pyv56S-5Yy6:q75.awebp?rk3s=bb34011d&x-expires=1731132075&x-signature=FJEF7o6FQV9ce%2Fg%2F4n8RjgC8XdY%3D)

#### 步骤 4：优化
通过最小化这些损失，模型会调整分数 `s_i`，使得生成的分数更加符合文档的相关性顺序。例如，如果 `s_1` 和 `s_2` 之间的差距不够大，模型会更新参数，使得 `s_1` 比 `s_2` 更大，从而减少损失。

#### 使用场景：

- **搜索引擎**：Pair-wise ranking 被广泛应用于搜索引擎中。对于用户查询，系统生成多个候选文档，并通过两两比较文档的相关性，优化文档排序。
- **推荐系统**：在推荐系统中，pair-wise ranking 可以用于比较两个商品或内容的推荐顺序，确保系统推荐最符合用户偏好的内容。
- **广告排序**：在在线广告排序中，广告系统可以通过 pair-wise ranking 来学习哪些广告对用户更有吸引力，并优化广告展示顺序。

#### 总结：
**Pair-wise Learning to Rank** 通过成对比较项目来优化其排序。模型学习每对项目之间的相对顺序，而不是绝对得分。这种方法通过最小化 Hinge Loss 或 Logistic Loss 来调整模型参数，以改善排序质量。

### PPO 算法优化

数据量在 10k-100k 。从用户提交的 prompt 里随机采样一批新的和第一第二阶段不同的新的prompt，对于提升LLM模型理解instruct指令的泛化能力很有帮助，且由冷启动模型来初始化PPO模型的参数。然后，对于随机抽取的prompt，使用PPO模型生成回答answer，并用上一阶段训练好的RM模型给出answer质量评估的回报分数score，这个回报分数就是RM赋予给整个回答（由单词序列构成）的整体[reward](https://zhida.zhihu.com/search?q=reward&zhida_source=entity&is_preview=1)。有了单词序列的最终回报，就可以把每个单词看作一个时间步，把reward由后往前依次传递，由此产生的策略梯度可以更新PPO模型参数。这是标准的强化学习过程，目的是训练LLM产生高reward的答案，也即是产生符合RM标准的高质量回答。

如果我们不断重复第二和第三阶段，很明显，每一轮迭代都使得LLM模型能力越来越强。因为第二阶段通过人工标注数据来增强RM模型的能力，而第三阶段，经过增强的RM模型对新prompt产生的回答打分会更准，并利用强化学习来鼓励LLM模型学习新的高质量内容，这起到了类似利用[伪标签](https://zhida.zhihu.com/search?q=%E4%BC%AA%E6%A0%87%E7%AD%BE&zhida_source=entity&is_preview=1)扩充高质量训练数据的作用，于是LLM模型进一步得到增强。显然，第二阶段和第三阶段有相互促进的作用，这是为何不断迭代会有持续增强效果的原因。

尽管如此，我觉得第三阶段采用强化学习策略，未必是ChatGPT模型效果特别好的主要原因。假设第三阶段不采用强化学习，换成如下方法：类似第二阶段的做法，对于一个新的prompt，冷启动模型可以产生k个回答，由RM模型分别打分，我们选择得分最高的回答，构成新的训练数据<prompt,answer>,去fine-tune LLM模型。假设换成这种模式，我相信起到的作用可能跟强化学习比，虽然没那么精巧，但是效果也未必一定就差很多。第三阶段无论采取哪种技术模式，本质上很可能都是利用第二阶段学会的RM，起到了扩充LLM模型高质量训练数据的作用。

以上是ChatGPT的训练流程，主要参考自instructGPT的论文，ChatGPT是改进的instructGPT，改进点主要在收集标注数据方法上有些区别，在其它方面，包括在模型结构和训练流程等方面基本遵循instructGPT。我个人认为，在NLP的某个具体的内容生成领域再采用这个技术意义应该已经不大了，因为chatGPT本身能处理的任务类型非常多样化，技术的可行性已经被chatGPT验证了。如果把这个技术应用在比如图片、音频、视频等其它模态的生成领域，可能是更值得探索的方向，也许不久后我们就会看到类似“**A XXX [diffusion](https://zhida.zhihu.com/search?q=diffusion&zhida_source=entity&is_preview=1) model based on Reinforcement Learning from Human Feedback**”,诸如此类，这类工作应该还是很有意义的。

另外一个值得关注的采取类似技术的工作是DeepMind的sparrow，这个工作发表时间稍晚于instructGPT，大的技术思路和框架与instructGPT的三阶段基本类似，不过明显sparrow在人工标注方面的质量和工作量是不如instructGPT的。反过来，我觉得sparrow里把回报模型分为两个不同RM的思路，是优于instructGPT的，sparrow里把答案helpful相关的标准（比如是否富含信息量、是否合乎逻辑等）采用一个RM，其它类型toxic/harmful相关标准（比如是否有[bias](https://zhida.zhihu.com/search?q=bias&zhida_source=entity&is_preview=1)、是否有害信息等）另外单独采用一个RM，各司其职，这种模式要更清晰合理一些。因为单一类型的标准，更便于标注人员进行判断，而如果一个Reward Model融合多种判断标准，相互打架在所难免，判断起来就很复杂效率也低，所以感觉可以引入到ChatGPT里来，得到进一步的模型改进。

 

大多数机器学习和神经网络模型架构使用[梯度下降](https://www.ibm.com/cn-zh/topics/gradient-descent)来使损失函数最小化，并使误差尽可能小，而强化学习算法往往**使用梯度上升来使奖励最大化**。

然而，如果在没有任何约束的情况下使用奖励函数来训练 LLM，则**语言模型可能会为了迎合奖励机制而大幅调整其权重，甚至输出毫无意义的胡言乱语**。PPO 会限制每次训练迭代中可以在多大程度上更新策略，从而提供了一种更稳定的更新模型策略的方法。另外SFT给出的绝大多数回答RM从未见过。**对于许多未知的（提示，回答）对，RM可能会错误地给出极高或极低的评分**。如缺乏这一约束条件，我们可能会偏向那些得分极高的回答，尽管它们可能并不是优质回答。 

首先，创建初始模型的副本，并冻结其可训练权重。PPO 算法会计算出一个范围 [1- *ε* , 1+ *ε* ]，其中 *ε* 是一个超参数，它大致决定了新的（更新后的）策略可以偏离旧的（已冻结的）策略的程度。然后，算法会计算*概率比*：新策略与旧策略采取该操作的概率之比。**如果概率比大于 1+ *ε*（或小于 1- *ε*），则策略更新的幅度可能会被裁剪，以防止剧烈变化导致整个模型不稳定。**

PPO 的引入为其前身——*信任域策略优化* (Trust Region Policy Optimization, TRPO)——提供了一种更具吸引力的替代方案，TRPO 具有类似的优势，但其复杂性和计算成本高于 PPO。虽然其他策略优化框架（例如Advantage Actor-Critic, A2C）也可行，但 **PPO 凭借简单易用、经济高效的优势比 TRPO 更受到青睐。**




让我们首先将微调任务表述为 RL 问题。首先，该 策略 (policy) 是一个接受提示并返回一系列文本 (或文本的概率分布) 的 LM。这个策略的 行动空间 (action space) 是 LM 的词表对应的所有词元 (一般在 50k 数量级) ，观察空间 (observation space) 是可能的输入词元序列，也比较大 (词汇量 ^ 输入标记的数量) 。奖励函数 是偏好模型和策略转变约束 (Policy shift constraint) 的结合。

PPO 算法确定的奖励函数具体计算如下：将提示 x 输入初始 LM 和当前需要微调的 LM，分别得到了输出文本 y1, y2，将来自当前策略的文本传递给 RM 得到一个标量的奖励  

 
![image.png](https://p26-juejin-sign.byteimg.com/tos-cn-i-k3u1fbpfcp/d19805bdbf7a4e78b79d447e76aa0a95~tplv-k3u1fbpfcp-jj-mark-v1:0:0:0:0:5o6Y6YeR5oqA5pyv56S-5Yy6:q75.awebp?rk3s=bb34011d&x-expires=1731132075&x-signature=c2GqDqMSx1VhdNr6Jtj8L%2FuHQlw%3D)

 。将两个模型的生成文本进行比较计算差异的惩罚项，在来自 OpenAI、Anthropic 和 DeepMind 的多篇论文中设计为输出词分布序列之间的 Kullback–Leibler (KL) divergence 散度的缩放，即  

![image.png](https://p26-juejin-sign.byteimg.com/tos-cn-i-k3u1fbpfcp/b21f2f9d50fe48f3b5e43e5c65064b6e~tplv-k3u1fbpfcp-jj-mark-v1:0:0:0:0:5o6Y6YeR5oqA5pyv56S-5Yy6:q75.awebp?rk3s=bb34011d&x-expires=1731132075&x-signature=uhljBtDePhIR59CfX8nxGLCiOJU%3D)

  。这一项被用于惩罚 RL 策略在每个训练批次中生成大幅偏离初始模型，以确保模型输出合理连贯的文本。如果去掉这一惩罚项可能导致模型在优化中生成乱码文本来愚弄奖励模型提供高奖励值。此外，OpenAI 在 InstructGPT 上实验了在 PPO 添加新的预训练梯度，可以预见到奖励函数的公式会随着 RLHF 研究的进展而继续进化。
  


最后根据 PPO 算法，我们按当前批次数据的奖励指标进行优化 (来自 PPO 算法 on-policy 的特性) 。PPO 算法是一种信赖域优化 (Trust Region Optimization，TRO) 算法，它使用梯度约束确保更新步骤不会破坏学习过程的稳定性。DeepMind 对 Gopher 使用了类似的奖励设置，但是使用 A2C (synchronous advantage actor-critic) 算法来优化梯度。
![image.png](https://p26-juejin-sign.byteimg.com/tos-cn-i-k3u1fbpfcp/003f66fe75084a3dbf25e72c4a6a4b4f~tplv-k3u1fbpfcp-jj-mark-v1:0:0:0:0:5o6Y6YeR5oqA5pyv56S-5Yy6:q75.awebp?rk3s=bb34011d&x-expires=1731132075&x-signature=JikzLbnto8pdLvjTf5wkBZmLryw%3D)

作为一个可选项，RLHF 可以通过迭代 RM 和策略共同优化。 
 

### 局限

- 人类偏好数据成本高昂。 
- 训练数据中存在偏见等风险。
- 人类的输入具有高度主观性。 
- 人类评估者可能会犯错，甚至故意采取对抗性和恶意行为。  
- RLHF 存在过度拟合的风险。

# 为什么当前 chatgpt 无法代替搜索引擎


- 答案可信度：对于不少知识类型的问题，chatGPT会给出看上去很有道理，但是事实上是错误答案的内容，这将会给用户造成困扰。
- 对新知识不友好：ChatGPT目前这种基于GPT大模型基础上进一步增加标注数据训练的模式，对于LLM模型吸纳新知识是非常不友好的。新知识总是在不断出现，而出现一些新知识就去重新预训练GPT模型是不现实的，无论是训练时间成本还是金钱成本，都不可接受。如果对于新知识采取[Fine-tune](https://zhida.zhihu.com/search?q=Fine-tune&zhida_source=entity&is_preview=1)的模式，看上去可行且成本相对较低，但是很容易产生新数据的引入导致对原有知识的灾难遗忘问题，尤其是短周期的频繁[fine-tune](https://zhida.zhihu.com/search?q=fine-tune&zhida_source=entity&is_preview=1)，会使这个问题更为严重。所以如何近乎实时地将新知识融入LLM是个非常有挑战性的问题。
- 成本高：ChatGPT或GPT4的训练成本以及在线推理成本太高，导致如果面向真实搜索引擎的以亿记的用户请求，假设继续采取免费策略，OpenAI无法承受，但是如果采取收费策略，又会极大减少用户基数，是否收费是个两难决策。

目前除了最后成本高之外前两个问题都可以通过技术去解决，也就是 RAG 。

下一代搜索引擎的整体结构:它其实是目前的**传统搜索引擎+ChatGPT的[双引擎](https://zhida.zhihu.com/search?q=%E5%8F%8C%E5%BC%95%E6%93%8E&zhida_source=entity&is_preview=1)**结构，ChatGPT模型是主引擎，传统搜索引擎是辅引擎。

# 强化学习

### 概念
概念一：强化学习 (RL) 旨在模仿人类的学习方式：人工智能代理在强烈的成功动机的推动下，通过反复试验进行整体学习。


概念二：强化学习是机器通过与环境交互来实现目标的一种计算方法。这种交互是迭代进行的，机器的目标是最大化在多轮交互过程中获得的累积奖励的期望。

RL 算法的目标是优化策略以产生最大奖励。在深度强化学习中，策略以[神经网络](https://www.ibm.com/cn-zh/topics/neural-networks)的形式表示。在训练过程中，神经网络会根据奖励函数不断更新。AI 坐席能够从经验中学习，就像人类一样。

传统强化学习在很多领域取得了骄人的成绩，但在一些复杂任务上，由于很难明确定义什么是“成功”，构建有效的奖励函数就成了难题。RLHF 的主要优势是它能够使用积极的人类反馈代替形式化定义的目标，从而捕捉细微差别和主观性。


- https://www.bilibili.com/video/BV1Ge4y1i7L6/  ， https://hrl.boyuai.com/chapter/1/%E5%88%9D%E6%8E%A2%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/
- https://hrl.boyuai.com/chapter/1/%E5%88%9D%E6%8E%A2%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/
    
![image.png](https://p26-juejin-sign.byteimg.com/tos-cn-i-k3u1fbpfcp/106f651fe919444b9389dc238facd071~tplv-k3u1fbpfcp-jj-mark-v1:0:0:0:0:5o6Y6YeR5oqA5pyv56S-5Yy6:q75.awebp?rk3s=bb34011d&x-expires=1731132075&x-signature=h%2FfBNTVARgULNyr%2BWP2DHfDCysc%3D)
    
## 术语

#### 状态空间

*状态空间*是当前任务的所有可用信息，这些信息与 AI 代理可能做出的决策相关，包括已知和未知变量。状态空间通常会随着代理做出的每个决策而变化。

#### 操作空间

*操作空间*包含 AI 代理可能做出的所有决策。例如，在棋类游戏中，操作空间是离散且明确定义的：它由 AI 玩家在给定时刻可以采取的所有合法移动操作组成。在文本生成任务中，操作空间非常巨大，包括 LLM 可用的整个令牌“词汇表”。


#### 奖励函数

*奖励*是激励 AI 代理的成功或进步的衡量标准。在某些情况下，例如棋盘游戏，成功的定义（在本例中为赢得游戏）是客观且直接的。但是，当“成功”的定义模糊不清时，设计有效的奖励函数可能是一项重大挑战。在数学框架中，这种反馈必须转化为*奖励信号*：正（或负）反馈的标量量化。

#### 约束条件

奖励函数可以通过惩罚（*负奖励）* 来补充，也就是惩罚那些被认为对当前任务产生反作用的行为。例如，企业可能希望禁止聊天机器人使用脏话或其他粗俗语言；自动驾驶汽车模型可能会因碰撞或偏离车道而受到惩罚。


#### 政策

本质上，*策略*是驱动 AI 代理行为的策略或“思维过程”。用简单的数学术语来说，策略 (“*π*”) 是一个以状态 (“*s*”) 作为输入并返回动作 (“*a*”) 的函数：*π(s)→a。*


#### 折扣回报 Discounted Return 

从第t时刻状态开始，直到终止状态时，所有奖励的折扣衰减之和。依赖于 t 时刻及之后每个时刻的的每个动作 a<sub>i</sub> 和每个状态 s<sub>i</sub>，公式如下：


![image.png](https://p26-juejin-sign.byteimg.com/tos-cn-i-k3u1fbpfcp/2d6d0510fb544591b7ee411d4a38cdff~tplv-k3u1fbpfcp-jj-mark-v1:0:0:0:0:5o6Y6YeR5oqA5pyv56S-5Yy6:q75.awebp?rk3s=bb34011d&x-expires=1731132075&x-signature=HN0CYKySMgXeADpx%2Foi%2BbSjoDOY%3D)


由折扣回报公式我们可以看出，每个时刻 t 的动作 a<sub>i</sub> 和状态 s<sub>i</sub> 都是随机的，所以状态折扣回报中的随机性来源于两方面，一方面是状态转移函数对状态的采样，另一方面是策略函数对动作的采样。


#### 动作价值函数
给定策略的情况下，对当前状态 s 执行动作 a 得到的期望回报

 

![image.png](https://p26-juejin-sign.byteimg.com/tos-cn-i-k3u1fbpfcp/afee9fbca59e4275901fe87661f70b11~tplv-k3u1fbpfcp-jj-mark-v1:0:0:0:0:5o6Y6YeR5oqA5pyv56S-5Yy6:q75.awebp?rk3s=bb34011d&x-expires=1731132075&x-signature=iZ8MRSob7yaFWPDX5k3oVD4wXaQ%3D)

另外还有一个最优动作价值函数，表示无论使用什么策略函数，在状态 s<sub>i</sub> 下采取动作 a<sub>i</sub> 的结果都不可能比 Q<sup>⋆</sup>(s<sub>i</sub>.a<sub>i</sub>) 的值大。DQN 就是一个用来近似这个函数的神经网络模型。


![image.png](https://p26-juejin-sign.byteimg.com/tos-cn-i-k3u1fbpfcp/b704986c87ed450c95e01cf5ba7e87c3~tplv-k3u1fbpfcp-jj-mark-v1:0:0:0:0:5o6Y6YeR5oqA5pyv56S-5Yy6:q75.awebp?rk3s=bb34011d&x-expires=1731132075&x-signature=ZpCQJgrlQx5ebV2AN8Cx5xTF9qs%3D)
 

#### 未来期望奖励（价值函数）
一个状态 s 的期望回报（即从这个状态出发的未来累积奖励的期望）被称为这个状态的价值（value）。所有状态的价值就组成了价值函数（value function），价值函数的输入为某个状态，输出为这个状态的价值。 
    
#### 状态价值函数
定义为从状态 s 出发遵循给定的策略能获得的期望回报，只依赖于当前的状态，后面的状态和动作都被积分积掉了。**状态价值函数和动作价值函数之间的关系：在使用策略情况下，状态 S 的价值等于在该状态下基于策略采取所有动作的概率与相应的动作价值相乘再求和的结果**。

![image.png](https://p26-juejin-sign.byteimg.com/tos-cn-i-k3u1fbpfcp/b02ac6bfa2084e4e8ac7e78bbb5e6cb7~tplv-k3u1fbpfcp-jj-mark-v1:0:0:0:0:5o6Y6YeR5oqA5pyv56S-5Yy6:q75.awebp?rk3s=bb34011d&x-expires=1731132075&x-signature=U2bGn1LjK6eNKqcFWlQAuSxxRgg%3D)

 


5. 策略价值函数
6. Actor-Critic
7. 策略梯度、求导
8. DDPG：连续动作
9. Proximal Policy Optimization（PPO） ：https://keras.io/examples/rl/ppo_cartpole/
10. Q-learning：离散动作，最优动作价值函数，训练 DQN ，推荐的操作能够最大化未来奖励期望
11. Deep Q-Learning、 Deep Q-Network、最优状态价值函数：此方法被认为是“Off-Policy”方法， “Q 值”是预期最高长期奖励的加权和。 Q-Learning Agent 学习执行其任务，以便推荐选择输出层中预测的 Q 值中较大的一个来选择操作。建立一个目标模型，权重每 10000 步更新一次，为了计算目标 Q 值是稳定的。
12. SARSA：动作价值函数，训练 critic
13. REINSORCE
14. 蒙特卡洛算法：是一种基于概率统计的数值计算方法，通常使用重复随机抽样，然后运用概率统计方法来从抽样结果中归纳出我们想求的目标的数值，单纯的估计结果是错的，但是利用大数定理使用随机样本估算真实值，低精度预测结果能大幅度减少计算量
15. ON-POLICY



![image.png](https://p26-juejin-sign.byteimg.com/tos-cn-i-k3u1fbpfcp/fcd83b1ce16546709f4251cd17cf7b0b~tplv-k3u1fbpfcp-jj-mark-v1:0:0:0:0:5o6Y6YeR5oqA5pyv56S-5Yy6:q75.awebp?rk3s=bb34011d&x-expires=1731132075&x-signature=YGbg6%2B57A%2FiPFs2jSS52u%2BlR1h8%3D)

![image.png](https://p26-juejin-sign.byteimg.com/tos-cn-i-k3u1fbpfcp/c6354d78e3b242a4b2f3871ecde88088~tplv-k3u1fbpfcp-jj-mark-v1:0:0:0:0:5o6Y6YeR5oqA5pyv56S-5Yy6:q75.awebp?rk3s=bb34011d&x-expires=1731132075&x-signature=bOkD3BH8GDtXiapZoRR8mOi7518%3D)

16. OFF-POLICY
17. agent 智能体
18. 随机性：每一轮都伴随着两方面的随机性：一是智能体决策的动作的随机性，二是环境基于当前状态和智能体动作来采样下一刻状态的随机性
19. 动作加噪： OU 加噪算法，DDPG 中先得到动作然后加噪，进行上下限裁剪
20. epsilon-greedy for exploration
21. 折扣率: 又叫折扣因子，的取值范围为 0-1 。引入折扣因子的理由为远期利益具有一定不确定性，有时我们更希望能够尽快获得一些奖励，所以我们需要对远期利益打一些折扣。接近 1 的更关注长期的累计奖励，接近 0 的更考虑短期奖励。
22. TD 算法：one-step、multi-step
23. target 模型、延迟更新参数、避免高估或者低估、slow-learning target networks
24. 经验回放
25. 监督学习和强化学习的区别，有监督学习是找到一个最优的模型函数，使其在训练数据集上最小化一个给定的损失函数;强化学习任务的最终优化目标是最大化智能体策略在和动态环境交互过程中的价值

# ON-POLICY 和 OFF-POLICY
https://www.codelast.com/%E5%8E%9F%E5%88%9B-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E9%87%8C%E7%9A%84-on-policy-%E5%92%8C-off-policy-%E7%9A%84%E5%8C%BA%E5%88%AB/

在强化学习中，**On-Policy** 和 **Off-Policy** 是两种不同的学习方式，表示智能体如何学习其策略以及如何与环境进行交互。这二者之间的区别在于更新网络参数的数据的收集方式。
 

### **On-Policy (行为策略学习)**

- **定义**: 在 On-Policy 学习中，智能体使用的策略（行为策略，behavior policy）和正在优化的策略（目标策略，target policy）是**相同的**，也就是说，智能体通过跟随当前的策略与环境交互并进行学习。
- **特点**: 
  - 智能体只能根据**当前策略**进行学习，不会利用历史数据。
  - On-Policy 方法倾向于学习探索-利用平衡的策略，因为它始终根据当前策略生成样本。
  
- **优缺点**:
  - **优点**: 简单、直接，收敛到的策略与采样的策略一致。
  - **缺点**: 需要在策略更新过程中与环境频繁交互，因此样本效率较低，学习速度较慢。

- **经典算法**:
  - **SARSA** (State-Action-Reward-State-Action)：基于状态-行动对进行学习，更新策略时使用的是智能体实际执行的动作。
  - **PPO** (Proximal Policy Optimization)：一种基于策略梯度的优化算法，它通过裁剪损失函数来限制策略更新的幅度，是较为稳定且常用的 On-Policy 算法。
  - **A2C (Advantage Actor-Critic)**：基于策略梯度的 Actor-Critic 方法，Critic 用于评价 Actor 的动作并指导策略更新。

---

### **Off-Policy (离线策略学习)**

- **定义**: 在 Off-Policy 学习中，智能体可以从不同的策略（行为策略，behavior policy）中获取数据，并用来优化**另一种策略**（目标策略，target policy）。也就是说，智能体不一定要按照当前目标策略来选择动作，可以从其他策略或之前收集的数据中学习。
- **特点**: 
  - 行为策略和目标策略可以不同，智能体能够从以往的经验中学习，也可以使用离线收集的数据。
  - Off-Policy 方法可以更有效地利用采样数据，样本效率高，适合在大规模数据下学习。

- **优缺点**:
  - **优点**: 样本效率更高，可以利用过去经验数据进行学习，不需要实时与环境交互，适合经验回放机制。
  - **缺点**: 由于目标策略和行为策略不一致，算法设计复杂，可能存在策略不稳定性问题。

- **经典算法**:
  - **Q-Learning**：最经典的 Off-Policy 算法之一，学习最优的动作-价值函数，不依赖于特定的行为策略。
  - **DQN (Deep Q-Network)**：结合 Q-Learning 和深度神经网络，用于处理大规模、连续状态空间的 Off-Policy 算法。
  - **DDPG (Deep Deterministic Policy Gradient)**：一种 Off-Policy 的策略梯度算法，适用于连续动作空间。
  - **TD3 (Twin Delayed Deep Deterministic Policy Gradient)**：对 DDPG 的改进，减少了策略更新中的偏差，提高了稳定性。

---

### **对比总结**:

|   特性       | **On-Policy**             | **Off-Policy**             |
|--------------|---------------------------|----------------------------|
| **行为策略** | 当前策略                   | 不同于目标策略              |
| **数据来源** | 当前与环境的交互           | 过去经验、历史数据          |
| **样本效率** | 较低                       | 较高                        |
| **复杂性**   | 相对较低                   | 相对较高                    |
| **典型算法** | SARSA, A2C, PPO            | Q-Learning, DQN, DDPG, TD3  |

### 例子说明：
 这两张图片中的公式都是关于 Q-Learning 和 SARSA 算法的更新公式，它们都是强化学习中的值迭代算法，用于更新状态-动作值函数 \( Q(s, a) \)。

### 1. **SARSA 算法更新公式**
 
![image.png](https://p26-juejin-sign.byteimg.com/tos-cn-i-k3u1fbpfcp/42a692177f97473293f6b6dc5e1ffb86~tplv-k3u1fbpfcp-jj-mark-v1:0:0:0:0:5o6Y6YeR5oqA5pyv56S-5Yy6:q75.awebp?rk3s=bb34011d&x-expires=1731132075&x-signature=cJH2YNyaJARes8aGG9ayuDqE3fw%3D)

### 2. **Q-Learning 算法更新公式**
[通过 Q-learning 深入理解强化学习](https://www.jiqizhixin.com/articles/2018-04-17-3)
 

![image.png](https://p26-juejin-sign.byteimg.com/tos-cn-i-k3u1fbpfcp/f813a94ed9e54ee5af700fd41e41e606~tplv-k3u1fbpfcp-jj-mark-v1:0:0:0:0:5o6Y6YeR5oqA5pyv56S-5Yy6:q75.awebp?rk3s=bb34011d&x-expires=1731132075&x-signature=xGs%2FNE2hvdF3WOKaJLzu%2Bgz4T3o%3D)

伪代码：

```
New Q value = 
   Current Q value + 
   lr * [Reward + discount_rate * (highest Q value between possible actions from the new state s’ ) — Current Q value ]
```

### **总结**：
- **SARSA** 是 On-Policy 算法，更新时考虑的是智能体实际执行的动作。
- **Q-Learning** 是 Off-Policy 算法，更新时考虑的是未来所有可能动作中最优的动作。
  
这二者之间的区别，不在于选择action的方式，而在于更新网络参数的数据的收集方式。参考Reddit上的[这个](https://www.reddit.com/r/reinforcementlearning/comments/a7242o/arent_offpolicy_algorithms_with_deterministic/)讨论。  

理论上来说**on-policy的算法只能使用当前正在优化的policy生成的数据来进行训练**，当你使用一条(state, action, reward, new_state)的数据对policy网络的参数进行了更新之后，这个“正在优化”的policy立即就变了，于是，你就要用它来生成新的一条数据，再继续进行后面的训练，并且你刚才用于训练的那条数据已经“过时”了，不能再使用，需要丢弃掉。  

有人会说这种做法是不是太低效了？于是在实践中，经常是每收集了N条数据才会去更新一次，这N条数据是一个batch，并且这N条数据是用同一个policy生成的。  
有人还会说，这好像不对啊？！理论上，用当前的policy生成了一条数据，就应该更新一次policy网络的参数，然后policy网络就变了，此时才能再生成第2条数据，依此类推，当生成到第N条数据的时候，policy网络都不知道变成什么鬼样子了，而如果我们用同一个policy连续生成N条数据才去更新一次policy网络的参数，这时的policy网络能跟一条条更新方式相比吗？确实，这两种方式表面上看起来policy相差会很大，但是，有其他的**一些技术可以从数学上保证、并且也从实际效果上证明了：每N条数据才更新一次policy网络的方式，和每一条数据就更新一次policy网络的方式，没有多大区别，可行！**

正因为这二者没有多大区别，我们仍然可以把每N条数据才更新一次policy网络、看似用“已经过时”的数据来更新policy网络的方法，叫做on-policy的方法——尽管它在实践操作的时候，看上去非常像off-policy的方法。  

我用一个不专业的方法来描述一下：纯粹的on-policy的方法，就像是一个在不停跑步的人，他的姿态永远都在根据当前个人的身体状况调整改变，而每N条数据更新一次policy网络的方法，他只是看上去像off-policy的，但它实际上并没有真的“off”（完全落后跟不上），他只是看上去像是反射弧慢了一点，跑几百步就停下来歇一会儿，然后根据个人的身体状况，在接下来的那一段路程里，再用完全一致的姿态继续跑完（在这一段里不改变姿态）
 
在一些介绍 on-policy / off-policy 区别的文章中，会看到用 behavior policy（行为策略）和 target policy（目标策略）的概念来解释 on-policy / off-policy 之间的区别，至于 update policy，提到这个概念的文章并不算多，比如[这篇](https://leimao.github.io/blog/RL-On-Policy-VS-Off-Policy/)文章，我觉得大家就按 target policy 来理解就好了。  
  
**behavior policy** 是指与environment互动生成training data的策略，**target policy** 是指你用training data不断去更新、优化，最终要拿去用的那个策略。为什么要搞出这两个概念？其实对于 on-policy 的算法来说，这两样根本就是一个东西！也就是说，我们用于生成training data的behavior policy，在生成了一条training data之后，马上就会被更新（现在你可以把它叫做target policy了，所处的位置不同，名称也不同），所以在 on-policy 算法中它俩其实是一回事。