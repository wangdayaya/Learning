![](https://p0-xtjj-private.juejin.cn/tos-cn-i-73owjymdk6/767c9bebbea5446ab2137e7c83780153~tplv-73owjymdk6-jj-mark-v1:0:0:0:0:5o6Y6YeR5oqA5pyv56S-5Yy6IEAg5oiR5piv546L5aSn5L2g5piv6LCB:q75.awebp?policy=eyJ2bSI6MywidWlkIjoiNTM2MjE3NDA1ODk1MTQ5In0%3D&rk3s=f64ab15b&x-orig-authkey=f32326d3454f2ac7e96d3d06cdbb035152127018&x-orig-expires=1750124589&x-orig-sign=xfO1oW3ypo8Fp5AmHbczlOsFVvM%3D)

<https://mp.weixin.qq.com/s/PI47n38yfDDw85KPNiNzDg> <https://arxiv.org/pdf/2506.01844>

<https://github.com/openvla/openvla> <https://arxiv.org/pdf/2406.09246>

<https://arxiv.org/pdf/2505.04769>

经典 VLA 模型：RT1、RT2、OpenVLA、Pi-0、SmolVLA

# 摘要

*   视觉-语言-行动 (VLA)模型标志着人工智能的一次变革性进步，旨在将感知、自然语言理解和具身行动统一在一个单一的计算机框架内。
*   这篇基础综述全面总结了视觉-语言-行动模型的最新进展。我们首先建立VLA系统的概念基础，追溯它们从跨模态学习架构到紧密集成视觉-语言模型(VLMs)、动作规划器和层次控制器的一般性代理的演变过程。
*   我们的方法采用文献综述框架，涵盖了过去三年发表的80多个VLA模型。
*   关键进展领域包括体系结构创新、参数高效训练策略和实时推理加速。
*   我们探索了人形机器人、自动驾驶汽车、医疗和工业机器人、精准农业以及增强现实导航等多种应用领域。
*   综述还进一步讨论了实时控制、多模态动作表示、系统可扩展性、对未见过的任务的泛化以及道德部署风险等主要挑战。
*   借鉴最先进的技术，我们提出了针对性的解决方案，包括代理式A适应、跨身体泛化以及统一的神经符号规划。

# 视觉-语言-动作模型的概念

![](https://p0-xtjj-private.juejin.cn/tos-cn-i-73owjymdk6/e47eaa628daf4f529e163f78f881b4c1~tplv-73owjymdk6-jj-mark-v1:0:0:0:0:5o6Y6YeR5oqA5pyv56S-5Yy6IEAg5oiR5piv546L5aSn5L2g5piv6LCB:q75.awebp?policy=eyJ2bSI6MywidWlkIjoiNTM2MjE3NDA1ODk1MTQ5In0%3D&rk3s=f64ab15b&x-orig-authkey=f32326d3454f2ac7e96d3d06cdbb035152127018&x-orig-expires=1750124589&x-orig-sign=0cotBhIepTkFdi65tlS%2FysIBvYc%3D)

1.  VLA 模型代表一类新型智能系统，它们能够联合处理视觉输入、解读自然语言并在动态环境中生成可执行动作。从技术上讲，VLA 结合视觉编码器（例如 CNN、ViT）、语言模型（例如 LLM、Transformer）以及策略模块或规划器，以实现任务条件控制，所有这些都通过端到端学习而非孤立的子系统实现，解决了视觉识别、语言理解和运动执行之间长期以来的脱节的困境。这些模型通常采用多模态融合技术（例如交叉注意、级联嵌入或 token 统一），将感官观察结果与文本指令对齐。典型的 VLA 模型通过摄像头或传感器数据观察环境，解读用语言表达的目标（例如“拿起红苹果”）（如图所示），并输出低级或高级动作序列。

2.  这种转变使模型能够在同一计算空间内解释视觉观察和语言指令，从而实现灵活的上下文-觉察推理。例如，在“捡起红色的成熟苹果”任务中，视觉编码器（通常是 ViT 或 ConvNeXt）对场景中的目标（例如，苹果、树叶、背景）进行分割和分类，识别颜色和成熟度属性。语言模型（通常是 T5、GPT 或 BERT 的变体）将指令编码为高维嵌入。然后，这些表示通过交叉注意或联合 token 化方案融合，生成一个统一的潜空间，为行动策略提供信息。

3.  VLA 模型区别于 VLM 核心创新在于其基于 token 的表示，该框架支持对感知、语言和物理动作空间进行整体推理，使用离散 token 对世界进行编码，将所有模态（视觉、语言、状态和动作）统一到共享的嵌入空间。这使得模型不仅能够理解“需要做什么”（语义推理），还能以完全可学习和可组合的方式理解“如何做”（控制策略执行）。

-   Prefix Tokens ：编码上下文和指令，前缀 token 是 VLA 模型的上下文主干。这些 token 将环境场景（通过图像或视频）及其附带的自然语言指令编码成紧凑的嵌入，从而为模型的内部表征做好准备。

![](https://p0-xtjj-private.juejin.cn/tos-cn-i-73owjymdk6/05ad07661dcd458fb044caf3c2259162~tplv-73owjymdk6-jj-mark-v1:0:0:0:0:5o6Y6YeR5oqA5pyv56S-5Yy6IEAg5oiR5piv546L5aSn5L2g5piv6LCB:q75.awebp?policy=eyJ2bSI6MywidWlkIjoiNTM2MjE3NDA1ODk1MTQ5In0%3D&rk3s=f64ab15b&x-orig-authkey=f32326d3454f2ac7e96d3d06cdbb035152127018&x-orig-expires=1750124589&x-orig-sign=g2MeBe%2BHTPi%2FVvkYmww7tOsiRbc%3D)

- State Tokens：嵌入机器人的配置，除了感知外部刺激之外，VLA 还必须感知其内部的物理状态。这通过使用状态 token 来实现，状态 token 编码关于智能体配置的实时信息——关节位置、力-扭矩读数、夹持器状态、末端执行器姿态，甚至附近目标的位置。这些 token 对于确保态势觉察和安全至关重要，尤其是在操作或运动过程中。

![](https://p0-xtjj-private.juejin.cn/tos-cn-i-73owjymdk6/6673ffdb4d264f6bbf083c8666510e17~tplv-73owjymdk6-jj-mark-v1:0:0:0:0:5o6Y6YeR5oqA5pyv56S-5Yy6IEAg5oiR5piv546L5aSn5L2g5piv6LCB:q75.awebp?policy=eyJ2bSI6MywidWlkIjoiNTM2MjE3NDA1ODk1MTQ5In0%3D&rk3s=f64ab15b&x-orig-authkey=f32326d3454f2ac7e96d3d06cdbb035152127018&x-orig-expires=1750124589&x-orig-sign=RFOgMXocRYmpLz4VqYW0zmNK%2BSo%3D)

- Action Tokens：自回归控制生成，VLA token 流水线的最后一层涉及动作 token ，这些 tokens 由模型自回归生成，用于表示运动控制的下一步。每个 token 对应一个低级控制信号，例如关节角度更新、扭矩值、车轮速度或高级运动原语。在推理过程中，模型以 prefix tokens 和 state tokens 为条件，一步一步地解码这些 token ，从而有效地将 VLA 模型转变为语言驱动的策略生成器。这种方案可以与现实世界的驱动系统无缝集成，支持可变长度的动作序列，并支持通过强化或模仿学习框架进行模型微调。值得注意的是，RT-2 和 PaLM-E 等模型体现这种设计，将感知、指令和具体化融合到统一的 token 流中。这种方法的妙处在于，它使得传统上用于文本生成的 Transformer 能够以类似于生成句子的方式生成物理动作序列——只不过在这里，句子本身就是动作。

![](https://p0-xtjj-private.juejin.cn/tos-cn-i-73owjymdk6/5db956a9874740b080284a2b535374ab~tplv-73owjymdk6-jj-mark-v1:0:0:0:0:5o6Y6YeR5oqA5pyv56S-5Yy6IEAg5oiR5piv546L5aSn5L2g5piv6LCB:q75.awebp?policy=eyJ2bSI6MywidWlkIjoiNTM2MjE3NDA1ODk1MTQ5In0%3D&rk3s=f64ab15b&x-orig-authkey=f32326d3454f2ac7e96d3d06cdbb035152127018&x-orig-expires=1750124589&x-orig-sign=pOmSE4Hm3PKiSNjGSXHfQJ7arxc%3D)

- 如图例子所示，该系统从多模态输入采集开始，收集三种不同的数据流：视觉观测（例如RGB-D帧）、自然语言命令和实时机器人状态信息（例如关节角度或速度）。使用预训练模块将它们独立地 token 化为离散的嵌入。图像通过视觉 Transformer (ViT) 主干处理以生成视觉 token，指令由 BERT 或 T5 等语言模型解析以生成语言 token，状态输入通过轻量级 MLP 编码器转换为紧凑的状态 token。然后使用跨模态注意机制融合这些 token，其中模型联合推理目标语义、空间布局和物理约束，这种融合的表示构成决策的上下文基础。这个表示为多模态融合步骤。融合的嵌入被传递到自回归解码器（通常是 Transformer）中，该解码器生成一系列动作 tokens。这些 tokens 可能对应于关节位移、夹持器力调节或高级运动原语（例如，“旋转手腕”）。随后，动作 tokens 被转换为控制命令并传递到执行循环，执行循环通过反馈机器人的更新状态来闭合感知-动作循环，从而为下一步推理提供信息。这种闭环机制使模型能够实时动态地适应扰动、目标移动或遮挡。

- 为了提供具体的实现细节，算法 1 形式化了 VLA（视觉-语言-动作）标记化过程。给定一个 RGB-D 帧 $I $ ，自然语言指令 $T$，和关节角度向量 $\theta $ ，算法生成一组可以按顺序执行的动作标记。图像 $I $ 通过一个 ViT 处理以产生 $V$ ，一组 400 个视觉 token 。同时，指令 $T $ 被一个 BERT 模型编码以产生 $L$ ，一个包含 12 个语义语言 token 的序列。同时，机器人状态 $\theta $ 通过一个多层感知机（MLP）传递以生成一个 64 维的状态嵌入 $S$ 。然后这些 token 通过一个交叉注意力模块融合以产生一个共享的 512 维表示 $F$ ，捕获执行基于上下文的动作所需的语义、意图和情境感知。最后一个策略解码器，如 FAST ，将融合的特征映射到 50 个离散的动作 token ，这些 token 随后可以被解码成电机命令 $\tau_{1:N}$ 。

 ![](https://p0-xtjj-private.juejin.cn/tos-cn-i-73owjymdk6/2cd6d0f6633e4a0ebf8448b899298a46~tplv-73owjymdk6-jj-mark-v1:0:0:0:0:5o6Y6YeR5oqA5pyv56S-5Yy6IEAg5oiR5piv546L5aSn5L2g5piv6LCB:q75.awebp?policy=eyJ2bSI6MywidWlkIjoiNTM2MjE3NDA1ODk1MTQ5In0%3D&rk3s=f64ab15b&x-orig-authkey=f32326d3454f2ac7e96d3d06cdbb035152127018&x-orig-expires=1750124589&x-orig-sign=x0mk01Hyk9b2NBjm%2Fcbyacdr2to%3D)

- 解码过程使用基于 Transformer 的架构实现，融合后的 token 被传递到解码器，解码器根据先前的 token 和上下文，自回归地预测下一个最可能的动作 token。最终的运动指令序列通过对输出进行去 token 化获得。此实现反映了大语言模型中文本生成的工作方式，但这里的“句子”是一条运动轨迹。

![](https://p0-xtjj-private.juejin.cn/tos-cn-i-73owjymdk6/da3f930c36374eb5b9234c662af43f38~tplv-73owjymdk6-jj-mark-v1:0:0:0:0:5o6Y6YeR5oqA5pyv56S-5Yy6IEAg5oiR5piv546L5aSn5L2g5piv6LCB:q75.awebp?policy=eyJ2bSI6MywidWlkIjoiNTM2MjE3NDA1ODk1MTQ5In0%3D&rk3s=f64ab15b&x-orig-authkey=f32326d3454f2ac7e96d3d06cdbb035152127018&x-orig-expires=1750124589&x-orig-sign=fMgT930dvZQblwesj3vBlq0mSGE%3D)

4.  训练 VLA 模型需要一种混合学习范式，该范式整合来自网络的语义知识和来自机器人数据集的任务落地信息

![](https://p0-xtjj-private.juejin.cn/tos-cn-i-73owjymdk6/acc609f7324944ad979544644c701841~tplv-73owjymdk6-jj-mark-v1:0:0:0:0:5o6Y6YeR5oqA5pyv56S-5Yy6IEAg5oiR5piv546L5aSn5L2g5piv6LCB:q75.awebp?policy=eyJ2bSI6MywidWlkIjoiNTM2MjE3NDA1ODk1MTQ5In0%3D&rk3s=f64ab15b&x-orig-authkey=f32326d3454f2ac7e96d3d06cdbb035152127018&x-orig-expires=1750124589&x-orig-sign=krVdb0LcYLCimjYYoUUQyrweJ%2FY%3D)
- 第一阶段，大规模互联网语料库构成模型语义先验的主干。这些数据集包括图像-字幕对、指令跟踪数据集以及视觉问答语料库。此类数据集支持对视觉和语言编码器进行预训练，帮助模型获得目标、动作和概念的通用表示。此阶段通常使用对比或掩码建模目标，以在共享的嵌入空间内对齐视觉和语言模态。重要的是，此阶段为 VLA 提供了基础性的“世界理解”，从而促进了组合泛化、目标落地和零样本迁移。
- 然而，仅有语义理解不足以执行物理任务。第二阶段侧重于将模型植根于具身体验。机器人轨迹数据集用于教导模型如何将语言和感知转化为动作。数据集包括在自然语言指令下的视频-动作对、关节轨迹和环境交互 。此阶段通常采用监督学习、强化学习或模仿学习来训练自回归策略解码器，使其基于融合的视觉-语言-状态嵌入预测动作 token。
-  最近的研究越来越多地采用多阶段或多任务训练策略。模型通常使用掩码语言模型在视觉-语言数据集上进行预训练，然后使用 token 级自回归损失在机器人演示数据上进行微调 。其他方法则采用课程学习，先完成简单任务（例如，物体推送）再完成复杂任务（例如，多步骤操作）。一些方法进一步利用域自适应（例如 Open-VLA 或模拟-到-现实迁移）来弥合合成分布与现实世界分布之间的差距。通过将语义先验与任务执行数据统一起来，这些学习范式使 VLA 模型能够跨任务、跨领域和跨具体实现进行泛化，从而构成了可扩展、指令跟随型智体的主干，这些智体能够在现实世界中稳健地运行。
- 通过共同微调这些数据集得以对齐。该模型学习将视觉和语言输入映射到适当的动作序列。这种训练范式不仅有助于模型理解目标的 affordance（例如，苹果可以被抓取）和动作结果（例如，举起需要力和轨迹），还能促进模型泛化到新场景。如果一个在厨房操作任务上训练的模型已经学习了物体定位、抓取和遵循语言指令的一般原理，那么它或许能够推断出如何在户外果园里摘苹果。RT-2 将动作生成视为文本生成的一种形式，其中每个动作 token 对应于机器人控制空间中的一个离散命令。
- VLA 的另一个优势在于它们能够执行自适应控制，利用传感器的实时反馈动态调整行为。在执行过程中，状态 token 会实时更新，反映传感器输入和关节反馈。然后，模型可以相应地修改其计划好的动作。例如，在采摘苹果的场景中，模型会不断动态地解释场景并调整抓取轨迹。这种能力模仿类似人类的适应性，是 VLA 系统相对于基于流水线的机器人的核心优势。

# 视觉-语言-动作模型的进展

1.  基于 Transformer 的语言模型（LLM）取得了显著成功，这催化了 VLA 模型的诞生，CLIP 和 Flamingo 等 VLM 通过对比学习建立稳健的视觉-文本对齐，实现零样本物体识别，并为 VLA 模型奠定了基础。从 2023 年到 2024 年，VLA 模型经历了重大的架构改进和改进的训练方法。 最近的 VLA 模型已趋向于三种主要的架构范式，它们在效率、模块化和鲁棒性之间取得平衡：早期融合模型、双系统架构和自校正框架。

- 早期融合模型：侧重于在输入阶段融合视觉和语言表征，然后再将它们传递到策略模块。 EF-VLA 接受图像-文本对，使用 CLIP 的冻结编码器对其进行编码，并在动作预测之前，在 Transformer 主干网络的早期阶段融合生成的嵌入。这种设计确保 CLIP 预训练过程中学习的语义一致性得以保留，从而减少过拟合并增强了泛化能力。通过避免对视觉语言模块进行微调，这种方法还能保持计算效率，并防止在特定领域训练期间发生灾难性遗忘。
-  双系统架构：受人类认知双-过程理论的启发，NVIDIA 的 Groot N1 等模型实现两个互补的子系统：快速反应模块（系统 1）和慢速推理规划器（系统 2）。系统 1 包含基于扩散的控制策略，运行延迟为 10 毫秒，非常适合细粒度的低级控制，例如末端执行器稳定或自适应抓取。相比之下，系统 2 使用 LLM 进行任务规划、技能组合和高级排序。规划器将长期目标（例如“清理桌子”）解析为原子子任务，而低级控制器则确保实时执行。这种分解可以实现多时间尺度推理并提高安全性，尤其是在快速反应和深思熟虑必须共存的环境中。
-  自校正框架：第三个架构演变是开发自校正 VLA 模型，该模型旨在无需外部监督即可检测故障情况并从中恢复。SC-VLA 引入一种混合执行循环，具有快速推理路径和慢速校正路径。在该框架中，默认行为是使用轻量级 Transformer 直接从融合嵌入中预测姿势或动作。当检测到故障（例如抓取失败或与障碍物碰撞）时，模型将调用执行思维链推理的辅助过程 。该路径查询内部 LLM（或外部专家系统）以诊断故障模式并生成纠正策略 。例如，如果机器人反复错误识别被遮挡的目标，LLM 可能会建议主动改变视点或重新调整夹持器。

2.  VLA 模型在训练和优化技术方面取得快速进展，这些技术能够协调多模态输入、降低计算需求并实现实时控制。关键进展领域包括：

- 数据高效学习
    *   – 基于海量视觉语言语料库（例如 LAION-5B）和机器人轨迹集合（例如 Open X-Embodiment）进行协同微调，将语义理解与运动技能相结合。OpenVLA（7B 参数）的成功率比 55B 参数的 RT-2 变体高出 16.5%，这表明协同微调能够以更少的参数实现强大的泛化能力。
     *   – 通过 UniSim 生成合成数据，生成逼真的场景（包括遮挡和动态光照），以增强罕见的边缘情况，将模型在杂乱环境中的鲁棒性提高 20% 以上。
     *   – 自监督预训练采用对比目标（类似 CLIP）在动作微调之前学习联合视觉文本嵌入，从而减少对特定任务标签的依赖。Qwen2-VL 利用自监督对齐，将下游抓取和放置收敛速度加快 12% 。
- 参数高效自适应。低秩自适应 (LoRA) 将轻量级适配器矩阵插入冻结的 Transformer 层，在保持性能的同时，将可训练权重减少高达 70%。 Pi-0 Fast 变型仅使用静态主干上的 10M 个适配器参数，即可提供连续 200 Hz 控制，且精度损失可忽略不计。
- 推理加速。
   *   – 双系统框架（例如 Groot N1）中的压缩动作token (FAST) 和并行解码可将策略步骤速度提高 2.5 倍，实现低于 5 ms 的延迟，同时对轨迹平滑度的影响不大。
   *   – 硬件感知优化（包括张量核量化和流水线注意核）将运行时内存占用缩小到 8 GB 以下，并支持在嵌入式 GPU 上进行实时推理。

3.  基于数据高效训练的进展，近期研究重点关注参数高效的自适应和推理加速技术——这对于在资源受限的机器人平台上部署至关重要。

- 低秩自适应 (LoRA)。LoRA 将小型可训练的秩分解矩阵注入冻结的 transformer 层，仅需增加几百万个权重即可对数十亿参数的 VLA 模型进行微调。至关重要的是，LoRA 适配的模型在适应新的机器人操作任务（例如，新的目标形状）的同时，保留了其高级语言基础和视觉推理能力，这使得无需超级计算资源的实验室也能使用大型 VLA 模型。
- 量化。将权重精度降低到 8 位整数 (INT8) 可将模型大小缩小一半，并使片上吞吐量翻倍。OpenVLA 实验表明，在 Jetson Orin 上，INT8 量化在拾取和放置基准测试中保持了 97% 的全精度任务成功率，而在细粒度灵活性任务中仅下降 5% 。诸如训练后量化和逐通道标定等补充方法，可以进一步最大限度地减少高动态范围传感器输入的精度损失。这些优化允许在 50 W 边缘模块上以 30 Hz 的频率实现连续控制环路。
- 模型剪枝。结构化剪枝会移除被识别为冗余的整个注意头或前馈子层。虽然在 VLA 中的探索不如在纯视觉或语言模型中那么多，但早期关于扩散策略的研究表明，对基于卷积神经网络的视觉编码器进行高达 20% 的剪枝，在抓取稳定性方面几乎不会产生性能下降。类似的方案应用于基于 Transformer 的 VLA（例如 RDT-1B），可以将内存占用减少 25%，同时任务成功率下降不到 2%，这为 4 GB 以下的部署铺平道路 。
- 压缩动作 token 化 (FAST)。FAST 将连续动作输出重新表述为频域 token，将长控制序列压缩为简洁的描述符。 Pi-0 Fast 变型通过将 1000 毫秒的动作窗口 tokens 为 16 个离散 tokens，在桌面 GPU 上实现 200 Hz 的策略速率，使用 300 M 参数扩散头实现 15 倍的推理速度提升。这种方法以最小的轨迹粒度换取大幅加速，适用于双手装配等动态任务中的高频控制。
- 并行解码和动作分块。自回归 VLA 传统上逐个token地解码动作，从而产生连续延迟。并行解码架构（例如 Groot N1）同时解码多组时空 token，在 100 Hz 频率下，在 7 自由度臂上实现 2.5 倍的端到端延迟减少，位置误差增加不到 3 毫米。动作分块进一步将多步骤例程抽象为单个 token（例如“取放杯子”），在厨房工作流程等长期任务中将推理步骤减少高达 40% 。
- 强化学习-监督混合训练。iRe-VLA 框架在模拟中的强化学习 (RL) 和基于人类演示的监督微调之间交替进行，以稳定策略更新。通过利用直接偏好优化 (DPO) 来塑造奖励模型，并利用保守 Q-Learning 来避免外推误差，iRe-VLA 与纯强化学习相比将样本复杂度降低 60%，同时保持语言条件先验赋予的语义保真度。这种混合方法可以为具有稀疏反馈的任务（例如动态避障）提供稳健的策略。
- 硬件-觉察优化。编译器级的图重写和内核融合（例如通过 NVIDIA TensorRT-LLM）利用目标硬件特性——张量核、融合注意机制和流水线内存迁移——来加速 Transformer 推理和扩散采样。在 OpenVLA-OFT 中，与标准 PyTorch 执行相比，此类优化将 RTX A2000 GPU 上的推理延迟降低 30%，并将每次推理的能耗降低 25% 。这使得在功耗预算严格的移动机器人和无人机上实现实时 VLA 成为可能。

讨论。参数高效的自适应和推理加速技术共同推动 VLA 部署的民主化：
- LoRA 和量化技术使小型实验室能够在消费级硬件上微调和运行拥有数十亿参数的 VLA，从而为机器人开启前沿的语义理解。
- 剪枝和 FAST token 化技术压缩模型和动作表征，在不牺牲灵巧任务精度的情况下，实现低于 4 GB、低于 5 ms 的控制循环。
- 并行解码和动作分块技术克服自回归策略的顺序瓶颈，支持敏捷操作和腿部运动所需的 100–200 Hz 决策率。
- 混合 RL-SL 训练可稳定复杂环境中的探索，而硬件-觉察编译则可确保边缘加速器上的实时性能。


4.  VLA 模型正迅速崛起，成为具身智能的基础构建模块，将感知、自然语言理解和运动控制集成到一个统一的架构中。通过将视觉和语言模态编码到共享的语义空间中，并生成上下文落地的动作，VLA 模型实现智体与其环境之间的无缝交互 。这种多模态能力使 VLA 成为现实世界中广泛应用的变革性智体。在人形机器人领域，Helix 和 RoboNurse-VLA 等系统将视觉、语言和灵巧操作相结合，以协助完成家务和外科手术，并展示了实时推理和安全感知控制。在自动驾驶汽车领域，OpenDriveVLA 和 ORION 等模型处理动态视觉流和自然语言指令，以便在复杂的城市环境中做出透明、自适应的驾驶决策。工业部署利用 VLA 架构实现高精度装配、检测和协同制造。在农业领域，基于 VLA 的机器人系统能够实现视觉引导的水果采摘、植物监测和异常检测，从而减少对劳动力的依赖并提高可持续性。此外，交互式增强现实系统的最新进展利用 VLA 模型进行实时、基于语言的空间导航，根据语音或视觉提示在室内和室外环境中引导用户。在这些领域，VLA 提供了一个统一的框架，用于执行稳健、适应性强且语义一致的任务，标志着向具身化通用智能体的关键转变。各个领域的更详细的介绍可以看论文内容。

    1.  ![](https://p0-xtjj-private.juejin.cn/tos-cn-i-73owjymdk6/4a717d11dbcf4dc5848c9f0cdc70d883~tplv-73owjymdk6-jj-mark-v1:0:0:0:0:5o6Y6YeR5oqA5pyv56S-5Yy6IEAg5oiR5piv546L5aSn5L2g5piv6LCB:q75.awebp?policy=eyJ2bSI6MywidWlkIjoiNTM2MjE3NDA1ODk1MTQ5In0%3D&rk3s=f64ab15b&x-orig-authkey=f32326d3454f2ac7e96d3d06cdbb035152127018&x-orig-expires=1750124589&x-orig-sign=GElWV9J68sE%2FwZ%2BYOCvQWHsuGd4%3D)

# 视觉-语言-动作模型的挑战和局限性

如图所示，VLA 模型面临着一系列多方面的挑战，涵盖算法、计算和伦理维度。

![](https://p0-xtjj-private.juejin.cn/tos-cn-i-73owjymdk6/cad128cc303e424b9a5f5e3cae6de2de~tplv-73owjymdk6-jj-mark-v1:0:0:0:0:5o6Y6YeR5oqA5pyv56S-5Yy6IEAg5oiR5piv546L5aSn5L2g5piv6LCB:q75.awebp?policy=eyJ2bSI6MywidWlkIjoiNTM2MjE3NDA1ODk1MTQ5In0%3D&rk3s=f64ab15b&x-orig-authkey=f32326d3454f2ac7e96d3d06cdbb035152127018&x-orig-expires=1750124589&x-orig-sign=b2d833t5HZx8WRvo%2BT8Rjz2fKUo%3D)

*   首先，由于自回归解码器的顺序性和多模态输入的高维性，在资源受限的硬件上实现实时推理仍然很困难。
*   其次，将视觉、语言和动作融合到连贯的策略中会在遇到意外的环境变化时引入安全漏洞。
*   第三，数据集偏差和基础错误会影响泛化能力，常常导致模型在分布外的任务中失败。
*   第四，集成不同的组件（感知、推理、控制）会产生难以优化和维护的复杂架构。
*   第五，大型 VLA 系统的能量和计算需求阻碍了其在嵌入式或移动平台上的部署。
*   最后，对环境变化的鲁棒性以及诸如隐私和偏见缓解等伦理考量，增加了社会和监管方面的顾虑。
