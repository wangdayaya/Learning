{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72bbee8d-1cab-4777-85a2-86b72f42a946",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\torch-1.13.1-py-3.8\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 16, 125, 32])\n",
      "torch.Size([4, 32, 125, 16])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "x = torch.randn(4,32,125,32)\n",
    "print(x[:, :16].shape)  # 第一个维度不变，第二维度取前 16 个，后面维度不免\n",
    "print(x[..., :16].shape)  # 表示对所有前面的维度保持不变，只对最后一个维度进行切片取前 16 个"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "01736890-19cf-454c-b5be-2e25c962831f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.6282, -0.2202]])\n",
      "tensor([[[-0.6282],\n",
      "         [-0.2202]]])\n",
      "tensor([[[-0.6282, -0.6282, -0.6282, -0.6282],\n",
      "         [-0.2202, -0.2202, -0.2202, -0.2202]]])\n",
      "tensor([[-0.6282, -0.6282, -0.6282, -0.6282, -0.2202, -0.2202, -0.2202, -0.2202]])\n",
      "tensor([[-0.6282, -0.2202, -0.6282, -0.2202, -0.6282, -0.2202, -0.6282, -0.2202]])\n",
      "tensor(False)\n"
     ]
    }
   ],
   "source": [
    "# k和v无法用 repeat(1,4,1,1) ，原因如下\n",
    "k = torch.randn(1,2)\n",
    "shape = list(k.shape)\n",
    "shape[1] *= 4\n",
    "print(k)\n",
    "a = k.unsqueeze(2)\n",
    "print(a)\n",
    "b = a.repeat(1,1,4)\n",
    "print(b)\n",
    "k1 = b.reshape(shape)\n",
    "print(k1)  \n",
    "k2 = k.repeat(1, 4)\n",
    "print(k2) \n",
    "print((k1 == k2).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f829b5b6-a6df-4fca-897b-888da522d6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "@torch.no_grad()\n",
    "def llama_rotary_embedding(length):\n",
    "    inv_freq = torch.arange(0, 32, 2) / 32\n",
    "    inv_freq = 1 / (500000 ** inv_freq)\n",
    "    inv_freq = inv_freq.reshape(16, 1)\n",
    "\n",
    "    position_ids = torch.arange(length).reshape(1, length).float()\n",
    "    freq = inv_freq.matmul(position_ids).transpose(0,1)\n",
    "    emb = torch.cat((freq, freq), -1)\n",
    "    return emb.cos(), emb.sin()\n",
    "\n",
    "def apply_rotary_pos_emb(x, cos, sin):\n",
    "    def rotate_half(x):\n",
    "        left = x[..., :16]\n",
    "        right = -x[..., 16:]\n",
    "        return torch.cat((right, left), -1)\n",
    "    return x * cos + rotate_half(x) * sin\n",
    "\n",
    "def get_causal_mask(attention_mask):\n",
    "    B, L = attention_mask.shape\n",
    "    min_value = -1e15\n",
    "    causal_mask = torch.full((L, L), min_value).triu(diagonal=1)\n",
    "    causal_mask = causal_mask.reshape(1,1,L,L).repeat(B, 1, 1, 1)\n",
    "    causal_mask = causal_mask.to(attention_mask.device)\n",
    "\n",
    "    mask = attention_mask.reshape(B, 1, 1, L) == 0\n",
    "    causal_mask = causal_mask.masked_fill(mask, min_value)\n",
    "    return causal_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "19c1a59f-c2a6-476c-a9c3-f32d0b60477b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlamaRMSNorm(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.weight = torch.nn.Parameter(torch.ones(1024))\n",
    "    def forward(self, x):\n",
    "        var = x.pow(2).mean(-1, keepdim=True)\n",
    "        x = x * (var + 1e-5).rsqrt()\n",
    "        return self.weight * x\n",
    "        \n",
    "class LlamaMLP(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.gate_proj = torch.nn.Linear(1024, 14336, bias=False)\n",
    "        self.up_proj = torch.nn.Linear(1024, 14336, bias=False)\n",
    "        self.down_proj = torch.nn.Linear(14336, 1024, bias=False)\n",
    "        self.act_fn = torch.nn.SiLU()\n",
    "    def forward(self, x):\n",
    "        left = self.act_fn(self.gate_proj(x))\n",
    "        right = self.up_proj(x)\n",
    "        return self.down_proj(left * right)\n",
    "\n",
    "class LlamaAttention(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.q_proj = torch.nn.Linear(1024, 1024, bias=False)\n",
    "        self.k_proj = torch.nn.Linear(1024, 256, bias=False)\n",
    "        self.v_proj = torch.nn.Linear(1024, 256, bias=False)\n",
    "        self.o_proj = torch.nn.Linear(1024, 1024, bias=False)\n",
    "\n",
    "    def forward(self, hidden_state, attention_mask):\n",
    "        B, L, _ = hidden_state.shape\n",
    "        q = self.q_proj(hidden_state).reshape(B, L, 32, 32).transpose(1,2)\n",
    "        k = self.k_proj(hidden_state).reshape(B, L, 8, 32).transpose(1,2)\n",
    "        v = self.v_proj(hidden_state).reshape(B, L, 8, 32).transpose(1,2)\n",
    "\n",
    "        cos, sin = llama_rotary_embedding(L)\n",
    "        cos, sin = cos.to(hidden_state.device), sin.to(hidden_state.device)\n",
    "        q = apply_rotary_pos_emb(q, cos, sin)\n",
    "        k = apply_rotary_pos_emb(k, cos, sin)\n",
    "        k = k.unsqueeze(2).repeat(1, 1, 4, 1, 1).reshape(B, -1, L, 32)\n",
    "        v = v.unsqueeze(2).repeat(1, 1, 4, 1, 1).reshape(B, -1, L, 32)\n",
    "\n",
    "        attn = q.matmul(k.transpose(2,3)) / math.sqrt(32)\n",
    "        attention_mask = get_causal_mask(attention_mask)\n",
    "        attn = (attn + attention_mask).softmax(-1)\n",
    "        attn = attn.matmul(v)\n",
    "        \n",
    "        attn = attn.transpose(1,2).reshape(B, L, -1)\n",
    "        attn = self.o_proj(attn)\n",
    "        return attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ea6d4845-bda1-4c05-9aa5-8f9e3b0d8839",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlamaDecoderLayer(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.self_attn = LlamaAttention()\n",
    "        self.mlp = LlamaMLP()\n",
    "        self.input_layernorm = LlamaRMSNorm()\n",
    "        self.post_attention_layernorm = LlamaRMSNorm()\n",
    "        \n",
    "\n",
    "    def forward(self, hidden_state, attention_mask):\n",
    "        res = hidden_state\n",
    "        hidden_state = self.input_layernorm(hidden_state)\n",
    "        hidden_state = self.self_attn(hidden_state, attention_mask) + res\n",
    "        res = hidden_state\n",
    "        hidden_state = self.post_attention_layernorm(hidden_state)\n",
    "        hidden_state = self.mlp(hidden_state) + res\n",
    "        return  hidden_state  \n",
    "        \n",
    "class LlamaModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embed_tokens = torch.nn.Embedding(128256, 1024, None)\n",
    "        self.layers = torch.nn.ModuleList([LlamaDecoderLayer() for _ in range(4)])\n",
    "        self.norm = LlamaRMSNorm()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        hidden_state = self.embed_tokens(input_ids)\n",
    "        for layer in self.layers:\n",
    "            hidden_state = layer(hidden_state, attention_mask)\n",
    "        hidden_state = self.norm(hidden_state)\n",
    "        return hidden_state\n",
    "\n",
    "class LlamaForCausalLM(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = LlamaModel()\n",
    "        self.lm_head = torch.nn.Linear(1024, 128256, bias=False)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        logits = self.model(input_ids, attention_mask)\n",
    "        logits = self.lm_head(logits)\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            shift_logits = logits[:, :-1].reshape(-1, 128256)\n",
    "            shift_labels = labels[:, 1:].reshape(-1)\n",
    "            loss = torch.nn.functional.cross_entropy(shift_logits, shift_labels)\n",
    "        return loss, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ffee776f-fc97-419a-b22b-694c04df8a0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(11.9729, grad_fn=<NllLossBackward0>) torch.Size([4, 125, 128256])\n",
      "tensor(11.9729, grad_fn=<NllLossBackward0>) torch.Size([4, 125, 128256])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(True), tensor(True))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import LlamaConfig, LlamaForCausalLM as LlamaForCausalLM_Original\n",
    "\n",
    "#测试是否和官方模型的计算输出一样\n",
    "config = \"{'vocab_size': 128256, 'max_position_embeddings': 8192, 'hidden_size': 4096, 'intermediate_size': 14336, 'num_hidden_layers': 32, 'num_attention_heads': 32, 'num_key_value_heads': 8, 'hidden_act': 'silu', 'initializer_range': 0.02, 'rms_norm_eps': 1e-05, 'pretraining_tp': 1, 'use_cache': True, 'rope_theta': 500000.0, 'rope_scaling': None, 'attention_bias': False, 'attention_dropout': 0.0, 'mlp_bias': False, 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'bfloat16', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': False, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['LlamaForCausalLM'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': 128000, 'pad_token_id': None, 'eos_token_id': 128001, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': '', 'transformers_version': '4.38.2', 'model_type': 'llama'}\"\n",
    "config = LlamaConfig.from_dict(eval(config))\n",
    "config.hidden_size = 1024\n",
    "config.num_hidden_layers = 4\n",
    "\n",
    "model_actor1 = LlamaForCausalLM_Original(config)\n",
    "model_actor2 = LlamaForCausalLM()\n",
    "\n",
    "model_actor2.load_state_dict(model_actor1.state_dict())\n",
    "\n",
    "input = {\n",
    "    'input_ids': torch.randint(100, 50000, [4, 125]),\n",
    "    'attention_mask': torch.ones(4, 125).long(),\n",
    "    'labels': torch.randint(100, 50000, [4, 125])\n",
    "}\n",
    "input['attention_mask'][:, 120:] = 0\n",
    "\n",
    "out = model_actor1(**input)\n",
    "loss, logits = model_actor2(**input)\n",
    "\n",
    "print(out.loss, out.logits.shape)\n",
    "print(loss, logits.shape)\n",
    "\n",
    "out.loss == loss, (out.logits == logits).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3279aa2-0f7a-4aef-b426-6d9ca503345f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-1.13.1-py-3.8",
   "language": "python",
   "name": "torch-1.13.1-py-3.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
