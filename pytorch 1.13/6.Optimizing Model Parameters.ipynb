{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df955c04-e243-486d-928e-0664c6e1134a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\torch-1.13.1-py-3.8\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7c4274b1-9f3a-4637-9a16-3a0c045ef31e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_input:\n",
      " tensor([[ 0.0054, -1.0517, -0.4395],\n",
      "        [-0.9735, -0.4136, -0.4568],\n",
      "        [-0.0906, -1.4874,  0.7973]])\n",
      "soft_output:\n",
      " tensor([[0.5029, 0.1748, 0.3223],\n",
      "        [0.2259, 0.3954, 0.3787],\n",
      "        [0.2719, 0.0673, 0.6608]])\n",
      "logsoftmax_output:\n",
      " tensor([[-0.6873, -1.7444, -1.1322],\n",
      "        [-1.4877, -0.9278, -0.9710],\n",
      "        [-1.3022, -2.6990, -0.4143]])\n",
      "nlloss_output:\n",
      " tensor(1.3392)\n",
      "crossentropyloss_output:\n",
      " tensor(1.3392)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "x_input=torch.randn(3,3)#随机生成输入 \n",
    "print('x_input:\\n',x_input) \n",
    "y_target=torch.tensor([1,2,0])#设置输出具体值 print('y_target\\n',y_target)\n",
    "\n",
    "#计算输入softmax，此时可以看到每一行加到一起结果都是1\n",
    "softmax_func=nn.Softmax(dim=1)\n",
    "soft_output=softmax_func(x_input)\n",
    "print('soft_output:\\n',soft_output)\n",
    " \n",
    "#对比softmax与log的结合与nn.LogSoftmaxloss(负对数似然损失)的输出结果，发现两者是一致的。\n",
    "logsoftmax_func=nn.LogSoftmax(dim=1)\n",
    "logsoftmax_output=logsoftmax_func(x_input)\n",
    "print('logsoftmax_output:\\n',logsoftmax_output)\n",
    "\n",
    "#pytorch中关于NLLLoss的默认参数配置为：reducetion=True、size_average=True ,根据标签把对应的结果取出来求平均再去掉负号\n",
    "nllloss_func=nn.NLLLoss()\n",
    "nlloss_output=nllloss_func(logsoftmax_output,y_target)\n",
    "print('nlloss_output:\\n',nlloss_output)\n",
    "\n",
    "#直接使用pytorch中的loss_func=nn.CrossEntropyLoss()看与经过NLLLoss的计算是不是一样\n",
    "crossentropyloss=nn.CrossEntropyLoss()\n",
    "crossentropyloss_output=crossentropyloss(x_input, y_target)\n",
    "print('crossentropyloss_output:\\n',crossentropyloss_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0685f393-9dae-4a6a-8a00-88388cc3eb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = datasets.FashionMNIST(root=\"data\", train=True, download=True, transform=ToTensor())\n",
    "test_data = datasets.FashionMNIST(root=\"data\", train=False, download=True, transform=ToTensor())\n",
    "train_dataloader = DataLoader(training_data, batch_size=64)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64)\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a6f9be1d-e13a-4179-b7b5-bd75b79084b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28]) 9\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "for i in training_data:\n",
    "    print(i[0].shape, i[1])\n",
    "    break\n",
    "\n",
    "for x, y in train_dataloader:\n",
    "    print(x.shape)\n",
    "    print(y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f3454cf7-baf9-4c31-9e75-72f4912351b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.792460  [   64/60000]\n",
      "loss: 0.870733  [ 6464/60000]\n",
      "loss: 0.632614  [12864/60000]\n",
      "loss: 0.848078  [19264/60000]\n",
      "loss: 0.742331  [25664/60000]\n",
      "loss: 0.728443  [32064/60000]\n",
      "loss: 0.818450  [38464/60000]\n",
      "loss: 0.776703  [44864/60000]\n",
      "loss: 0.800395  [51264/60000]\n",
      "loss: 0.760595  [57664/60000]\n",
      "Accuracy: 72.1%, Avg loss: 0.760791 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.753925  [   64/60000]\n",
      "loss: 0.840109  [ 6464/60000]\n",
      "loss: 0.600129  [12864/60000]\n",
      "loss: 0.823758  [19264/60000]\n",
      "loss: 0.720855  [25664/60000]\n",
      "loss: 0.702934  [32064/60000]\n",
      "loss: 0.793813  [38464/60000]\n",
      "loss: 0.759571  [44864/60000]\n",
      "loss: 0.778112  [51264/60000]\n",
      "loss: 0.739770  [57664/60000]\n",
      "Accuracy: 73.2%, Avg loss: 0.738399 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.720256  [   64/60000]\n",
      "loss: 0.812392  [ 6464/60000]\n",
      "loss: 0.572530  [12864/60000]\n",
      "loss: 0.803252  [19264/60000]\n",
      "loss: 0.702219  [25664/60000]\n",
      "loss: 0.681940  [32064/60000]\n",
      "loss: 0.771314  [38464/60000]\n",
      "loss: 0.744646  [44864/60000]\n",
      "loss: 0.758817  [51264/60000]\n",
      "loss: 0.721351  [57664/60000]\n",
      "Accuracy: 74.2%, Avg loss: 0.718585 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "epochs = 5\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch*batch_size + len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "epochs = 3\n",
    "for t  in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fdcb038f-ca95-4dba-931f-1c53592368a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('linear_relu_stack.0.weight',\n",
       "              tensor([[ 5.5497e-03, -1.0505e-02, -1.7027e-02,  ..., -8.1669e-03,\n",
       "                        3.5819e-02,  2.1796e-02],\n",
       "                      [ 1.5335e-02, -2.9324e-02,  1.6550e-02,  ..., -1.8013e-02,\n",
       "                        1.4317e-02, -5.4089e-05],\n",
       "                      [ 5.6648e-03, -2.6010e-02, -2.9144e-03,  ...,  7.4799e-03,\n",
       "                        1.1316e-03, -2.8733e-02],\n",
       "                      ...,\n",
       "                      [-2.2052e-02,  7.9496e-04,  1.4152e-02,  ..., -4.1585e-04,\n",
       "                        2.7763e-02,  1.0144e-02],\n",
       "                      [ 7.9229e-03, -2.6692e-02,  2.0166e-02,  ..., -1.6596e-02,\n",
       "                       -8.1415e-03,  3.1815e-02],\n",
       "                      [-3.3667e-02, -1.4509e-02, -1.9302e-02,  ...,  1.9705e-02,\n",
       "                       -1.1895e-02, -2.8388e-02]])),\n",
       "             ('linear_relu_stack.0.bias',\n",
       "              tensor([-1.3159e-02, -6.8923e-03, -4.9877e-03, -2.7703e-02, -8.1573e-03,\n",
       "                       8.0231e-03,  3.3331e-02, -2.0317e-02, -2.3943e-02,  1.1286e-02,\n",
       "                       1.0644e-02,  3.6182e-02,  1.7634e-02, -2.6902e-03,  1.9715e-02,\n",
       "                      -2.0063e-02, -8.5395e-03,  6.8235e-03,  1.8426e-03,  2.8260e-02,\n",
       "                      -2.7385e-02,  1.2349e-02, -1.3461e-02,  3.2904e-03, -1.5472e-03,\n",
       "                       1.2737e-02,  4.0248e-03,  4.6979e-02, -3.3030e-02,  1.1487e-03,\n",
       "                       1.9890e-02, -1.3354e-02, -2.5158e-02, -1.8047e-02,  2.7123e-02,\n",
       "                       3.0310e-02,  2.1489e-02,  2.2761e-02, -1.2159e-02, -2.2126e-02,\n",
       "                       3.5956e-02,  3.8948e-02,  5.0970e-03, -2.8161e-02, -1.9714e-02,\n",
       "                       1.7583e-03, -9.6522e-03, -1.1035e-02,  1.1784e-03, -1.0483e-02,\n",
       "                       2.8340e-02, -7.2495e-03,  2.7745e-02,  3.7829e-02,  3.6010e-02,\n",
       "                       1.7867e-02,  2.6868e-02, -8.1107e-03,  3.1868e-02, -1.2467e-02,\n",
       "                       1.6825e-02, -3.0595e-02, -2.5991e-02, -2.9355e-02, -2.5698e-02,\n",
       "                       3.1593e-02,  4.6534e-05,  3.4478e-02, -4.2457e-04, -2.9048e-02,\n",
       "                      -2.2262e-02,  3.3258e-02,  3.0146e-02,  7.1601e-03,  2.4703e-02,\n",
       "                       2.6416e-02,  2.0854e-02,  3.0980e-02, -1.7278e-02, -1.0450e-02,\n",
       "                      -7.2724e-03,  2.6367e-03,  1.4685e-02, -1.2393e-02, -1.1503e-02,\n",
       "                       2.0123e-03,  8.6292e-03, -5.1637e-03,  3.3978e-03,  5.7865e-03,\n",
       "                      -1.8307e-02, -3.1416e-02, -3.5499e-02,  3.5345e-02,  2.5239e-02,\n",
       "                      -2.1964e-02, -1.1623e-02, -9.8757e-04,  3.7589e-02,  8.6797e-03,\n",
       "                      -4.3330e-03, -1.2291e-03,  1.1422e-02, -1.7887e-02, -1.0107e-02,\n",
       "                       3.4800e-02, -1.8584e-02,  3.0269e-02, -3.3807e-02, -1.3220e-02,\n",
       "                       2.5873e-02,  3.1922e-02,  2.9583e-03,  3.9450e-02,  5.4914e-04,\n",
       "                       3.4002e-02, -1.5609e-02,  1.9413e-02, -2.9815e-02, -1.2013e-02,\n",
       "                       4.7685e-03, -2.8404e-03, -1.7285e-02, -2.2052e-02, -1.4024e-03,\n",
       "                       5.5412e-02, -5.0236e-03,  1.9530e-02,  2.3639e-02, -2.9420e-02,\n",
       "                       1.9750e-02,  3.3633e-02,  3.9593e-02, -1.1663e-02,  1.2939e-02,\n",
       "                       1.5573e-02,  2.0323e-02, -1.5419e-02, -3.2633e-02, -2.5502e-02,\n",
       "                       1.5099e-02,  2.6896e-02, -2.1690e-02,  3.2458e-03, -2.0114e-02,\n",
       "                       4.0843e-02,  4.8792e-04,  1.2278e-02,  2.0243e-02, -7.7678e-03,\n",
       "                       2.2311e-02,  2.8819e-02,  6.8196e-03,  3.3363e-02,  4.0694e-02,\n",
       "                      -4.0794e-03, -2.8608e-02,  1.3588e-02,  1.5164e-02,  4.1760e-02,\n",
       "                       5.1483e-02,  1.8178e-02,  3.1916e-02, -1.6621e-02,  2.7131e-02,\n",
       "                       4.0115e-02,  3.0812e-02,  2.9022e-02,  1.4822e-02, -1.9477e-02,\n",
       "                      -3.2216e-02, -8.5670e-03, -3.3125e-03, -8.4498e-03, -2.4270e-02,\n",
       "                       3.5123e-02,  9.6312e-03, -1.6202e-02,  2.3706e-02, -2.2585e-03,\n",
       "                      -3.1029e-02, -4.1697e-02, -1.4147e-02, -2.3722e-02,  4.1635e-02,\n",
       "                      -1.4059e-03, -3.2022e-02,  2.8825e-02,  4.9883e-02,  2.8717e-02,\n",
       "                      -9.1052e-03,  7.1007e-03,  3.4286e-02,  3.7779e-02,  2.6704e-02,\n",
       "                       1.2966e-02,  1.9452e-02, -3.2992e-02,  5.5408e-02, -3.2179e-02,\n",
       "                      -2.6500e-02,  1.5922e-02,  4.5898e-02, -1.5820e-02,  4.4284e-02,\n",
       "                      -2.3476e-02,  1.1523e-02, -2.4121e-02,  1.1503e-03, -3.4958e-02,\n",
       "                      -1.9643e-02,  3.8627e-02, -3.3260e-02, -2.3403e-02,  1.0323e-02,\n",
       "                       2.6442e-02,  1.0245e-03,  1.1571e-02, -3.5694e-02,  2.6803e-02,\n",
       "                       1.1400e-02,  4.2871e-02, -1.6442e-02,  1.0903e-02, -2.5036e-02,\n",
       "                       1.0137e-02,  3.3732e-02,  2.6882e-02,  1.9332e-03,  3.5837e-02,\n",
       "                       7.9207e-03,  2.1536e-02,  3.4528e-03, -8.5568e-04,  2.2488e-03,\n",
       "                       2.3513e-02, -3.7415e-03,  2.8036e-02,  1.7882e-02, -1.2050e-02,\n",
       "                       2.4635e-02,  1.5481e-02,  1.4780e-02, -4.7348e-03, -2.4047e-03,\n",
       "                      -1.9284e-03,  1.6496e-02,  2.4873e-02, -1.6468e-02,  2.3558e-02,\n",
       "                      -4.4340e-03, -1.1798e-03, -1.5338e-02, -3.5534e-04,  1.5662e-02,\n",
       "                      -2.8139e-02,  4.2518e-02,  1.8304e-02,  1.2290e-02, -1.6927e-02,\n",
       "                      -2.0223e-02,  3.2758e-02,  2.3405e-03, -1.8485e-02,  4.0252e-02,\n",
       "                       3.1794e-02,  3.2852e-02, -3.6921e-02,  2.3336e-02, -1.5125e-02,\n",
       "                      -2.4572e-02,  3.5336e-02,  4.1946e-02, -1.7344e-03,  1.5754e-02,\n",
       "                      -1.7854e-03, -2.8205e-02, -3.5540e-03,  1.2797e-03,  3.5877e-02,\n",
       "                       2.8264e-02,  8.0773e-03, -1.7166e-02, -1.5134e-02, -1.8185e-02,\n",
       "                       2.6845e-02, -2.0378e-02,  7.1342e-03,  1.6274e-02,  4.4202e-02,\n",
       "                       1.1439e-02, -1.3603e-04, -2.0340e-04, -1.4996e-02, -1.3020e-02,\n",
       "                       6.5224e-03, -1.4028e-02,  1.0882e-02, -2.8261e-02,  5.6990e-03,\n",
       "                      -1.3786e-02, -5.8598e-03,  2.3449e-02, -1.6244e-02, -3.7464e-03,\n",
       "                       2.0817e-02,  3.1749e-02, -1.9994e-03,  3.9399e-02,  3.7891e-02,\n",
       "                      -2.5717e-02, -1.6647e-03, -9.1259e-03, -2.9825e-02,  3.4508e-02,\n",
       "                       8.5242e-04,  2.1414e-02, -7.3535e-03, -1.3576e-02,  3.2953e-02,\n",
       "                       1.9285e-02,  4.6644e-03,  2.3494e-02, -2.5798e-02,  2.1818e-02,\n",
       "                      -2.9025e-02,  4.1334e-03,  1.1713e-02,  4.4728e-02,  2.8181e-02,\n",
       "                       5.4600e-03, -1.1444e-02, -1.0430e-02,  2.7729e-02, -2.9665e-03,\n",
       "                      -1.5111e-02,  2.6388e-02, -3.1761e-02, -2.1884e-02, -2.5764e-03,\n",
       "                       2.8188e-02,  3.8915e-02, -1.7449e-03, -1.7888e-02,  1.8287e-03,\n",
       "                       3.2325e-02, -2.0071e-02,  2.4595e-02,  2.6144e-02,  8.8285e-03,\n",
       "                      -1.8872e-02,  3.6258e-03,  1.8141e-02, -1.0920e-02, -3.2945e-02,\n",
       "                       7.5181e-03,  1.3556e-02, -1.7764e-02,  1.5975e-03, -6.6589e-03,\n",
       "                      -2.7702e-02,  3.3869e-02, -1.1210e-02, -2.8141e-03,  1.9182e-02,\n",
       "                      -2.9956e-02,  1.5313e-02, -6.4571e-03, -3.1681e-02,  5.7797e-04,\n",
       "                       2.8026e-02, -1.2284e-02, -3.3414e-02,  2.8791e-03, -2.6018e-02,\n",
       "                       5.0467e-02,  3.1490e-03, -2.1622e-02,  5.0727e-03,  4.1275e-02,\n",
       "                       4.1749e-02,  2.0500e-02,  1.9765e-02,  2.3945e-02, -1.2942e-03,\n",
       "                      -5.0331e-05,  2.7304e-02,  1.8902e-02, -9.8032e-03,  3.7710e-02,\n",
       "                      -6.5135e-04,  1.5107e-02, -3.9692e-02,  2.4191e-02, -1.8192e-02,\n",
       "                      -2.9727e-02, -1.6351e-02,  3.3097e-02, -1.8911e-02,  1.8010e-02,\n",
       "                       1.8553e-02,  1.8105e-02,  1.6005e-02, -7.4921e-03,  2.9169e-02,\n",
       "                      -3.2444e-02, -4.1730e-02,  4.4170e-02,  1.8281e-02,  2.1561e-02,\n",
       "                       2.9714e-02,  4.8457e-02, -9.4369e-03, -3.3266e-02,  2.0307e-02,\n",
       "                      -1.1980e-02,  2.7323e-02,  4.4478e-02,  2.1905e-02, -1.6004e-02,\n",
       "                       4.2007e-02,  8.2381e-03, -2.7315e-02,  7.6543e-03, -3.0426e-02,\n",
       "                       1.8380e-02, -2.7896e-02,  2.4622e-03,  7.7829e-03, -1.5094e-02,\n",
       "                       2.1643e-02, -9.5385e-03,  2.6872e-04, -2.3668e-02,  4.9741e-02,\n",
       "                       3.1333e-03, -6.0886e-03,  1.2558e-02,  1.2376e-02, -4.7569e-03,\n",
       "                       1.0053e-02, -2.1418e-02,  1.2369e-02, -2.0597e-02, -2.9835e-02,\n",
       "                       2.3318e-02, -1.3378e-02,  3.0717e-02, -2.6887e-02,  9.5843e-03,\n",
       "                      -4.9840e-03,  4.1396e-02, -1.3501e-02,  3.6814e-02, -3.0412e-02,\n",
       "                       1.7503e-02,  8.3980e-03, -9.9296e-03,  8.9764e-03,  1.6839e-02,\n",
       "                      -6.0267e-03, -1.9929e-02,  2.0645e-02, -1.1701e-02,  3.1692e-02,\n",
       "                      -4.1675e-02,  1.9245e-02,  2.0764e-02, -2.8600e-02,  1.5954e-02,\n",
       "                      -2.9917e-03,  2.2513e-02, -1.1697e-03, -3.1279e-02,  2.7253e-02,\n",
       "                       6.5665e-03, -1.7575e-02,  3.2608e-02, -8.7520e-04, -3.3454e-02,\n",
       "                       2.9566e-02,  1.4423e-02,  1.8181e-02,  7.7523e-03,  1.1588e-03,\n",
       "                      -1.3042e-02,  3.1182e-02,  1.9328e-02, -2.9377e-02, -2.9709e-02,\n",
       "                      -7.6749e-03,  2.7939e-02, -2.5799e-02,  2.6883e-02, -1.0025e-02,\n",
       "                      -1.3961e-02, -2.6195e-02,  1.0745e-02, -2.3749e-02,  6.5005e-03,\n",
       "                       2.1645e-02,  2.6985e-02, -2.5158e-02,  1.1879e-02,  7.9267e-03,\n",
       "                       4.3596e-03,  1.6396e-02, -1.0855e-02, -3.8046e-02, -1.9561e-02,\n",
       "                      -1.0226e-02,  2.4701e-02])),\n",
       "             ('linear_relu_stack.2.weight',\n",
       "              tensor([[ 0.0377,  0.0209, -0.0022,  ..., -0.0440,  0.0236,  0.0069],\n",
       "                      [-0.0302,  0.0093,  0.0375,  ..., -0.0375, -0.0435,  0.0267],\n",
       "                      [-0.0339,  0.0173, -0.0123,  ..., -0.0272,  0.0014,  0.0051],\n",
       "                      ...,\n",
       "                      [ 0.0181, -0.0093,  0.0106,  ...,  0.0035, -0.0208,  0.0014],\n",
       "                      [ 0.0384,  0.0155,  0.0189,  ..., -0.0235, -0.0133, -0.0087],\n",
       "                      [ 0.0147, -0.0109,  0.0248,  ...,  0.0007,  0.0432,  0.0170]])),\n",
       "             ('linear_relu_stack.2.bias',\n",
       "              tensor([-2.0696e-03,  2.3276e-03, -2.7145e-02, -1.5079e-02,  6.6393e-02,\n",
       "                       3.0438e-02,  1.3976e-02,  1.4122e-02, -3.4481e-02, -4.3044e-02,\n",
       "                       1.1459e-02, -1.1610e-02,  2.0379e-02,  2.5523e-02,  6.1149e-03,\n",
       "                       3.5321e-03, -3.3072e-02,  2.5055e-02,  2.9478e-03, -2.8877e-02,\n",
       "                      -5.2592e-02,  2.6001e-02,  4.4271e-03,  3.7519e-02,  1.1914e-02,\n",
       "                       6.6798e-03,  5.2803e-02, -3.9638e-02,  4.2561e-02,  4.8754e-02,\n",
       "                      -1.6766e-02, -9.9380e-03,  3.2220e-03, -9.8930e-03, -7.7805e-03,\n",
       "                       2.6310e-02,  2.2023e-02,  1.6838e-02,  1.1603e-02, -2.7813e-02,\n",
       "                       6.3075e-02,  4.9207e-02,  2.9076e-02, -1.8014e-02,  4.4185e-02,\n",
       "                      -2.2552e-02,  3.6041e-02, -2.6080e-02, -1.2447e-02,  2.4854e-02,\n",
       "                       1.6323e-02,  2.4819e-02,  4.3097e-02,  4.5659e-02, -3.4994e-02,\n",
       "                       1.4997e-02,  6.2367e-03, -3.3512e-03,  6.4896e-03, -2.8588e-02,\n",
       "                      -2.0034e-02, -1.8758e-02,  8.9875e-03, -9.1101e-03,  1.1156e-02,\n",
       "                       5.2842e-04, -1.3253e-02, -3.0457e-02,  4.3973e-02,  3.1821e-02,\n",
       "                       1.7079e-02,  6.5879e-02,  3.1979e-02, -4.7172e-02,  5.3317e-03,\n",
       "                       2.2340e-02,  2.9073e-03,  5.9833e-03,  8.2309e-03,  2.2753e-02,\n",
       "                       1.5702e-02,  1.6897e-02,  6.0488e-03,  3.3139e-02, -3.1367e-03,\n",
       "                      -8.7849e-03, -4.2547e-02, -3.5364e-02,  3.5616e-02,  2.1125e-02,\n",
       "                      -4.6416e-02, -5.9575e-03,  1.0192e-02,  4.5498e-02,  3.9589e-02,\n",
       "                       1.2851e-02,  4.6826e-02,  2.2714e-02, -3.0782e-03,  2.4924e-02,\n",
       "                       1.6319e-02,  4.1289e-02,  8.3152e-03,  4.3858e-02, -1.0515e-02,\n",
       "                      -3.7816e-02, -2.3430e-02, -2.6882e-02, -4.2304e-02,  4.4971e-02,\n",
       "                      -3.3977e-02,  4.3550e-03,  2.4921e-02,  1.6237e-02,  4.4222e-03,\n",
       "                       3.4371e-02, -3.0275e-02,  2.7253e-02, -2.5879e-02,  2.4008e-02,\n",
       "                       5.4880e-02, -3.8245e-02, -3.3557e-02,  5.0831e-03,  1.7131e-02,\n",
       "                      -3.1524e-02, -5.4264e-02,  5.5748e-02, -8.4783e-03, -3.6636e-02,\n",
       "                       3.7513e-02,  3.1818e-02,  2.3258e-02, -3.6036e-02,  2.0574e-02,\n",
       "                      -4.5370e-02, -4.6691e-04,  3.2838e-02,  1.1815e-02,  2.1345e-03,\n",
       "                       6.4180e-02,  2.1287e-02,  5.6145e-02,  2.4007e-02,  3.8005e-02,\n",
       "                       8.1170e-03,  4.7605e-02, -8.8535e-03, -2.4768e-02,  3.8190e-02,\n",
       "                       4.2908e-02, -4.7126e-03,  5.9426e-02, -8.1177e-03,  6.1829e-02,\n",
       "                      -4.7163e-02, -3.9975e-02, -2.8790e-02, -2.1317e-02, -1.0078e-02,\n",
       "                       6.3932e-02, -3.7558e-02, -2.8048e-02, -3.3756e-02, -1.7149e-02,\n",
       "                      -1.7432e-02,  1.7052e-02,  5.9309e-02, -3.4176e-03,  1.9450e-02,\n",
       "                       1.8214e-02, -6.3203e-03,  2.6996e-02, -1.3386e-02,  2.8502e-02,\n",
       "                       2.6828e-02,  4.5098e-03, -4.2981e-03, -3.3172e-02,  8.0969e-04,\n",
       "                       1.5414e-02,  6.0218e-02, -1.9730e-02, -2.2663e-02, -1.7338e-02,\n",
       "                       3.1163e-02,  5.4145e-02, -2.0669e-02,  6.4835e-03, -3.8660e-02,\n",
       "                      -2.8466e-02, -4.1037e-02, -2.7495e-02, -2.7277e-02, -2.7942e-02,\n",
       "                      -2.9808e-02,  3.7112e-02, -1.7520e-02,  2.8938e-03, -2.0957e-02,\n",
       "                       4.2438e-02, -1.7842e-02, -4.7970e-03,  3.7534e-02, -4.5078e-02,\n",
       "                       3.4444e-02,  3.0171e-02,  2.7356e-02,  1.2401e-02, -3.5158e-02,\n",
       "                       2.2585e-02, -6.6493e-03,  4.3348e-02,  1.4891e-02,  1.0869e-02,\n",
       "                       1.0880e-02,  2.2389e-02,  5.4435e-02,  3.0215e-02, -5.9445e-05,\n",
       "                       6.7305e-02,  3.0245e-02,  2.7249e-02,  1.7569e-02, -7.1442e-04,\n",
       "                       1.6016e-03,  4.4674e-02, -2.3466e-02,  4.4007e-02,  4.5533e-02,\n",
       "                      -2.0083e-02,  6.0092e-02, -3.1967e-02,  1.3707e-02,  3.0868e-02,\n",
       "                      -2.6367e-02,  5.2559e-02,  1.5459e-02, -1.0263e-02,  6.8658e-03,\n",
       "                      -4.2331e-02, -3.2043e-02,  6.4013e-02, -6.4496e-03,  2.1670e-02,\n",
       "                      -2.6459e-02,  3.0217e-02, -8.8753e-03,  3.6060e-03, -4.5140e-02,\n",
       "                      -3.1120e-02, -3.8715e-02, -9.6144e-03,  5.8531e-02,  3.1250e-02,\n",
       "                      -2.5702e-02, -2.2411e-02, -2.0028e-02, -3.6487e-02, -1.2524e-02,\n",
       "                       3.5606e-02,  5.8255e-02,  2.1673e-02, -3.0403e-02, -1.7860e-02,\n",
       "                       3.3907e-02, -5.2481e-02, -2.4329e-02, -2.4104e-02, -3.2032e-02,\n",
       "                      -4.1097e-02,  9.8030e-03, -2.1160e-02,  1.3130e-02, -3.7392e-02,\n",
       "                       1.9187e-02, -2.6815e-02, -9.8554e-03,  5.3049e-02,  1.0409e-02,\n",
       "                       7.5805e-03, -3.5400e-02, -3.1142e-02, -3.8190e-02,  9.4228e-03,\n",
       "                       4.3016e-02,  1.4321e-02, -1.2432e-02,  1.2304e-02,  4.7719e-02,\n",
       "                       2.7780e-02, -4.9013e-02, -2.2932e-02, -2.3003e-02,  1.2249e-03,\n",
       "                       3.6117e-02,  2.0397e-02, -3.9994e-03, -2.0040e-02,  6.2789e-02,\n",
       "                      -3.9825e-02,  6.0984e-02,  1.6826e-02, -6.7379e-03,  1.8542e-02,\n",
       "                      -3.6586e-02,  9.9612e-03,  2.4090e-02,  2.7501e-02, -1.3634e-02,\n",
       "                       2.3529e-02, -6.4443e-03,  6.5206e-02, -2.4961e-02, -2.7917e-02,\n",
       "                       3.2702e-02,  1.3105e-02,  7.6691e-03,  2.4009e-03,  3.2705e-02,\n",
       "                       2.7571e-02,  3.1001e-02,  5.3529e-02, -3.1399e-02, -2.1517e-02,\n",
       "                      -1.5683e-02,  2.2672e-02,  3.1835e-02, -2.7920e-02, -1.4833e-02,\n",
       "                      -1.8425e-02,  3.0539e-02,  3.5627e-03,  4.7632e-03,  4.0280e-02,\n",
       "                      -2.3908e-02, -2.4562e-02, -4.7344e-02,  8.1605e-03, -2.5285e-02,\n",
       "                      -1.1850e-02,  5.9724e-02,  2.5024e-02, -6.7018e-04,  1.4392e-02,\n",
       "                       3.9935e-03,  2.1132e-02, -3.0045e-03, -6.2380e-03,  7.7874e-04,\n",
       "                       5.2812e-03,  5.5431e-02, -3.2933e-02, -1.0695e-02,  2.8907e-02,\n",
       "                       1.2316e-02, -1.7810e-02,  3.1435e-02,  2.1247e-02, -1.1176e-02,\n",
       "                      -2.6300e-02,  1.6361e-02,  4.9327e-02, -8.3520e-04, -3.6564e-02,\n",
       "                      -2.6965e-02,  3.1121e-02,  3.7838e-02, -3.3733e-02, -4.0368e-03,\n",
       "                       3.3510e-03,  8.8114e-03, -1.3754e-02,  1.0809e-02,  1.1579e-02,\n",
       "                       1.1639e-03, -3.2613e-02,  2.1602e-02,  1.9133e-02, -3.5144e-02,\n",
       "                       3.1501e-02,  4.0400e-02,  2.6811e-02, -1.0648e-02,  1.7404e-02,\n",
       "                       4.7923e-03,  2.4768e-02,  1.3439e-02, -2.6653e-02, -2.9486e-03,\n",
       "                      -2.2550e-02,  2.1078e-02,  1.7353e-02, -1.7852e-02,  8.2741e-03,\n",
       "                       1.3247e-02,  1.5947e-02,  2.1598e-02, -2.4952e-02, -1.2844e-02,\n",
       "                      -3.8676e-02, -3.4928e-02,  1.2860e-02, -1.9309e-02, -3.6710e-02,\n",
       "                      -3.0337e-02,  4.0293e-03, -3.5156e-02, -9.8424e-03,  4.0320e-02,\n",
       "                      -3.0309e-02,  1.1933e-02, -3.3294e-02,  3.3404e-02,  3.7242e-02,\n",
       "                       1.7501e-02, -3.8235e-02,  4.4990e-02,  9.8402e-03, -3.8626e-02,\n",
       "                       6.7284e-03,  5.6196e-03, -3.8609e-02, -4.4904e-02,  4.4634e-02,\n",
       "                      -2.2323e-03, -7.4857e-03,  5.6475e-03, -1.1466e-02, -1.4998e-02,\n",
       "                       2.5219e-03,  5.4646e-02, -3.2620e-02, -2.9007e-02,  3.5571e-02,\n",
       "                       4.1245e-02, -3.8486e-02, -5.1460e-02,  5.1708e-02,  1.1998e-03,\n",
       "                       4.7979e-02,  3.9588e-02,  3.4874e-03,  1.6904e-02,  3.2918e-02,\n",
       "                       3.3590e-02,  1.5740e-02,  3.6080e-02,  4.2529e-02,  1.6539e-02,\n",
       "                       4.3101e-02, -2.8749e-02,  1.0718e-02, -4.4761e-02,  6.1891e-02,\n",
       "                       4.0401e-02,  4.0476e-02, -7.9192e-03,  5.1381e-02,  1.8014e-02,\n",
       "                       5.0576e-02,  2.1874e-02,  2.4618e-02,  1.2165e-02,  4.4502e-02,\n",
       "                      -1.6519e-02, -1.1901e-02, -3.3729e-02,  3.4165e-02, -1.6850e-02,\n",
       "                      -3.9907e-02, -1.8403e-03, -2.6998e-02, -9.5512e-03,  4.0161e-02,\n",
       "                      -1.7248e-02,  2.0630e-02, -3.2183e-02, -1.4461e-02, -1.9181e-02,\n",
       "                       4.3810e-02,  2.8944e-02,  2.5579e-02,  3.5678e-02, -4.0885e-02,\n",
       "                      -5.6543e-02, -3.9565e-02,  1.4967e-02,  1.5662e-02,  1.6177e-02,\n",
       "                      -2.8977e-02, -1.0707e-02, -1.7781e-02, -5.0056e-02,  1.8550e-02,\n",
       "                       6.6912e-02, -3.1753e-02, -2.7474e-02,  2.8365e-02,  1.7389e-02,\n",
       "                      -2.4164e-02, -1.5605e-02, -2.6614e-02, -3.4330e-02,  3.6787e-02,\n",
       "                      -6.7918e-03, -2.7045e-02, -3.5102e-02,  4.7851e-02,  9.6393e-03,\n",
       "                       4.9804e-02, -2.3737e-02])),\n",
       "             ('linear_relu_stack.4.weight',\n",
       "              tensor([[-1.4446e-02, -1.4116e-02, -2.7056e-02,  ..., -6.7476e-02,\n",
       "                        1.0282e-02, -4.0662e-02],\n",
       "                      [ 4.8203e-03,  1.7504e-02, -2.6444e-02,  ..., -1.1416e-01,\n",
       "                       -1.2564e-02, -2.7677e-02],\n",
       "                      [-1.2062e-05,  4.0283e-02, -6.5572e-04,  ..., -9.8320e-02,\n",
       "                       -4.2616e-03,  2.4129e-02],\n",
       "                      ...,\n",
       "                      [ 1.5232e-02,  2.2208e-02,  1.1271e-01,  ...,  1.4851e-01,\n",
       "                        5.1422e-02, -6.4639e-02],\n",
       "                      [-3.6112e-03, -3.7060e-02,  2.9956e-02,  ...,  3.7823e-02,\n",
       "                       -1.3791e-02,  4.2580e-02],\n",
       "                      [-7.3986e-04, -1.5207e-02, -4.5117e-02,  ...,  1.1359e-01,\n",
       "                       -4.6826e-02,  7.0449e-02]])),\n",
       "             ('linear_relu_stack.4.bias',\n",
       "              tensor([-0.0224,  0.0508, -0.0746,  0.0361, -0.1293,  0.3350,  0.0139,  0.0281,\n",
       "                      -0.0812, -0.1196]))])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609b4eff-a168-4de6-8229-4e7e28ce2702",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-1.13.1-py-3.8",
   "language": "python",
   "name": "torch-1.13.1-py-3.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
